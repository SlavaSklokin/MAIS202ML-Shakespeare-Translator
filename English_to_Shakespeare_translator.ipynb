{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "English_to_Shakespeare_translator.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QL1vnxsLMH9F"
      },
      "source": [
        "Imports"
      ],
      "id": "QL1vnxsLMH9F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "qsjqyp7UMH9K",
        "outputId": "f7baf7c8-7a55-496b-ecae-648ebcf0a68e"
      },
      "source": [
        "# --- H E R E   B E   I M P O R T S --- #\n",
        "#___________________________________________________________________________________\n",
        "# These are all the modules we'll be using later. Make sure you can import them\n",
        "# before proceeding further.\n",
        "import collections\n",
        "import logging\n",
        "import os\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "import sys\n",
        "import time\n",
        "import requests\n",
        "import urllib.request\n",
        "import io\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from string import digits\n",
        "from collections import Counter\n",
        "import csv\n",
        "from bs4 import BeautifulSoup\n",
        "import scipy\n",
        "import math\n",
        "import random\n",
        "import fileinput\n",
        "from pickle import dump\n",
        "import pandas\n",
        "import statsmodels\n",
        "\n",
        "import numpy as np\n",
        "from numpy import array, asarray, zeros\n",
        "\n",
        "import unicodedata\n",
        "from unicodedata import normalize\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from matplotlib import pylab\n",
        "%matplotlib inline\n",
        "\n",
        "#!pip install Keras\n",
        "#!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow.data\n",
        "# Seq2Seq Items\n",
        "#import tensorflow.contrib.seq2seq as seq2seq\n",
        "from tensorflow.python.ops.rnn_cell import LSTMCell, MultiRNNCell\n",
        "#from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
        "from tensorflow.python.layers.core import Dense\n",
        "\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import keras\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Embedding, Input, LSTM\n",
        "from keras.utils.vis_utils import plot_model\n",
        "from keras.utils import to_categorical"
      ],
      "id": "qsjqyp7UMH9K",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-5348f34ecc06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0m___________________________________________________________________________________\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;31m# These are all the modules we'll be using later. Make sure you can import them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# before proceeding further.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name '___________________________________________________________________________________' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3wS4CruMH9M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fdc642e-599e-4db0-8ba0-8eb5089394bb"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.config.list_physical_devices('GPU')\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], enable=True)"
      ],
      "id": "y3wS4CruMH9M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EraNrFbMH9O"
      },
      "source": [
        "Set up for tokenization"
      ],
      "id": "5EraNrFbMH9O"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3MmA9gOMH9O"
      },
      "source": [
        "max_length = 100\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "num_words = None,\n",
        "filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n',\n",
        "lower=True,\n",
        "split=' ',\n",
        "char_level=False, #if True, every character is tokenized instead of word,\n",
        "oov_token='eNiV')"
      ],
      "id": "j3MmA9gOMH9O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqyRg0puMH9O",
        "outputId": "3e4f0c03-73d8-49a6-fd11-73bcc53df749"
      },
      "source": [
        "print(len(tokenizer.word_index)+1)#this is out of order--for testing"
      ],
      "id": "wqyRg0puMH9O",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7HWcZvNMH9P"
      },
      "source": [
        "Data scraping "
      ],
      "id": "r7HWcZvNMH9P"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MfV_Vsq-MH9Q"
      },
      "source": [
        "# get rid of line breaks -- used by scraper\n",
        "def killbreaks(string):\n",
        "    string = string.replace('\\n', ' ')\n",
        "    string = string.replace('\\r', ' ')\n",
        "    return string"
      ],
      "id": "MfV_Vsq-MH9Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV-FNjiKMH9Q"
      },
      "source": [
        "def sparknotesScraper(play, parity):\n",
        "    '''Writes a processed .txt file for the given shakespearean play on SparkNotes.\n",
        "        \n",
        "        Argument: play name as spelled on SparkNotes\n",
        "            ex: 'Romeo and Juliet' is romeojuliet\n",
        "    \n",
        "        Return: None\n",
        "        \n",
        "        Output: Two text files, play.txt and playCleaned.txt'''\n",
        "    # i is the current page, sparknotes counts most of them in even numbers\n",
        "    # which will be the training set while odd numbers will be used for the test set\n",
        "    if parity == \"even\" :\n",
        "        i=2\n",
        "    if parity == \"odd pt1\" :\n",
        "        i=3\n",
        "    if parity == \"odd pt2\" :\n",
        "        i=269\n",
        "        \n",
        "    print(\"Reading {0} from https://www.sparknotes.com/nofear/shakespeare/{0}/ ...\".format(play))\n",
        "    # makes a text file to which to copy <div> contents\n",
        "    with open('{0}.txt'.format(play),\"w+\",encoding=\"utf-8\")  as fp:\n",
        "        \n",
        "        # limit set at 500 pages, just to prevent infinite loop\n",
        "        #    in case break statement not triggered\n",
        "        while i <= 1000 :\n",
        "        \n",
        "            # 200 is the successful access status code. 300 are redirects\n",
        "            # and above is garbage, non-200 code means the page doesn't\n",
        "            # exist or is unreachable\n",
        "            head = requests.head(\"https://www.sparknotes.com/nofear/shakespeare/{0}/page_{1}/\".format(play,i))\n",
        "            if head.status_code >= 300:\n",
        "                print(\"End of visible play reached, {0} is last visible page at i={1}\".format(int(i/2),i))\n",
        "                break\n",
        "                \n",
        "            # start reading html content\n",
        "            # some <div>s contain linebreaks, killbreaks() gets rid of them\n",
        "            with urllib.request.urlopen(\"https://www.sparknotes.com/nofear/shakespeare/{0}/page_{1}/\".format(play,i)) as page:\n",
        "                soup = BeautifulSoup(page)\n",
        "                table = soup.find(\"table\")\n",
        "                rows = table.findAll(\"tr\")\n",
        "                for row in rows:\n",
        "                    for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--original\"}):\n",
        "                        for div in td.find_all(\"div\"):\n",
        "                            fp.write(killbreaks(div.text)+\" \")\n",
        "                        fp.write(\"\\n\")\n",
        "                    for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--modern\"}):\n",
        "                        for div in td.find_all(\"div\"):\n",
        "                            fp.write(killbreaks(div.text)+\" \")\n",
        "                        fp.write(\"\\n\\n\")\n",
        "            \n",
        "            if i%50 == 0:\n",
        "                print(\"Reached page {0} at i={1},\".format(int(i/2),i))\n",
        "            \n",
        "            i+=2\n",
        "            \n",
        "        print('\\'{0}\\' is read.'.format(play))"
      ],
      "id": "zV-FNjiKMH9Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_1ZL0Du3MH9R",
        "outputId": "4ac416ff-1f43-45d6-d959-3fa92299a9b1"
      },
      "source": [
        "# Plays\n",
        "sparknotesScraper('hamlet', \"even\")\n",
        "sparknotesScraper('macbeth', \"even\")\n",
        "sparknotesScraper('romeojuliet', \"even\")\n",
        "sparknotesScraper('lear', \"even\")\n",
        "sparknotesScraper('juliuscaesar', \"even\")\n",
        "sparknotesScraper('henryv', \"even\")\n",
        "sparknotesScraper('coriolanus', \"even\")\n",
        "sparknotesScraper('asyoulikeit', \"even\")\n",
        "sparknotesScraper('antony-and-cleopatra', \"even\")\n",
        "sparknotesScraper('measure-for-measure', \"even\")\n",
        "sparknotesScraper('errors', \"even\")\n",
        "sparknotesScraper('merchant', \"even\")\n",
        "sparknotesScraper('msnd', \"even\")\n",
        "sparknotesScraper('othello', \"even\")\n",
        "sparknotesScraper('richardii', \"even\")\n",
        "sparknotesScraper('richardiii', \"even\")\n",
        "sparknotesScraper('shrew', \"even\")\n",
        "sparknotesScraper('tempest', \"even\")\n",
        "sparknotesScraper('twelfthnight', \"even\")\n",
        "sparknotesScraper('twogentlemen', \"even\")\n",
        "sparknotesScraper('winterstale', \"even\")"
      ],
      "id": "_1ZL0Du3MH9R",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading hamlet from https://www.sparknotes.com/nofear/shakespeare/hamlet/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "Reached page 150 at i=300,\n",
            "End of visible play reached, 169 is last visible page at i=338\n",
            "'hamlet' is read.\n",
            "Reading macbeth from https://www.sparknotes.com/nofear/shakespeare/macbeth/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "End of visible play reached, 110 is last visible page at i=220\n",
            "'macbeth' is read.\n",
            "Reading romeojuliet from https://www.sparknotes.com/nofear/shakespeare/romeojuliet/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "End of visible play reached, 144 is last visible page at i=288\n",
            "'romeojuliet' is read.\n",
            "Reading lear from https://www.sparknotes.com/nofear/shakespeare/lear/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "Reached page 150 at i=300,\n",
            "End of visible play reached, 156 is last visible page at i=312\n",
            "'lear' is read.\n",
            "Reading juliuscaesar from https://www.sparknotes.com/nofear/shakespeare/juliuscaesar/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "End of visible play reached, 120 is last visible page at i=240\n",
            "'juliuscaesar' is read.\n",
            "Reading henry4pt1 from https://www.sparknotes.com/nofear/shakespeare/henry4pt1/ ...\n",
            "End of visible play reached, 1 is last visible page at i=2\n",
            "'henry4pt1' is read.\n",
            "Reading henry4pt2 from https://www.sparknotes.com/nofear/shakespeare/henry4pt2/ ...\n",
            "End of visible play reached, 1 is last visible page at i=2\n",
            "'henry4pt2' is read.\n",
            "Reading henryv from https://www.sparknotes.com/nofear/shakespeare/henryv/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "End of visible play reached, 139 is last visible page at i=278\n",
            "'henryv' is read.\n",
            "Reading coriolanus from https://www.sparknotes.com/nofear/shakespeare/coriolanus/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "Reached page 150 at i=300,\n",
            "End of visible play reached, 168 is last visible page at i=336\n",
            "'coriolanus' is read.\n",
            "Reading asyoulikeit from https://www.sparknotes.com/nofear/shakespeare/asyoulikeit/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "End of visible play reached, 122 is last visible page at i=244\n",
            "'asyoulikeit' is read.\n",
            "Reading antony-and-cleopatra from https://www.sparknotes.com/nofear/shakespeare/antony-and-cleopatra/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "Reached page 150 at i=300,\n",
            "Reached page 175 at i=350,\n",
            "End of visible play reached, 177 is last visible page at i=354\n",
            "'antony-and-cleopatra' is read.\n",
            "Reading measure-for-measure from https://www.sparknotes.com/nofear/shakespeare/measure-for-measure/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "End of visible play reached, 128 is last visible page at i=256\n",
            "'measure-for-measure' is read.\n",
            "Reading errors from https://www.sparknotes.com/nofear/shakespeare/errors/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "End of visible play reached, 82 is last visible page at i=164\n",
            "'errors' is read.\n",
            "Reading merchant from https://www.sparknotes.com/nofear/shakespeare/merchant/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "End of visible play reached, 116 is last visible page at i=232\n",
            "'merchant' is read.\n",
            "Reading msnd from https://www.sparknotes.com/nofear/shakespeare/msnd/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "End of visible play reached, 91 is last visible page at i=182\n",
            "'msnd' is read.\n",
            "Reading othello from https://www.sparknotes.com/nofear/shakespeare/othello/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "Reached page 150 at i=300,\n",
            "End of visible play reached, 154 is last visible page at i=308\n",
            "'othello' is read.\n",
            "Reading richardii from https://www.sparknotes.com/nofear/shakespeare/richardii/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "End of visible play reached, 117 is last visible page at i=234\n",
            "'richardii' is read.\n",
            "Reading richardiii from https://www.sparknotes.com/nofear/shakespeare/richardiii/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "Reached page 150 at i=300,\n",
            "End of visible play reached, 172 is last visible page at i=344\n",
            "'richardiii' is read.\n",
            "Reading shrew from https://www.sparknotes.com/nofear/shakespeare/shrew/ ...\n",
            "End of visible play reached, 23 is last visible page at i=46\n",
            "'shrew' is read.\n",
            "Reading tempest from https://www.sparknotes.com/nofear/shakespeare/tempest/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "End of visible play reached, 102 is last visible page at i=204\n",
            "'tempest' is read.\n",
            "Reading twelfthnight from https://www.sparknotes.com/nofear/shakespeare/twelfthnight/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "End of visible play reached, 122 is last visible page at i=244\n",
            "'twelfthnight' is read.\n",
            "Reading twogentlemen from https://www.sparknotes.com/nofear/shakespeare/twogentlemen/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "End of visible play reached, 95 is last visible page at i=190\n",
            "'twogentlemen' is read.\n",
            "Reading winterstale from https://www.sparknotes.com/nofear/shakespeare/winterstale/ ...\n",
            "Reached page 25 at i=50,\n",
            "Reached page 50 at i=100,\n",
            "Reached page 75 at i=150,\n",
            "Reached page 100 at i=200,\n",
            "Reached page 125 at i=250,\n",
            "End of visible play reached, 141 is last visible page at i=282\n",
            "'winterstale' is read.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TPiU8Q2MH9R"
      },
      "source": [
        "# store all the even indexed plays in one file for trainind\n",
        "filenames = ['hamlet.txt', 'romeojuliet.txt', 'lear.txt', 'juliuscaesar.txt', 'henryv.txt', 'coriolanus.txt', 'errors.txt', 'asyoulikeit.txt','antony-and-cleopatra.txt', 'measure-for-measure.txt', 'merchant.txt', 'msnd.txt', 'othello.txt', 'richardii.txt', 'richardiii.txt', 'shrew.txt', 'tempest.txt', 'twelfthnight.txt', 'twogentlemen.txt', 'winterstale.txt']\n",
        "with open('trainingSet.txt', 'w', encoding=\"utf-8\") as outfile:\n",
        "    for fname in filenames:\n",
        "        with open(fname, encoding=\"utf-8\") as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)"
      ],
      "id": "7TPiU8Q2MH9R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k3gzV5tRMH9S"
      },
      "source": [
        "Preprocessing"
      ],
      "id": "k3gzV5tRMH9S"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iu56r1MiMH9T"
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "    w = w.strip()\n",
        "\n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w\n",
        "\n",
        "def load_document(filename):\n",
        "    #open the file as read only\n",
        "    file = open(filename, mode='rt', encoding='utf-8')\n",
        "    #store in variable  \n",
        "    text=file.read()\n",
        "    #close it\n",
        "    file.close\n",
        "    return text\n",
        "\n",
        "def make_pairs(document):\n",
        "    #group the corresponding lines\n",
        "    to_be_paired = document.split('\\n\\n')\n",
        "    #pair the corresponding translations\n",
        "    line_pairs = [line.split('\\n') for line in to_be_paired]\n",
        "    return line_pairs\n",
        "\n",
        "#---dataset cleaning------\n",
        "##Note that this is partially done by the tokenizer already\n",
        "##however, we implemented our own version Before using that\n",
        "##tokenizer, so we keep both for compatibility just in case\n",
        "def clean(pairs, num_sentences):\n",
        "    if num_sentences == 'all':\n",
        "        num_sentences = len(pairs)-1\n",
        "    cleaned= []\n",
        "    # regex for char filtering\n",
        "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
        "    for pair in pairs[:num_sentences]:\n",
        "        clean_pair = [] \n",
        "        for line in pair:\n",
        "            # normalize unicode characters\n",
        "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
        "            line = line.decode('UTF-8')\n",
        "            # tokenize on white space\n",
        "            line = line.split()\n",
        "            # convert to lowercase\n",
        "            line = [word.lower() for word in line]\n",
        "            # remove punctuation from each token\n",
        "            line = [re_print.sub('', word) for word in line]\n",
        "            # remove tokens with numbers in them\n",
        "            line = [word.translate(str.maketrans('', '', digits)) for word in line]\n",
        "            # add space before punctuation\n",
        "            line = [re.sub('([.,!?()])', r' \\1 ', word) for word in line]\n",
        "            # remove extra spaces\n",
        "            line = [re.sub(' +', ' ', word) for word in line]\n",
        "            # store as string\n",
        "            clean_pair.append(' '.join(line))\n",
        "        # keep only pairs of 2\n",
        "        if len(clean_pair) == 2:\n",
        "            # split into sentences instead of lines\n",
        "            if len(nltk.tokenize.sent_tokenize(clean_pair[0])) > 1 :\n",
        "                first = nltk.tokenize.sent_tokenize(clean_pair[0])\n",
        "                second = nltk.tokenize.sent_tokenize(clean_pair[1])\n",
        "                # add a start and an end token to the sentences\n",
        "                first = ['<start> ' + sentence + ' <end>' for sentence in first]\n",
        "                second = ['<start> ' + sentence + ' <end>' for sentence in second]\n",
        "                length = min(len(first), len(second))\n",
        "                i=0\n",
        "                while i < length :\n",
        "                # keep only short sentences and verify the lengths are about the same\n",
        "                    if len(first[i]) < max_length +15 and (2.5*len(first[i]) > len(second[i]) or 2.5*len(first[i]) > len(second[i])):\n",
        "                         cleaned.append([first[i],second[i]])\n",
        "                    i+=1\n",
        "            else :\n",
        "                # keep only short sentences\n",
        "                if len(clean_pair[0]) < max_length :\n",
        "                    # add a start and an end token to the sentences\n",
        "                    clean_pair = ['<start> ' + sentence + ' <end>' for sentence in clean_pair]\n",
        "                    cleaned.append(clean_pair)\n",
        "    #print(cleaned)\n",
        "    return zip(*cleaned)\n",
        "\n",
        "def load_dataset(path, num_sentences):\n",
        "    # load dataset\n",
        "    doc = load_document(path)\n",
        "    # split into pairs\n",
        "    pairs = make_pairs(doc)\n",
        "    #clean sentences\n",
        "    cleaned_pairs = clean(pairs, num_sentences)\n",
        "    # associate sentences to their respective language \n",
        "    shakespeare, modern = cleaned_pairs\n",
        "    # make a vocabulary and fit the tokenizer on it\n",
        "    vocabularySource = shakespeare + modern\n",
        "    tokenizer.fit_on_texts(vocabularySource)\n",
        "    # vectorize\n",
        "    modern_vector = pad(tokenize(modern))\n",
        "    shakespeare_vector = pad(tokenize(shakespeare))\n",
        "    \n",
        "    return modern_vector, shakespeare_vector, vocabularySource\n",
        "\n",
        "def pad(language_vector):\n",
        "    for sentence in language_vector:\n",
        "        # add 0s to sentence vector until it is the same length as max_length\n",
        "        for i in range(len(sentence), max_length):\n",
        "            sentence.append(0) \n",
        "    return language_vector\n",
        "\n",
        "def tokenize(array):\n",
        "    token = tokenizer.texts_to_sequences(array)\n",
        "    return token"
      ],
      "id": "Iu56r1MiMH9T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5T5RCooiQVLm",
        "outputId": "818106b3-f5bc-4c86-c4ac-ef3ce06ae517"
      },
      "source": [
        "# uncomment this cell to mount drive\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ],
      "id": "5T5RCooiQVLm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVQD8LL4MH9T"
      },
      "source": [
        "# run this line if you are using jupyter\n",
        "modern_vector, shakespeare_vector, vocabularySource = load_dataset('trainingSet.txt', 'all')\n",
        "# uncomment this line instead if you are using drive\n",
        "#modern_vector, shakespeare_vector, vocabularySource = load_dataset('/content/gdrive/MyDrive/text_files/trainingSet.txt', 'all')"
      ],
      "id": "ZVQD8LL4MH9T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tuub-5IjMH9T"
      },
      "source": [
        "-------------------------------------------\n",
        "The vast majority of what follows is unoriginal -- we were authorized to make use of pre-built libraries such as scikit-learn, but to our knowledge, there exists no ready-made transformer translator, and we were (rather unsurprisingly) unable to create our own.\n",
        "\n",
        "The principal source is thus: https://www.tensorflow.org/tutorials/text/transformer\n",
        "\n",
        "The modifications, where they occur, are mostly for dataset compatibility -- ours and the tutorials' were different and we did not have the same capabilities or built-in functions such as tokenizers."
      ],
      "id": "Tuub-5IjMH9T"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCwM7knmMH9T"
      },
      "source": [
        "Positional Encoding"
      ],
      "id": "OCwM7knmMH9T"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TQ1RQjOMH9T",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "98be116f-1fd7-4095-9fa6-5c9886771e40"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "    angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "    return pos * angle_rates\n",
        "def positional_encoding(position, d_model):\n",
        "    angle_rads = get_angles(np.arange(position)[:, np.newaxis],np.arange(d_model)[np.newaxis, :],d_model)\n",
        "    # apply sin to even indices in the array; 2i\n",
        "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "    # apply cos to odd indices in the array; 2i+1\n",
        "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "    pos_encoding = angle_rads[np.newaxis, ...]\n",
        "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
        "\n",
        "n, d = 2048, 512\n",
        "pos_encoding = positional_encoding(n, d)\n",
        "print(pos_encoding.shape)\n",
        "pos_encoding = pos_encoding[0]\n",
        "\n",
        "# Juggle the dimensions for the plot\n",
        "pos_encoding = tf.reshape(pos_encoding, (n, d//2, 2))\n",
        "pos_encoding = tf.transpose(pos_encoding, (2,1,0))\n",
        "pos_encoding = tf.reshape(pos_encoding, (d, n))\n",
        "\n",
        "plt.pcolormesh(pos_encoding, cmap='RdBu')\n",
        "plt.ylabel('Depth')\n",
        "plt.xlabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "id": "_TQ1RQjOMH9T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1, 2048, 512)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6zIsuGYMH9U"
      },
      "source": [
        "Padding Mask"
      ],
      "id": "v6zIsuGYMH9U"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mv1Tg3RJMH9V"
      },
      "source": [
        "def create_padding_mask(seq):\n",
        "    seq = tf.cast(tf.math.equal(seq, -1), tf.float32)\n",
        "    # add extra dimensions to add the padding to the attention logits.\n",
        "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
        "\n",
        "def create_look_ahead_mask(size):\n",
        "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
        "    return mask  # (seq_len, seq_len)"
      ],
      "id": "mv1Tg3RJMH9V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBVrQ6haMH9V"
      },
      "source": [
        "Attention"
      ],
      "id": "QBVrQ6haMH9V"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPL-q89mMH9V"
      },
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "    \"\"\"Calculate the attention weights.\n",
        "      q, k, v must have matching leading dimensions.\n",
        "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "      The mask has different shapes depending on its type(padding or look ahead) \n",
        "      but it must be broadcastable for addition.\n",
        "\n",
        "      Args:\n",
        "        q: query shape == (..., seq_len_q, depth)\n",
        "        k: key shape == (..., seq_len_k, depth)\n",
        "        v: value shape == (..., seq_len_v, depth_v)\n",
        "        mask: Float tensor with shape broadcastable \n",
        "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "      Returns:\n",
        "        output, attention_weights\n",
        "    \"\"\"\n",
        "\n",
        "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    # scale matmul_qk\n",
        "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "    # add the mask to the scaled tensor.\n",
        "    if mask is not None:\n",
        "        scaled_attention_logits += (mask * -1e9)  \n",
        "\n",
        "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "    # add up to 1.\n",
        "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "    return output, attention_weights"
      ],
      "id": "lPL-q89mMH9V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9IQQLDpMH9V"
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        assert d_model % self.num_heads == 0\n",
        "\n",
        "        self.depth = d_model // self.num_heads\n",
        "\n",
        "        self.wq = tf.keras.layers.Dense(d_model)\n",
        "        self.wk = tf.keras.layers.Dense(d_model)\n",
        "        self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "        self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    def split_heads(self, x, batch_size):\n",
        "        \"\"\"Split the last dimension into (num_heads, depth).\n",
        "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
        "        \"\"\"\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
        "\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "        return output, attention_weights"
      ],
      "id": "D9IQQLDpMH9V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SRQy2fnZMH9V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8301b59-4332-4fda-d37f-0ef66dcd1172"
      },
      "source": [
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((1, 60, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out, attn = temp_mha(y, k=y, q=y, mask=None)\n",
        "out.shape, attn.shape"
      ],
      "id": "SRQy2fnZMH9V",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([1, 60, 512]), TensorShape([1, 8, 60, 60]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r76ueoJUMH9W"
      },
      "source": [
        "Encoder and Decoder Layers"
      ],
      "id": "r76ueoJUMH9W"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dui8VlQrMH9W"
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "    return tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "    ])"
      ],
      "id": "Dui8VlQrMH9W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t95_HE4FMH9W"
      },
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        return out2"
      ],
      "id": "t95_HE4FMH9W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xN2YsWPMH9X"
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
        "\n",
        "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn1 = self.dropout1(attn1, training=training)\n",
        "        out1 = self.layernorm1(attn1 + x)\n",
        "\n",
        "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
        "        attn2 = self.dropout2(attn2, training=training)\n",
        "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
        "        ffn_output = self.dropout3(ffn_output, training=training)\n",
        "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
        "\n",
        "        return out3, attn_weights_block1, attn_weights_block2"
      ],
      "id": "0xN2YsWPMH9X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCYyHITSMH9X"
      },
      "source": [
        "Encoder and Decoder"
      ],
      "id": "GCYyHITSMH9X"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nd6g3AdzMH9X"
      },
      "source": [
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
        "\n",
        "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
        "\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, training, mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "\n",
        "        # adding embedding and position encoding.\n",
        "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.enc_layers[i](x, training, mask)\n",
        "\n",
        "        return x  # (batch_size, input_seq_len, d_model)"
      ],
      "id": "Nd6g3AdzMH9X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjx4Y8LAMH9X"
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
        "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
        "\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                           for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        attention_weights = {}\n",
        "\n",
        "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
        "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "\n",
        "        x = self.dropout(x, training=training)\n",
        "\n",
        "        for i in range(self.num_layers):\n",
        "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
        "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
        "\n",
        "        # x.shape == (batch_size, target_seq_len, d_model)\n",
        "        return x, attention_weights"
      ],
      "id": "vjx4Y8LAMH9X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wZ9_JcrMH9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c0cfdc-b40a-4fee-9c80-89aacd6f4a39"
      },
      "source": [
        "sample_encoder = Encoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, input_vocab_size=8500,\n",
        "                         maximum_position_encoding=10000)\n",
        "temp_input = tf.random.uniform((64, 62), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "sample_encoder_output = sample_encoder(temp_input, training=False, mask=None)\n",
        "\n",
        "print (sample_encoder_output.shape)  # (batch_size, input_seq_len, d_model)"
      ],
      "id": "-wZ9_JcrMH9Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 62, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5m3gdOQMH9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b1eb3ff-4a34-40cd-c946-0c8d89ce380d"
      },
      "source": [
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048, target_vocab_size=8000,\n",
        "                         maximum_position_encoding=5000)\n",
        "temp_input = tf.random.uniform((64, 26), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "output, attn = sample_decoder(temp_input, \n",
        "                              enc_output=sample_encoder_output, \n",
        "                              training=False,\n",
        "                              look_ahead_mask=None, \n",
        "                              padding_mask=None)\n",
        "\n",
        "output.shape, attn['decoder_layer2_block2'].shape"
      ],
      "id": "o5m3gdOQMH9Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 26, 512]), TensorShape([64, 8, 26, 62]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mTzdX5A5MH9Y"
      },
      "source": [
        "Transformer model"
      ],
      "id": "mTzdX5A5MH9Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yb6_Ll1CMH9Y"
      },
      "source": [
        "class Transformer(tf.keras.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,  input_vocab_size, pe_input, rate)\n",
        "\n",
        "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
        "\n",
        "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
        "\n",
        "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
        "\n",
        "        enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
        "\n",
        "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
        "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
        "\n",
        "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
        "\n",
        "        return final_output, attention_weights"
      ],
      "id": "Yb6_Ll1CMH9Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEmiaJHUMH9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b378e4e-54e7-4c9a-c1c1-ad5d06f3834d"
      },
      "source": [
        "sample_transformer = Transformer(\n",
        "    num_layers=2, d_model=512, num_heads=8, dff=2048, \n",
        "    input_vocab_size=8500, target_vocab_size=8000, \n",
        "    pe_input=10000, pe_target=6000)\n",
        "\n",
        "temp_input = tf.random.uniform((64, 38), dtype=tf.int64, minval=0, maxval=200)\n",
        "temp_target = tf.random.uniform((64, 36), dtype=tf.int64, minval=0, maxval=200)\n",
        "\n",
        "fn_out, _ = sample_transformer(temp_input, temp_target, training=False, \n",
        "                               enc_padding_mask=None, \n",
        "                               look_ahead_mask=None,\n",
        "                               dec_padding_mask=None)\n",
        "\n",
        "fn_out.shape  # (batch_size, tar_seq_len, target_vocab_size)"
      ],
      "id": "TEmiaJHUMH9Y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([64, 36, 8000])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjomNer_MH9Z"
      },
      "source": [
        "#Settings/parameters and model initialization\n",
        "#While mostly arbitrary, these hyperparameters\n",
        "#were inspired by the Attention is All You Need\n",
        "#paper\n",
        "num_layers = 6\n",
        "d_model = 128\n",
        "dff = 1024\n",
        "num_heads = 16\n",
        "dropout_rate = 0.2\n",
        "\n",
        "transformer = Transformer( num_layers=num_layers,\n",
        "                           d_model=d_model,\n",
        "                           num_heads=num_heads,\n",
        "                           dff=dff,\n",
        "                           input_vocab_size=len(tokenizer.word_index)+1,\n",
        "                           target_vocab_size=len(tokenizer.word_index)+1, \n",
        "                           pe_input=1000, \n",
        "                           pe_target=1000,\n",
        "                           rate=dropout_rate)"
      ],
      "id": "NjomNer_MH9Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXGUxLHqMH9Z"
      },
      "source": [
        "Schedule"
      ],
      "id": "mXGUxLHqMH9Z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FAYRvnPMH9Z"
      },
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "    def __init__(self, d_model, warmup_steps=4000):\n",
        "        super(CustomSchedule, self).__init__()\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "\n",
        "    def __call__(self, step):\n",
        "        arg1 = tf.math.rsqrt(step)\n",
        "        arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
        "    \n",
        "learning_rate = CustomSchedule(d_model)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ],
      "id": "1FAYRvnPMH9Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAJbr_6WMH9Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "1bea080b-cb89-4051-cc7f-0793bc50d340"
      },
      "source": [
        "learning_rate = CustomSchedule(d_model)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, \n",
        "                                     epsilon=1e-9)\n",
        "temp_learning_rate_schedule = CustomSchedule(d_model)\n",
        "\n",
        "plt.plot(temp_learning_rate_schedule(tf.range(40000, dtype=tf.float32)))\n",
        "plt.ylabel(\"Learning Rate\")\n",
        "plt.xlabel(\"Train Step\")"
      ],
      "id": "PAJbr_6WMH9Z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0, 'Train Step')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xj577DMEMH9Z"
      },
      "source": [
        "Loss and accuracy functions "
      ],
      "id": "xj577DMEMH9Z"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXpZE819MH9Z"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "\n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
        "\n",
        "def accuracy_function(real, pred):\n",
        "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
        "\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    accuracies = tf.math.logical_and(mask, accuracies)\n",
        "\n",
        "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
        "    mask = tf.cast(mask, dtype=tf.float32)\n",
        "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ],
      "id": "FXpZE819MH9Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOZolh3pMH9a"
      },
      "source": [
        "def create_masks(inp, tar):\n",
        "    # Encoder padding mask\n",
        "    enc_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 2nd attention block in the decoder.\n",
        "    # This padding mask is used to mask the encoder outputs.\n",
        "    dec_padding_mask = create_padding_mask(inp)\n",
        "\n",
        "    # Used in the 1st attention block in the decoder.\n",
        "    # It is used to pad and mask future tokens in the input received by \n",
        "    # the decoder.\n",
        "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
        "    dec_target_padding_mask = create_padding_mask(tar)\n",
        "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
        "\n",
        "    return enc_padding_mask, combined_mask, dec_padding_mask"
      ],
      "id": "rOZolh3pMH9a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrdOo-9gMH9b"
      },
      "source": [
        "tensormodEngENC = tf.convert_to_tensor(modern_vector, dtype=tf.int64)\n",
        "tensorshakeEngENC = tf.convert_to_tensor(shakespeare_vector, dtype=tf.int64)\n",
        "dataset = tf.data.Dataset.from_tensor_slices((tensormodEngENC,tensorshakeEngENC))\n",
        "\n",
        "#This function is technically useless, but included for\n",
        "#compatibility -- the tutorial source we used does more\n",
        "#with it, but it is not needed in our implementation.\n",
        "#Unfortunately, it is called in a complicated (for us) way.\n",
        "def tokenize_pairs(sha, mod):\n",
        "    return sha, mod"
      ],
      "id": "VrdOo-9gMH9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTnZBE95MH9b"
      },
      "source": [
        "#Partition dataset into batches for training\n",
        "BUFFER_SIZE = 20000\n",
        "BATCH_SIZE = 100\n",
        "def make_batches(ds):\n",
        "    return (\n",
        "        ds\n",
        "        .cache()\n",
        "        .shuffle(BUFFER_SIZE)\n",
        "        .batch(BATCH_SIZE)\n",
        "        .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "        .prefetch(tf.data.AUTOTUNE))\n",
        "\n",
        "dset = make_batches(dataset)"
      ],
      "id": "DTnZBE95MH9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdlWh_oMMH9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88f2bf6e-0f30-4b33-d1b5-b928d01e2a7f"
      },
      "source": [
        "#Weights are saved as checkpoints\n",
        "checkpoint_path = \"/content/gdrive/MyDrive/checkpoints_new/train\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')"
      ],
      "id": "bdlWh_oMMH9b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Latest checkpoint restored!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmGmpBDzMH9b"
      },
      "source": [
        "Training - do not run"
      ],
      "id": "FmGmpBDzMH9b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xggGuzh1MH9b"
      },
      "source": [
        "#Setup for training\n",
        "EPOCHS = 600   # <<<================||X+\n",
        "\n",
        "train_step_signature = [\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
        "]\n",
        "\n",
        "@tf.function(input_signature=train_step_signature)\n",
        "def train_step(inp, tar):\n",
        "    tar_inp = tar[:, :-1]\n",
        "    tar_real = tar[:, 1:]\n",
        "\n",
        "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        predictions, _ = transformer(inp, tar_inp, \n",
        "                                         True, \n",
        "                                         enc_padding_mask, \n",
        "                                         combined_mask, \n",
        "                                         dec_padding_mask)\n",
        "        loss = loss_function(tar_real, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
        "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
        "\n",
        "    train_loss(loss)\n",
        "    train_accuracy(accuracy_function(tar_real, predictions))"
      ],
      "id": "xggGuzh1MH9b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiKZMfGrMH9b",
        "outputId": "2320a2f2-5a71-482a-a51d-50f2e6e577b4"
      },
      "source": [
        "# --- T R A I N I N G --- #\n",
        "# GPUs BEWARE\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    # inp -> english, tar -> shakespearean\n",
        "    for (batch, (inp, tar)) in enumerate(dset):\n",
        "        train_step(inp, tar)\n",
        "\n",
        "        if batch % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
        "\n",
        "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
      ],
      "id": "QiKZMfGrMH9b",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 0.2870 Accuracy 0.9255\n",
            "Epoch 1 Batch 50 Loss 0.2689 Accuracy 0.9282\n",
            "Epoch 1 Batch 100 Loss 0.2788 Accuracy 0.9254\n",
            "Epoch 1 Batch 150 Loss 0.2844 Accuracy 0.9233\n",
            "Epoch 1 Batch 200 Loss 0.2923 Accuracy 0.9209\n",
            "Epoch 1 Batch 250 Loss 0.3005 Accuracy 0.9186\n",
            "Epoch 1 Batch 300 Loss 0.3091 Accuracy 0.9164\n",
            "Epoch 1 Loss 0.3113 Accuracy 0.9160\n",
            "Time taken for 1 epoch: 183.03 secs\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.2621 Accuracy 0.9241\n",
            "Epoch 2 Batch 50 Loss 0.2769 Accuracy 0.9250\n",
            "Epoch 2 Batch 100 Loss 0.2834 Accuracy 0.9223\n",
            "Epoch 2 Batch 150 Loss 0.2890 Accuracy 0.9209\n",
            "Epoch 2 Batch 200 Loss 0.2937 Accuracy 0.9196\n",
            "Epoch 2 Batch 250 Loss 0.3010 Accuracy 0.9175\n",
            "Epoch 2 Batch 300 Loss 0.3084 Accuracy 0.9157\n",
            "Epoch 2 Loss 0.3120 Accuracy 0.9148\n",
            "Time taken for 1 epoch: 182.62 secs\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2431 Accuracy 0.9276\n",
            "Epoch 3 Batch 50 Loss 0.2760 Accuracy 0.9246\n",
            "Epoch 3 Batch 100 Loss 0.2831 Accuracy 0.9224\n",
            "Epoch 3 Batch 150 Loss 0.2896 Accuracy 0.9206\n",
            "Epoch 3 Batch 200 Loss 0.2967 Accuracy 0.9188\n",
            "Epoch 3 Batch 250 Loss 0.3023 Accuracy 0.9173\n",
            "Epoch 3 Batch 300 Loss 0.3087 Accuracy 0.9156\n",
            "Epoch 3 Loss 0.3115 Accuracy 0.9151\n",
            "Time taken for 1 epoch: 182.43 secs\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.2501 Accuracy 0.9291\n",
            "Epoch 4 Batch 50 Loss 0.2754 Accuracy 0.9261\n",
            "Epoch 4 Batch 100 Loss 0.2810 Accuracy 0.9235\n",
            "Epoch 4 Batch 150 Loss 0.2877 Accuracy 0.9218\n",
            "Epoch 4 Batch 200 Loss 0.2934 Accuracy 0.9199\n",
            "Epoch 4 Batch 250 Loss 0.2992 Accuracy 0.9179\n",
            "Epoch 4 Batch 300 Loss 0.3075 Accuracy 0.9159\n",
            "Epoch 4 Loss 0.3107 Accuracy 0.9150\n",
            "Time taken for 1 epoch: 182.41 secs\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.2473 Accuracy 0.9434\n",
            "Epoch 5 Batch 50 Loss 0.2725 Accuracy 0.9256\n",
            "Epoch 5 Batch 100 Loss 0.2790 Accuracy 0.9236\n",
            "Epoch 5 Batch 150 Loss 0.2850 Accuracy 0.9219\n",
            "Epoch 5 Batch 200 Loss 0.2928 Accuracy 0.9198\n",
            "Epoch 5 Batch 250 Loss 0.2991 Accuracy 0.9180\n",
            "Epoch 5 Batch 300 Loss 0.3081 Accuracy 0.9158\n",
            "Saving checkpoint for epoch 5 at ./checkpoints/train\\ckpt-121\n",
            "Epoch 5 Loss 0.3121 Accuracy 0.9147\n",
            "Time taken for 1 epoch: 183.53 secs\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2865 Accuracy 0.9214\n",
            "Epoch 6 Batch 50 Loss 0.2709 Accuracy 0.9260\n",
            "Epoch 6 Batch 100 Loss 0.2818 Accuracy 0.9223\n",
            "Epoch 6 Batch 150 Loss 0.2851 Accuracy 0.9212\n",
            "Epoch 6 Batch 200 Loss 0.2926 Accuracy 0.9193\n",
            "Epoch 6 Batch 250 Loss 0.2997 Accuracy 0.9172\n",
            "Epoch 6 Batch 300 Loss 0.3087 Accuracy 0.9152\n",
            "Epoch 6 Loss 0.3115 Accuracy 0.9146\n",
            "Time taken for 1 epoch: 182.78 secs\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2677 Accuracy 0.9314\n",
            "Epoch 7 Batch 50 Loss 0.2725 Accuracy 0.9264\n",
            "Epoch 7 Batch 100 Loss 0.2801 Accuracy 0.9233\n",
            "Epoch 7 Batch 150 Loss 0.2865 Accuracy 0.9216\n",
            "Epoch 7 Batch 200 Loss 0.2940 Accuracy 0.9195\n",
            "Epoch 7 Batch 250 Loss 0.3003 Accuracy 0.9180\n",
            "Epoch 7 Batch 300 Loss 0.3080 Accuracy 0.9162\n",
            "Epoch 7 Loss 0.3112 Accuracy 0.9155\n",
            "Time taken for 1 epoch: 182.34 secs\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.2578 Accuracy 0.9346\n",
            "Epoch 8 Batch 50 Loss 0.2729 Accuracy 0.9246\n",
            "Epoch 8 Batch 100 Loss 0.2828 Accuracy 0.9215\n",
            "Epoch 8 Batch 150 Loss 0.2862 Accuracy 0.9214\n",
            "Epoch 8 Batch 200 Loss 0.2913 Accuracy 0.9199\n",
            "Epoch 8 Batch 250 Loss 0.2988 Accuracy 0.9180\n",
            "Epoch 8 Batch 300 Loss 0.3070 Accuracy 0.9157\n",
            "Epoch 8 Loss 0.3106 Accuracy 0.9149\n",
            "Time taken for 1 epoch: 183.11 secs\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.2503 Accuracy 0.9295\n",
            "Epoch 9 Batch 50 Loss 0.2657 Accuracy 0.9278\n",
            "Epoch 9 Batch 100 Loss 0.2750 Accuracy 0.9241\n",
            "Epoch 9 Batch 150 Loss 0.2817 Accuracy 0.9222\n",
            "Epoch 9 Batch 200 Loss 0.2891 Accuracy 0.9205\n",
            "Epoch 9 Batch 250 Loss 0.2969 Accuracy 0.9188\n",
            "Epoch 9 Batch 300 Loss 0.3054 Accuracy 0.9166\n",
            "Epoch 9 Loss 0.3092 Accuracy 0.9157\n",
            "Time taken for 1 epoch: 182.38 secs\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.2448 Accuracy 0.9404\n",
            "Epoch 10 Batch 50 Loss 0.2782 Accuracy 0.9238\n",
            "Epoch 10 Batch 100 Loss 0.2826 Accuracy 0.9236\n",
            "Epoch 10 Batch 150 Loss 0.2884 Accuracy 0.9216\n",
            "Epoch 10 Batch 200 Loss 0.2944 Accuracy 0.9198\n",
            "Epoch 10 Batch 250 Loss 0.3008 Accuracy 0.9181\n",
            "Epoch 10 Batch 300 Loss 0.3092 Accuracy 0.9159\n",
            "Saving checkpoint for epoch 10 at ./checkpoints/train\\ckpt-122\n",
            "Epoch 10 Loss 0.3120 Accuracy 0.9154\n",
            "Time taken for 1 epoch: 183.60 secs\n",
            "\n",
            "Epoch 11 Batch 0 Loss 0.2493 Accuracy 0.9329\n",
            "Epoch 11 Batch 50 Loss 0.2734 Accuracy 0.9246\n",
            "Epoch 11 Batch 100 Loss 0.2790 Accuracy 0.9234\n",
            "Epoch 11 Batch 150 Loss 0.2815 Accuracy 0.9229\n",
            "Epoch 11 Batch 200 Loss 0.2891 Accuracy 0.9207\n",
            "Epoch 11 Batch 250 Loss 0.2980 Accuracy 0.9182\n",
            "Epoch 11 Batch 300 Loss 0.3053 Accuracy 0.9164\n",
            "Epoch 11 Loss 0.3082 Accuracy 0.9156\n",
            "Time taken for 1 epoch: 182.42 secs\n",
            "\n",
            "Epoch 12 Batch 0 Loss 0.2912 Accuracy 0.9204\n",
            "Epoch 12 Batch 50 Loss 0.2791 Accuracy 0.9233\n",
            "Epoch 12 Batch 100 Loss 0.2793 Accuracy 0.9238\n",
            "Epoch 12 Batch 150 Loss 0.2844 Accuracy 0.9221\n",
            "Epoch 12 Batch 200 Loss 0.2911 Accuracy 0.9204\n",
            "Epoch 12 Batch 250 Loss 0.2989 Accuracy 0.9182\n",
            "Epoch 12 Batch 300 Loss 0.3073 Accuracy 0.9163\n",
            "Epoch 12 Loss 0.3103 Accuracy 0.9155\n",
            "Time taken for 1 epoch: 182.59 secs\n",
            "\n",
            "Epoch 13 Batch 0 Loss 0.2586 Accuracy 0.9353\n",
            "Epoch 13 Batch 50 Loss 0.2724 Accuracy 0.9258\n",
            "Epoch 13 Batch 100 Loss 0.2792 Accuracy 0.9238\n",
            "Epoch 13 Batch 150 Loss 0.2840 Accuracy 0.9224\n",
            "Epoch 13 Batch 200 Loss 0.2924 Accuracy 0.9203\n",
            "Epoch 13 Batch 250 Loss 0.2997 Accuracy 0.9182\n",
            "Epoch 13 Batch 300 Loss 0.3078 Accuracy 0.9160\n",
            "Epoch 13 Loss 0.3105 Accuracy 0.9153\n",
            "Time taken for 1 epoch: 183.04 secs\n",
            "\n",
            "Epoch 14 Batch 0 Loss 0.2974 Accuracy 0.9221\n",
            "Epoch 14 Batch 50 Loss 0.2728 Accuracy 0.9246\n",
            "Epoch 14 Batch 100 Loss 0.2756 Accuracy 0.9246\n",
            "Epoch 14 Batch 150 Loss 0.2810 Accuracy 0.9230\n",
            "Epoch 14 Batch 200 Loss 0.2892 Accuracy 0.9206\n",
            "Epoch 14 Batch 250 Loss 0.2970 Accuracy 0.9184\n",
            "Epoch 14 Batch 300 Loss 0.3051 Accuracy 0.9162\n",
            "Epoch 14 Loss 0.3090 Accuracy 0.9152\n",
            "Time taken for 1 epoch: 182.30 secs\n",
            "\n",
            "Epoch 15 Batch 0 Loss 0.2336 Accuracy 0.9362\n",
            "Epoch 15 Batch 50 Loss 0.2738 Accuracy 0.9243\n",
            "Epoch 15 Batch 100 Loss 0.2808 Accuracy 0.9224\n",
            "Epoch 15 Batch 150 Loss 0.2846 Accuracy 0.9214\n",
            "Epoch 15 Batch 200 Loss 0.2904 Accuracy 0.9200\n",
            "Epoch 15 Batch 250 Loss 0.2987 Accuracy 0.9177\n",
            "Epoch 15 Batch 300 Loss 0.3056 Accuracy 0.9159\n",
            "Saving checkpoint for epoch 15 at ./checkpoints/train\\ckpt-123\n",
            "Epoch 15 Loss 0.3089 Accuracy 0.9152\n",
            "Time taken for 1 epoch: 183.43 secs\n",
            "\n",
            "Epoch 16 Batch 0 Loss 0.2695 Accuracy 0.9128\n",
            "Epoch 16 Batch 50 Loss 0.2683 Accuracy 0.9269\n",
            "Epoch 16 Batch 100 Loss 0.2778 Accuracy 0.9242\n",
            "Epoch 16 Batch 150 Loss 0.2838 Accuracy 0.9222\n",
            "Epoch 16 Batch 200 Loss 0.2910 Accuracy 0.9200\n",
            "Epoch 16 Batch 250 Loss 0.2988 Accuracy 0.9181\n",
            "Epoch 16 Batch 300 Loss 0.3060 Accuracy 0.9164\n",
            "Epoch 16 Loss 0.3098 Accuracy 0.9156\n",
            "Time taken for 1 epoch: 182.44 secs\n",
            "\n",
            "Epoch 17 Batch 0 Loss 0.2749 Accuracy 0.9229\n",
            "Epoch 17 Batch 50 Loss 0.2726 Accuracy 0.9247\n",
            "Epoch 17 Batch 100 Loss 0.2760 Accuracy 0.9243\n",
            "Epoch 17 Batch 150 Loss 0.2808 Accuracy 0.9234\n",
            "Epoch 17 Batch 200 Loss 0.2890 Accuracy 0.9208\n",
            "Epoch 17 Batch 250 Loss 0.2972 Accuracy 0.9185\n",
            "Epoch 17 Batch 300 Loss 0.3054 Accuracy 0.9165\n",
            "Epoch 17 Loss 0.3085 Accuracy 0.9157\n",
            "Time taken for 1 epoch: 182.39 secs\n",
            "\n",
            "Epoch 18 Batch 0 Loss 0.2328 Accuracy 0.9350\n",
            "Epoch 18 Batch 50 Loss 0.2687 Accuracy 0.9275\n",
            "Epoch 18 Batch 100 Loss 0.2761 Accuracy 0.9241\n",
            "Epoch 18 Batch 150 Loss 0.2838 Accuracy 0.9220\n",
            "Epoch 18 Batch 200 Loss 0.2909 Accuracy 0.9200\n",
            "Epoch 18 Batch 250 Loss 0.2978 Accuracy 0.9183\n",
            "Epoch 18 Batch 300 Loss 0.3061 Accuracy 0.9163\n",
            "Epoch 18 Loss 0.3085 Accuracy 0.9156\n",
            "Time taken for 1 epoch: 182.33 secs\n",
            "\n",
            "Epoch 19 Batch 0 Loss 0.2888 Accuracy 0.9159\n",
            "Epoch 19 Batch 50 Loss 0.2747 Accuracy 0.9258\n",
            "Epoch 19 Batch 100 Loss 0.2784 Accuracy 0.9238\n",
            "Epoch 19 Batch 150 Loss 0.2854 Accuracy 0.9218\n",
            "Epoch 19 Batch 200 Loss 0.2903 Accuracy 0.9203\n",
            "Epoch 19 Batch 250 Loss 0.2973 Accuracy 0.9183\n",
            "Epoch 19 Batch 300 Loss 0.3059 Accuracy 0.9161\n",
            "Epoch 19 Loss 0.3091 Accuracy 0.9153\n",
            "Time taken for 1 epoch: 182.59 secs\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.3122 Accuracy 0.9125\n",
            "Epoch 20 Batch 50 Loss 0.2716 Accuracy 0.9255\n",
            "Epoch 20 Batch 100 Loss 0.2773 Accuracy 0.9237\n",
            "Epoch 20 Batch 150 Loss 0.2842 Accuracy 0.9218\n",
            "Epoch 20 Batch 200 Loss 0.2899 Accuracy 0.9200\n",
            "Epoch 20 Batch 250 Loss 0.2970 Accuracy 0.9182\n",
            "Epoch 20 Batch 300 Loss 0.3059 Accuracy 0.9160\n",
            "Saving checkpoint for epoch 20 at ./checkpoints/train\\ckpt-124\n",
            "Epoch 20 Loss 0.3088 Accuracy 0.9152\n",
            "Time taken for 1 epoch: 183.93 secs\n",
            "\n",
            "Epoch 21 Batch 0 Loss 0.2870 Accuracy 0.9283\n",
            "Epoch 21 Batch 50 Loss 0.2720 Accuracy 0.9253\n",
            "Epoch 21 Batch 100 Loss 0.2780 Accuracy 0.9240\n",
            "Epoch 21 Batch 150 Loss 0.2829 Accuracy 0.9225\n",
            "Epoch 21 Batch 200 Loss 0.2908 Accuracy 0.9204\n",
            "Epoch 21 Batch 250 Loss 0.2987 Accuracy 0.9184\n",
            "Epoch 21 Batch 300 Loss 0.3067 Accuracy 0.9162\n",
            "Epoch 21 Loss 0.3105 Accuracy 0.9155\n",
            "Time taken for 1 epoch: 182.47 secs\n",
            "\n",
            "Epoch 22 Batch 0 Loss 0.2362 Accuracy 0.9344\n",
            "Epoch 22 Batch 50 Loss 0.2693 Accuracy 0.9270\n",
            "Epoch 22 Batch 100 Loss 0.2763 Accuracy 0.9244\n",
            "Epoch 22 Batch 150 Loss 0.2812 Accuracy 0.9230\n",
            "Epoch 22 Batch 200 Loss 0.2884 Accuracy 0.9212\n",
            "Epoch 22 Batch 250 Loss 0.2956 Accuracy 0.9195\n",
            "Epoch 22 Batch 300 Loss 0.3042 Accuracy 0.9172\n",
            "Epoch 22 Loss 0.3075 Accuracy 0.9165\n",
            "Time taken for 1 epoch: 182.41 secs\n",
            "\n",
            "Epoch 23 Batch 0 Loss 0.2772 Accuracy 0.9330\n",
            "Epoch 23 Batch 50 Loss 0.2651 Accuracy 0.9272\n",
            "Epoch 23 Batch 100 Loss 0.2726 Accuracy 0.9260\n",
            "Epoch 23 Batch 150 Loss 0.2796 Accuracy 0.9238\n",
            "Epoch 23 Batch 200 Loss 0.2877 Accuracy 0.9214\n",
            "Epoch 23 Batch 250 Loss 0.2936 Accuracy 0.9200\n",
            "Epoch 23 Batch 300 Loss 0.3022 Accuracy 0.9180\n",
            "Epoch 23 Loss 0.3050 Accuracy 0.9173\n",
            "Time taken for 1 epoch: 182.46 secs\n",
            "\n",
            "Epoch 24 Batch 0 Loss 0.2450 Accuracy 0.9272\n",
            "Epoch 24 Batch 50 Loss 0.2727 Accuracy 0.9261\n",
            "Epoch 24 Batch 100 Loss 0.2753 Accuracy 0.9254\n",
            "Epoch 24 Batch 150 Loss 0.2824 Accuracy 0.9232\n",
            "Epoch 24 Batch 200 Loss 0.2897 Accuracy 0.9212\n",
            "Epoch 24 Batch 250 Loss 0.2954 Accuracy 0.9195\n",
            "Epoch 24 Batch 300 Loss 0.3042 Accuracy 0.9172\n",
            "Epoch 24 Loss 0.3073 Accuracy 0.9164\n",
            "Time taken for 1 epoch: 182.77 secs\n",
            "\n",
            "Epoch 25 Batch 0 Loss 0.2892 Accuracy 0.9196\n",
            "Epoch 25 Batch 50 Loss 0.2673 Accuracy 0.9271\n",
            "Epoch 25 Batch 100 Loss 0.2761 Accuracy 0.9247\n",
            "Epoch 25 Batch 150 Loss 0.2809 Accuracy 0.9231\n",
            "Epoch 25 Batch 200 Loss 0.2881 Accuracy 0.9215\n",
            "Epoch 25 Batch 250 Loss 0.2947 Accuracy 0.9198\n",
            "Epoch 25 Batch 300 Loss 0.3036 Accuracy 0.9175\n",
            "Saving checkpoint for epoch 25 at ./checkpoints/train\\ckpt-125\n",
            "Epoch 25 Loss 0.3068 Accuracy 0.9167\n",
            "Time taken for 1 epoch: 183.62 secs\n",
            "\n",
            "Epoch 26 Batch 0 Loss 0.3000 Accuracy 0.9150\n",
            "Epoch 26 Batch 50 Loss 0.2705 Accuracy 0.9260\n",
            "Epoch 26 Batch 100 Loss 0.2751 Accuracy 0.9249\n",
            "Epoch 26 Batch 150 Loss 0.2821 Accuracy 0.9229\n",
            "Epoch 26 Batch 200 Loss 0.2892 Accuracy 0.9208\n",
            "Epoch 26 Batch 250 Loss 0.2968 Accuracy 0.9187\n",
            "Epoch 26 Batch 300 Loss 0.3043 Accuracy 0.9170\n",
            "Epoch 26 Loss 0.3069 Accuracy 0.9163\n",
            "Time taken for 1 epoch: 182.42 secs\n",
            "\n",
            "Epoch 27 Batch 0 Loss 0.2269 Accuracy 0.9359\n",
            "Epoch 27 Batch 50 Loss 0.2693 Accuracy 0.9258\n",
            "Epoch 27 Batch 100 Loss 0.2757 Accuracy 0.9248\n",
            "Epoch 27 Batch 150 Loss 0.2818 Accuracy 0.9232\n",
            "Epoch 27 Batch 200 Loss 0.2879 Accuracy 0.9214\n",
            "Epoch 27 Batch 250 Loss 0.2948 Accuracy 0.9194\n",
            "Epoch 27 Batch 300 Loss 0.3028 Accuracy 0.9176\n",
            "Epoch 27 Loss 0.3059 Accuracy 0.9167\n",
            "Time taken for 1 epoch: 183.15 secs\n",
            "\n",
            "Epoch 28 Batch 0 Loss 0.2717 Accuracy 0.9253\n",
            "Epoch 28 Batch 50 Loss 0.2700 Accuracy 0.9257\n",
            "Epoch 28 Batch 100 Loss 0.2742 Accuracy 0.9246\n",
            "Epoch 28 Batch 150 Loss 0.2779 Accuracy 0.9238\n",
            "Epoch 28 Batch 200 Loss 0.2840 Accuracy 0.9223\n",
            "Epoch 28 Batch 250 Loss 0.2924 Accuracy 0.9200\n",
            "Epoch 28 Batch 300 Loss 0.3004 Accuracy 0.9180\n",
            "Epoch 28 Loss 0.3030 Accuracy 0.9173\n",
            "Time taken for 1 epoch: 182.38 secs\n",
            "\n",
            "Epoch 29 Batch 0 Loss 0.2670 Accuracy 0.9265\n",
            "Epoch 29 Batch 50 Loss 0.2671 Accuracy 0.9273\n",
            "Epoch 29 Batch 100 Loss 0.2744 Accuracy 0.9248\n",
            "Epoch 29 Batch 150 Loss 0.2779 Accuracy 0.9245\n",
            "Epoch 29 Batch 200 Loss 0.2842 Accuracy 0.9226\n",
            "Epoch 29 Batch 250 Loss 0.2915 Accuracy 0.9206\n",
            "Epoch 29 Batch 300 Loss 0.2994 Accuracy 0.9186\n",
            "Epoch 29 Loss 0.3029 Accuracy 0.9178\n",
            "Time taken for 1 epoch: 182.38 secs\n",
            "\n",
            "Epoch 30 Batch 0 Loss 0.2341 Accuracy 0.9461\n",
            "Epoch 30 Batch 50 Loss 0.2624 Accuracy 0.9282\n",
            "Epoch 30 Batch 100 Loss 0.2719 Accuracy 0.9255\n",
            "Epoch 30 Batch 150 Loss 0.2801 Accuracy 0.9233\n",
            "Epoch 30 Batch 200 Loss 0.2870 Accuracy 0.9212\n",
            "Epoch 30 Batch 250 Loss 0.2942 Accuracy 0.9190\n",
            "Epoch 30 Batch 300 Loss 0.3028 Accuracy 0.9166\n",
            "Saving checkpoint for epoch 30 at ./checkpoints/train\\ckpt-126\n",
            "Epoch 30 Loss 0.3052 Accuracy 0.9160\n",
            "Time taken for 1 epoch: 183.61 secs\n",
            "\n",
            "Epoch 31 Batch 0 Loss 0.2437 Accuracy 0.9276\n",
            "Epoch 31 Batch 50 Loss 0.2661 Accuracy 0.9272\n",
            "Epoch 31 Batch 100 Loss 0.2722 Accuracy 0.9252\n",
            "Epoch 31 Batch 150 Loss 0.2782 Accuracy 0.9233\n",
            "Epoch 31 Batch 200 Loss 0.2855 Accuracy 0.9214\n",
            "Epoch 31 Batch 250 Loss 0.2931 Accuracy 0.9197\n",
            "Epoch 31 Batch 300 Loss 0.3016 Accuracy 0.9174\n",
            "Epoch 31 Loss 0.3039 Accuracy 0.9168\n",
            "Time taken for 1 epoch: 182.44 secs\n",
            "\n",
            "Epoch 32 Batch 0 Loss 0.2318 Accuracy 0.9436\n",
            "Epoch 32 Batch 50 Loss 0.2666 Accuracy 0.9275\n",
            "Epoch 32 Batch 100 Loss 0.2700 Accuracy 0.9266\n",
            "Epoch 32 Batch 150 Loss 0.2777 Accuracy 0.9242\n",
            "Epoch 32 Batch 200 Loss 0.2850 Accuracy 0.9222\n",
            "Epoch 32 Batch 250 Loss 0.2927 Accuracy 0.9199\n",
            "Epoch 32 Batch 300 Loss 0.3011 Accuracy 0.9176\n",
            "Epoch 32 Loss 0.3043 Accuracy 0.9169\n",
            "Time taken for 1 epoch: 182.40 secs\n",
            "\n",
            "Epoch 33 Batch 0 Loss 0.2683 Accuracy 0.9208\n",
            "Epoch 33 Batch 50 Loss 0.2628 Accuracy 0.9289\n",
            "Epoch 33 Batch 100 Loss 0.2712 Accuracy 0.9264\n",
            "Epoch 33 Batch 150 Loss 0.2768 Accuracy 0.9246\n",
            "Epoch 33 Batch 200 Loss 0.2843 Accuracy 0.9224\n",
            "Epoch 33 Batch 250 Loss 0.2922 Accuracy 0.9202\n",
            "Epoch 33 Batch 300 Loss 0.2998 Accuracy 0.9182\n",
            "Epoch 33 Loss 0.3025 Accuracy 0.9176\n",
            "Time taken for 1 epoch: 182.99 secs\n",
            "\n",
            "Epoch 34 Batch 0 Loss 0.2385 Accuracy 0.9458\n",
            "Epoch 34 Batch 50 Loss 0.2651 Accuracy 0.9274\n",
            "Epoch 34 Batch 100 Loss 0.2733 Accuracy 0.9250\n",
            "Epoch 34 Batch 150 Loss 0.2792 Accuracy 0.9232\n",
            "Epoch 34 Batch 200 Loss 0.2869 Accuracy 0.9211\n",
            "Epoch 34 Batch 250 Loss 0.2929 Accuracy 0.9199\n",
            "Epoch 34 Batch 300 Loss 0.3010 Accuracy 0.9178\n",
            "Epoch 34 Loss 0.3041 Accuracy 0.9171\n",
            "Time taken for 1 epoch: 182.37 secs\n",
            "\n",
            "Epoch 35 Batch 0 Loss 0.2633 Accuracy 0.9247\n",
            "Epoch 35 Batch 50 Loss 0.2594 Accuracy 0.9278\n",
            "Epoch 35 Batch 100 Loss 0.2654 Accuracy 0.9267\n",
            "Epoch 35 Batch 150 Loss 0.2734 Accuracy 0.9246\n",
            "Epoch 35 Batch 200 Loss 0.2807 Accuracy 0.9228\n",
            "Epoch 35 Batch 250 Loss 0.2879 Accuracy 0.9208\n",
            "Epoch 35 Batch 300 Loss 0.2965 Accuracy 0.9188\n",
            "Saving checkpoint for epoch 35 at ./checkpoints/train\\ckpt-127\n",
            "Epoch 35 Loss 0.2997 Accuracy 0.9180\n",
            "Time taken for 1 epoch: 183.52 secs\n",
            "\n",
            "Epoch 36 Batch 0 Loss 0.2606 Accuracy 0.9333\n",
            "Epoch 36 Batch 50 Loss 0.2648 Accuracy 0.9274\n",
            "Epoch 36 Batch 100 Loss 0.2670 Accuracy 0.9265\n",
            "Epoch 36 Batch 150 Loss 0.2736 Accuracy 0.9251\n",
            "Epoch 36 Batch 200 Loss 0.2808 Accuracy 0.9232\n",
            "Epoch 36 Batch 250 Loss 0.2878 Accuracy 0.9214\n",
            "Epoch 36 Batch 300 Loss 0.2973 Accuracy 0.9191\n",
            "Epoch 36 Loss 0.3010 Accuracy 0.9180\n",
            "Time taken for 1 epoch: 182.40 secs\n",
            "\n",
            "Epoch 37 Batch 0 Loss 0.2457 Accuracy 0.9328\n",
            "Epoch 37 Batch 50 Loss 0.2605 Accuracy 0.9294\n",
            "Epoch 37 Batch 100 Loss 0.2687 Accuracy 0.9272\n",
            "Epoch 37 Batch 150 Loss 0.2748 Accuracy 0.9254\n",
            "Epoch 37 Batch 200 Loss 0.2812 Accuracy 0.9231\n",
            "Epoch 37 Batch 250 Loss 0.2885 Accuracy 0.9212\n",
            "Epoch 37 Batch 300 Loss 0.2965 Accuracy 0.9190\n",
            "Epoch 37 Loss 0.2997 Accuracy 0.9183\n",
            "Time taken for 1 epoch: 183.02 secs\n",
            "\n",
            "Epoch 38 Batch 0 Loss 0.2829 Accuracy 0.9178\n",
            "Epoch 38 Batch 50 Loss 0.2697 Accuracy 0.9265\n",
            "Epoch 38 Batch 100 Loss 0.2757 Accuracy 0.9243\n",
            "Epoch 38 Batch 150 Loss 0.2809 Accuracy 0.9231\n",
            "Epoch 38 Batch 200 Loss 0.2865 Accuracy 0.9216\n",
            "Epoch 38 Batch 250 Loss 0.2931 Accuracy 0.9201\n",
            "Epoch 38 Batch 300 Loss 0.3005 Accuracy 0.9182\n",
            "Epoch 38 Loss 0.3041 Accuracy 0.9174\n",
            "Time taken for 1 epoch: 182.64 secs\n",
            "\n",
            "Epoch 39 Batch 0 Loss 0.2674 Accuracy 0.9367\n",
            "Epoch 39 Batch 50 Loss 0.2698 Accuracy 0.9276\n",
            "Epoch 39 Batch 100 Loss 0.2714 Accuracy 0.9265\n",
            "Epoch 39 Batch 150 Loss 0.2777 Accuracy 0.9250\n",
            "Epoch 39 Batch 200 Loss 0.2842 Accuracy 0.9231\n",
            "Epoch 39 Batch 250 Loss 0.2905 Accuracy 0.9214\n",
            "Epoch 39 Batch 300 Loss 0.2987 Accuracy 0.9192\n",
            "Epoch 39 Loss 0.3020 Accuracy 0.9183\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 40 Batch 0 Loss 0.2770 Accuracy 0.9114\n",
            "Epoch 40 Batch 50 Loss 0.2706 Accuracy 0.9256\n",
            "Epoch 40 Batch 100 Loss 0.2739 Accuracy 0.9253\n",
            "Epoch 40 Batch 150 Loss 0.2787 Accuracy 0.9236\n",
            "Epoch 40 Batch 200 Loss 0.2842 Accuracy 0.9222\n",
            "Epoch 40 Batch 250 Loss 0.2896 Accuracy 0.9209\n",
            "Epoch 40 Batch 300 Loss 0.2982 Accuracy 0.9185\n",
            "Saving checkpoint for epoch 40 at ./checkpoints/train\\ckpt-128\n",
            "Epoch 40 Loss 0.3017 Accuracy 0.9176\n",
            "Time taken for 1 epoch: 183.50 secs\n",
            "\n",
            "Epoch 41 Batch 0 Loss 0.2637 Accuracy 0.9232\n",
            "Epoch 41 Batch 50 Loss 0.2656 Accuracy 0.9263\n",
            "Epoch 41 Batch 100 Loss 0.2702 Accuracy 0.9258\n",
            "Epoch 41 Batch 150 Loss 0.2759 Accuracy 0.9244\n",
            "Epoch 41 Batch 200 Loss 0.2844 Accuracy 0.9221\n",
            "Epoch 41 Batch 250 Loss 0.2916 Accuracy 0.9202\n",
            "Epoch 41 Batch 300 Loss 0.2996 Accuracy 0.9183\n",
            "Epoch 41 Loss 0.3026 Accuracy 0.9176\n",
            "Time taken for 1 epoch: 182.42 secs\n",
            "\n",
            "Epoch 42 Batch 0 Loss 0.2415 Accuracy 0.9339\n",
            "Epoch 42 Batch 50 Loss 0.2614 Accuracy 0.9281\n",
            "Epoch 42 Batch 100 Loss 0.2680 Accuracy 0.9260\n",
            "Epoch 42 Batch 150 Loss 0.2758 Accuracy 0.9239\n",
            "Epoch 42 Batch 200 Loss 0.2818 Accuracy 0.9224\n",
            "Epoch 42 Batch 250 Loss 0.2890 Accuracy 0.9205\n",
            "Epoch 42 Batch 300 Loss 0.2965 Accuracy 0.9187\n",
            "Epoch 42 Loss 0.2997 Accuracy 0.9178\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 43 Batch 0 Loss 0.2424 Accuracy 0.9375\n",
            "Epoch 43 Batch 50 Loss 0.2646 Accuracy 0.9277\n",
            "Epoch 43 Batch 100 Loss 0.2730 Accuracy 0.9255\n",
            "Epoch 43 Batch 150 Loss 0.2762 Accuracy 0.9244\n",
            "Epoch 43 Batch 200 Loss 0.2827 Accuracy 0.9228\n",
            "Epoch 43 Batch 250 Loss 0.2898 Accuracy 0.9208\n",
            "Epoch 43 Batch 300 Loss 0.2972 Accuracy 0.9188\n",
            "Epoch 43 Loss 0.3001 Accuracy 0.9181\n",
            "Time taken for 1 epoch: 182.42 secs\n",
            "\n",
            "Epoch 44 Batch 0 Loss 0.2630 Accuracy 0.9289\n",
            "Epoch 44 Batch 50 Loss 0.2608 Accuracy 0.9291\n",
            "Epoch 44 Batch 100 Loss 0.2668 Accuracy 0.9268\n",
            "Epoch 44 Batch 150 Loss 0.2742 Accuracy 0.9252\n",
            "Epoch 44 Batch 200 Loss 0.2809 Accuracy 0.9232\n",
            "Epoch 44 Batch 250 Loss 0.2892 Accuracy 0.9209\n",
            "Epoch 44 Batch 300 Loss 0.2967 Accuracy 0.9193\n",
            "Epoch 44 Loss 0.2991 Accuracy 0.9188\n",
            "Time taken for 1 epoch: 182.49 secs\n",
            "\n",
            "Epoch 45 Batch 0 Loss 0.2859 Accuracy 0.9226\n",
            "Epoch 45 Batch 50 Loss 0.2681 Accuracy 0.9264\n",
            "Epoch 45 Batch 100 Loss 0.2703 Accuracy 0.9263\n",
            "Epoch 45 Batch 150 Loss 0.2744 Accuracy 0.9249\n",
            "Epoch 45 Batch 200 Loss 0.2797 Accuracy 0.9235\n",
            "Epoch 45 Batch 250 Loss 0.2881 Accuracy 0.9211\n",
            "Epoch 45 Batch 300 Loss 0.2952 Accuracy 0.9195\n",
            "Saving checkpoint for epoch 45 at ./checkpoints/train\\ckpt-129\n",
            "Epoch 45 Loss 0.2985 Accuracy 0.9188\n",
            "Time taken for 1 epoch: 184.16 secs\n",
            "\n",
            "Epoch 46 Batch 0 Loss 0.2667 Accuracy 0.9295\n",
            "Epoch 46 Batch 50 Loss 0.2615 Accuracy 0.9283\n",
            "Epoch 46 Batch 100 Loss 0.2684 Accuracy 0.9266\n",
            "Epoch 46 Batch 150 Loss 0.2747 Accuracy 0.9252\n",
            "Epoch 46 Batch 200 Loss 0.2816 Accuracy 0.9231\n",
            "Epoch 46 Batch 250 Loss 0.2874 Accuracy 0.9215\n",
            "Epoch 46 Batch 300 Loss 0.2957 Accuracy 0.9194\n",
            "Epoch 46 Loss 0.2978 Accuracy 0.9188\n",
            "Time taken for 1 epoch: 182.87 secs\n",
            "\n",
            "Epoch 47 Batch 0 Loss 0.2547 Accuracy 0.9295\n",
            "Epoch 47 Batch 50 Loss 0.2609 Accuracy 0.9290\n",
            "Epoch 47 Batch 100 Loss 0.2671 Accuracy 0.9267\n",
            "Epoch 47 Batch 150 Loss 0.2750 Accuracy 0.9247\n",
            "Epoch 47 Batch 200 Loss 0.2813 Accuracy 0.9233\n",
            "Epoch 47 Batch 250 Loss 0.2894 Accuracy 0.9210\n",
            "Epoch 47 Batch 300 Loss 0.2972 Accuracy 0.9192\n",
            "Epoch 47 Loss 0.2999 Accuracy 0.9187\n",
            "Time taken for 1 epoch: 182.49 secs\n",
            "\n",
            "Epoch 48 Batch 0 Loss 0.2174 Accuracy 0.9408\n",
            "Epoch 48 Batch 50 Loss 0.2647 Accuracy 0.9271\n",
            "Epoch 48 Batch 100 Loss 0.2737 Accuracy 0.9253\n",
            "Epoch 48 Batch 150 Loss 0.2784 Accuracy 0.9241\n",
            "Epoch 48 Batch 200 Loss 0.2826 Accuracy 0.9226\n",
            "Epoch 48 Batch 250 Loss 0.2894 Accuracy 0.9208\n",
            "Epoch 48 Batch 300 Loss 0.2967 Accuracy 0.9188\n",
            "Epoch 48 Loss 0.3001 Accuracy 0.9181\n",
            "Time taken for 1 epoch: 182.44 secs\n",
            "\n",
            "Epoch 49 Batch 0 Loss 0.2541 Accuracy 0.9273\n",
            "Epoch 49 Batch 50 Loss 0.2661 Accuracy 0.9272\n",
            "Epoch 49 Batch 100 Loss 0.2675 Accuracy 0.9274\n",
            "Epoch 49 Batch 150 Loss 0.2717 Accuracy 0.9267\n",
            "Epoch 49 Batch 200 Loss 0.2800 Accuracy 0.9242\n",
            "Epoch 49 Batch 250 Loss 0.2879 Accuracy 0.9219\n",
            "Epoch 49 Batch 300 Loss 0.2961 Accuracy 0.9196\n",
            "Epoch 49 Loss 0.2995 Accuracy 0.9187\n",
            "Time taken for 1 epoch: 182.78 secs\n",
            "\n",
            "Epoch 50 Batch 0 Loss 0.2719 Accuracy 0.9227\n",
            "Epoch 50 Batch 50 Loss 0.2560 Accuracy 0.9303\n",
            "Epoch 50 Batch 100 Loss 0.2644 Accuracy 0.9280\n",
            "Epoch 50 Batch 150 Loss 0.2723 Accuracy 0.9259\n",
            "Epoch 50 Batch 200 Loss 0.2794 Accuracy 0.9240\n",
            "Epoch 50 Batch 250 Loss 0.2874 Accuracy 0.9218\n",
            "Epoch 50 Batch 300 Loss 0.2950 Accuracy 0.9200\n",
            "Saving checkpoint for epoch 50 at ./checkpoints/train\\ckpt-130\n",
            "Epoch 50 Loss 0.2983 Accuracy 0.9193\n",
            "Time taken for 1 epoch: 183.48 secs\n",
            "\n",
            "Epoch 51 Batch 0 Loss 0.2477 Accuracy 0.9311\n",
            "Epoch 51 Batch 50 Loss 0.2613 Accuracy 0.9293\n",
            "Epoch 51 Batch 100 Loss 0.2698 Accuracy 0.9277\n",
            "Epoch 51 Batch 150 Loss 0.2736 Accuracy 0.9264\n",
            "Epoch 51 Batch 200 Loss 0.2800 Accuracy 0.9243\n",
            "Epoch 51 Batch 250 Loss 0.2880 Accuracy 0.9220\n",
            "Epoch 51 Batch 300 Loss 0.2959 Accuracy 0.9199\n",
            "Epoch 51 Loss 0.2993 Accuracy 0.9190\n",
            "Time taken for 1 epoch: 182.62 secs\n",
            "\n",
            "Epoch 52 Batch 0 Loss 0.2585 Accuracy 0.9363\n",
            "Epoch 52 Batch 50 Loss 0.2586 Accuracy 0.9297\n",
            "Epoch 52 Batch 100 Loss 0.2646 Accuracy 0.9276\n",
            "Epoch 52 Batch 150 Loss 0.2698 Accuracy 0.9263\n",
            "Epoch 52 Batch 200 Loss 0.2765 Accuracy 0.9245\n",
            "Epoch 52 Batch 250 Loss 0.2845 Accuracy 0.9225\n",
            "Epoch 52 Batch 300 Loss 0.2935 Accuracy 0.9203\n",
            "Epoch 52 Loss 0.2971 Accuracy 0.9193\n",
            "Time taken for 1 epoch: 182.51 secs\n",
            "\n",
            "Epoch 53 Batch 0 Loss 0.2222 Accuracy 0.9401\n",
            "Epoch 53 Batch 50 Loss 0.2592 Accuracy 0.9304\n",
            "Epoch 53 Batch 100 Loss 0.2665 Accuracy 0.9275\n",
            "Epoch 53 Batch 150 Loss 0.2720 Accuracy 0.9263\n",
            "Epoch 53 Batch 200 Loss 0.2794 Accuracy 0.9243\n",
            "Epoch 53 Batch 250 Loss 0.2862 Accuracy 0.9226\n",
            "Epoch 53 Batch 300 Loss 0.2945 Accuracy 0.9206\n",
            "Epoch 53 Loss 0.2973 Accuracy 0.9198\n",
            "Time taken for 1 epoch: 182.42 secs\n",
            "\n",
            "Epoch 54 Batch 0 Loss 0.2733 Accuracy 0.9109\n",
            "Epoch 54 Batch 50 Loss 0.2546 Accuracy 0.9296\n",
            "Epoch 54 Batch 100 Loss 0.2625 Accuracy 0.9281\n",
            "Epoch 54 Batch 150 Loss 0.2696 Accuracy 0.9258\n",
            "Epoch 54 Batch 200 Loss 0.2789 Accuracy 0.9232\n",
            "Epoch 54 Batch 250 Loss 0.2866 Accuracy 0.9209\n",
            "Epoch 54 Batch 300 Loss 0.2944 Accuracy 0.9190\n",
            "Epoch 54 Loss 0.2971 Accuracy 0.9183\n",
            "Time taken for 1 epoch: 182.49 secs\n",
            "\n",
            "Epoch 55 Batch 0 Loss 0.2575 Accuracy 0.9349\n",
            "Epoch 55 Batch 50 Loss 0.2555 Accuracy 0.9308\n",
            "Epoch 55 Batch 100 Loss 0.2651 Accuracy 0.9282\n",
            "Epoch 55 Batch 150 Loss 0.2720 Accuracy 0.9261\n",
            "Epoch 55 Batch 200 Loss 0.2776 Accuracy 0.9243\n",
            "Epoch 55 Batch 250 Loss 0.2849 Accuracy 0.9224\n",
            "Epoch 55 Batch 300 Loss 0.2942 Accuracy 0.9200\n",
            "Saving checkpoint for epoch 55 at ./checkpoints/train\\ckpt-131\n",
            "Epoch 55 Loss 0.2971 Accuracy 0.9194\n",
            "Time taken for 1 epoch: 184.28 secs\n",
            "\n",
            "Epoch 56 Batch 0 Loss 0.2482 Accuracy 0.9341\n",
            "Epoch 56 Batch 50 Loss 0.2528 Accuracy 0.9307\n",
            "Epoch 56 Batch 100 Loss 0.2646 Accuracy 0.9278\n",
            "Epoch 56 Batch 150 Loss 0.2714 Accuracy 0.9259\n",
            "Epoch 56 Batch 200 Loss 0.2780 Accuracy 0.9239\n",
            "Epoch 56 Batch 250 Loss 0.2849 Accuracy 0.9220\n",
            "Epoch 56 Batch 300 Loss 0.2944 Accuracy 0.9196\n",
            "Epoch 56 Loss 0.2973 Accuracy 0.9191\n",
            "Time taken for 1 epoch: 183.14 secs\n",
            "\n",
            "Epoch 57 Batch 0 Loss 0.2456 Accuracy 0.9360\n",
            "Epoch 57 Batch 50 Loss 0.2505 Accuracy 0.9321\n",
            "Epoch 57 Batch 100 Loss 0.2639 Accuracy 0.9276\n",
            "Epoch 57 Batch 150 Loss 0.2706 Accuracy 0.9257\n",
            "Epoch 57 Batch 200 Loss 0.2774 Accuracy 0.9241\n",
            "Epoch 57 Batch 250 Loss 0.2842 Accuracy 0.9223\n",
            "Epoch 57 Batch 300 Loss 0.2909 Accuracy 0.9208\n",
            "Epoch 57 Loss 0.2944 Accuracy 0.9200\n",
            "Time taken for 1 epoch: 183.10 secs\n",
            "\n",
            "Epoch 58 Batch 0 Loss 0.2480 Accuracy 0.9366\n",
            "Epoch 58 Batch 50 Loss 0.2625 Accuracy 0.9279\n",
            "Epoch 58 Batch 100 Loss 0.2668 Accuracy 0.9269\n",
            "Epoch 58 Batch 150 Loss 0.2704 Accuracy 0.9257\n",
            "Epoch 58 Batch 200 Loss 0.2772 Accuracy 0.9240\n",
            "Epoch 58 Batch 250 Loss 0.2838 Accuracy 0.9224\n",
            "Epoch 58 Batch 300 Loss 0.2921 Accuracy 0.9203\n",
            "Epoch 58 Loss 0.2951 Accuracy 0.9196\n",
            "Time taken for 1 epoch: 183.12 secs\n",
            "\n",
            "Epoch 59 Batch 0 Loss 0.2659 Accuracy 0.9279\n",
            "Epoch 59 Batch 50 Loss 0.2599 Accuracy 0.9298\n",
            "Epoch 59 Batch 100 Loss 0.2643 Accuracy 0.9284\n",
            "Epoch 59 Batch 150 Loss 0.2688 Accuracy 0.9270\n",
            "Epoch 59 Batch 200 Loss 0.2749 Accuracy 0.9254\n",
            "Epoch 59 Batch 250 Loss 0.2831 Accuracy 0.9229\n",
            "Epoch 59 Batch 300 Loss 0.2911 Accuracy 0.9205\n",
            "Epoch 59 Loss 0.2943 Accuracy 0.9197\n",
            "Time taken for 1 epoch: 183.10 secs\n",
            "\n",
            "Epoch 60 Batch 0 Loss 0.2631 Accuracy 0.9301\n",
            "Epoch 60 Batch 50 Loss 0.2595 Accuracy 0.9297\n",
            "Epoch 60 Batch 100 Loss 0.2646 Accuracy 0.9282\n",
            "Epoch 60 Batch 150 Loss 0.2720 Accuracy 0.9260\n",
            "Epoch 60 Batch 200 Loss 0.2776 Accuracy 0.9245\n",
            "Epoch 60 Batch 250 Loss 0.2845 Accuracy 0.9226\n",
            "Epoch 60 Batch 300 Loss 0.2913 Accuracy 0.9208\n",
            "Saving checkpoint for epoch 60 at ./checkpoints/train\\ckpt-132\n",
            "Epoch 60 Loss 0.2939 Accuracy 0.9201\n",
            "Time taken for 1 epoch: 184.41 secs\n",
            "\n",
            "Epoch 61 Batch 0 Loss 0.2061 Accuracy 0.9496\n",
            "Epoch 61 Batch 50 Loss 0.2536 Accuracy 0.9308\n",
            "Epoch 61 Batch 100 Loss 0.2611 Accuracy 0.9287\n",
            "Epoch 61 Batch 150 Loss 0.2658 Accuracy 0.9275\n",
            "Epoch 61 Batch 200 Loss 0.2729 Accuracy 0.9258\n",
            "Epoch 61 Batch 250 Loss 0.2798 Accuracy 0.9238\n",
            "Epoch 61 Batch 300 Loss 0.2891 Accuracy 0.9217\n",
            "Epoch 61 Loss 0.2925 Accuracy 0.9208\n",
            "Time taken for 1 epoch: 183.12 secs\n",
            "\n",
            "Epoch 62 Batch 0 Loss 0.2285 Accuracy 0.9309\n",
            "Epoch 62 Batch 50 Loss 0.2546 Accuracy 0.9285\n",
            "Epoch 62 Batch 100 Loss 0.2605 Accuracy 0.9278\n",
            "Epoch 62 Batch 150 Loss 0.2680 Accuracy 0.9260\n",
            "Epoch 62 Batch 200 Loss 0.2753 Accuracy 0.9242\n",
            "Epoch 62 Batch 250 Loss 0.2823 Accuracy 0.9224\n",
            "Epoch 62 Batch 300 Loss 0.2919 Accuracy 0.9200\n",
            "Epoch 62 Loss 0.2945 Accuracy 0.9195\n",
            "Time taken for 1 epoch: 183.04 secs\n",
            "\n",
            "Epoch 63 Batch 0 Loss 0.2798 Accuracy 0.9193\n",
            "Epoch 63 Batch 50 Loss 0.2594 Accuracy 0.9284\n",
            "Epoch 63 Batch 100 Loss 0.2648 Accuracy 0.9267\n",
            "Epoch 63 Batch 150 Loss 0.2695 Accuracy 0.9253\n",
            "Epoch 63 Batch 200 Loss 0.2776 Accuracy 0.9236\n",
            "Epoch 63 Batch 250 Loss 0.2850 Accuracy 0.9215\n",
            "Epoch 63 Batch 300 Loss 0.2922 Accuracy 0.9198\n",
            "Epoch 63 Loss 0.2948 Accuracy 0.9193\n",
            "Time taken for 1 epoch: 183.14 secs\n",
            "\n",
            "Epoch 64 Batch 0 Loss 0.2804 Accuracy 0.9225\n",
            "Epoch 64 Batch 50 Loss 0.2580 Accuracy 0.9297\n",
            "Epoch 64 Batch 100 Loss 0.2660 Accuracy 0.9275\n",
            "Epoch 64 Batch 150 Loss 0.2688 Accuracy 0.9268\n",
            "Epoch 64 Batch 200 Loss 0.2760 Accuracy 0.9245\n",
            "Epoch 64 Batch 250 Loss 0.2830 Accuracy 0.9227\n",
            "Epoch 64 Batch 300 Loss 0.2907 Accuracy 0.9207\n",
            "Epoch 64 Loss 0.2935 Accuracy 0.9200\n",
            "Time taken for 1 epoch: 183.08 secs\n",
            "\n",
            "Epoch 65 Batch 0 Loss 0.2566 Accuracy 0.9376\n",
            "Epoch 65 Batch 50 Loss 0.2677 Accuracy 0.9279\n",
            "Epoch 65 Batch 100 Loss 0.2664 Accuracy 0.9278\n",
            "Epoch 65 Batch 150 Loss 0.2722 Accuracy 0.9261\n",
            "Epoch 65 Batch 200 Loss 0.2777 Accuracy 0.9243\n",
            "Epoch 65 Batch 250 Loss 0.2841 Accuracy 0.9231\n",
            "Epoch 65 Batch 300 Loss 0.2902 Accuracy 0.9215\n",
            "Saving checkpoint for epoch 65 at ./checkpoints/train\\ckpt-133\n",
            "Epoch 65 Loss 0.2935 Accuracy 0.9208\n",
            "Time taken for 1 epoch: 184.25 secs\n",
            "\n",
            "Epoch 66 Batch 0 Loss 0.2838 Accuracy 0.9253\n",
            "Epoch 66 Batch 50 Loss 0.2554 Accuracy 0.9310\n",
            "Epoch 66 Batch 100 Loss 0.2615 Accuracy 0.9287\n",
            "Epoch 66 Batch 150 Loss 0.2687 Accuracy 0.9267\n",
            "Epoch 66 Batch 200 Loss 0.2759 Accuracy 0.9247\n",
            "Epoch 66 Batch 250 Loss 0.2830 Accuracy 0.9230\n",
            "Epoch 66 Batch 300 Loss 0.2908 Accuracy 0.9210\n",
            "Epoch 66 Loss 0.2935 Accuracy 0.9203\n",
            "Time taken for 1 epoch: 183.07 secs\n",
            "\n",
            "Epoch 67 Batch 0 Loss 0.2360 Accuracy 0.9381\n",
            "Epoch 67 Batch 50 Loss 0.2594 Accuracy 0.9297\n",
            "Epoch 67 Batch 100 Loss 0.2643 Accuracy 0.9278\n",
            "Epoch 67 Batch 150 Loss 0.2714 Accuracy 0.9259\n",
            "Epoch 67 Batch 200 Loss 0.2781 Accuracy 0.9245\n",
            "Epoch 67 Batch 250 Loss 0.2843 Accuracy 0.9228\n",
            "Epoch 67 Batch 300 Loss 0.2916 Accuracy 0.9208\n",
            "Epoch 67 Loss 0.2949 Accuracy 0.9200\n",
            "Time taken for 1 epoch: 183.76 secs\n",
            "\n",
            "Epoch 68 Batch 0 Loss 0.2723 Accuracy 0.9329\n",
            "Epoch 68 Batch 50 Loss 0.2623 Accuracy 0.9299\n",
            "Epoch 68 Batch 100 Loss 0.2646 Accuracy 0.9293\n",
            "Epoch 68 Batch 150 Loss 0.2675 Accuracy 0.9279\n",
            "Epoch 68 Batch 200 Loss 0.2740 Accuracy 0.9259\n",
            "Epoch 68 Batch 250 Loss 0.2812 Accuracy 0.9239\n",
            "Epoch 68 Batch 300 Loss 0.2891 Accuracy 0.9218\n",
            "Epoch 68 Loss 0.2921 Accuracy 0.9210\n",
            "Time taken for 1 epoch: 183.12 secs\n",
            "\n",
            "Epoch 69 Batch 0 Loss 0.2453 Accuracy 0.9260\n",
            "Epoch 69 Batch 50 Loss 0.2559 Accuracy 0.9298\n",
            "Epoch 69 Batch 100 Loss 0.2615 Accuracy 0.9288\n",
            "Epoch 69 Batch 150 Loss 0.2689 Accuracy 0.9266\n",
            "Epoch 69 Batch 200 Loss 0.2760 Accuracy 0.9249\n",
            "Epoch 69 Batch 250 Loss 0.2821 Accuracy 0.9232\n",
            "Epoch 69 Batch 300 Loss 0.2892 Accuracy 0.9214\n",
            "Epoch 69 Loss 0.2922 Accuracy 0.9206\n",
            "Time taken for 1 epoch: 182.99 secs\n",
            "\n",
            "Epoch 70 Batch 0 Loss 0.2376 Accuracy 0.9359\n",
            "Epoch 70 Batch 50 Loss 0.2557 Accuracy 0.9299\n",
            "Epoch 70 Batch 100 Loss 0.2634 Accuracy 0.9277\n",
            "Epoch 70 Batch 150 Loss 0.2681 Accuracy 0.9266\n",
            "Epoch 70 Batch 200 Loss 0.2732 Accuracy 0.9256\n",
            "Epoch 70 Batch 250 Loss 0.2805 Accuracy 0.9236\n",
            "Epoch 70 Batch 300 Loss 0.2892 Accuracy 0.9213\n",
            "Saving checkpoint for epoch 70 at ./checkpoints/train\\ckpt-134\n",
            "Epoch 70 Loss 0.2920 Accuracy 0.9206\n",
            "Time taken for 1 epoch: 184.07 secs\n",
            "\n",
            "Epoch 71 Batch 0 Loss 0.2418 Accuracy 0.9350\n",
            "Epoch 71 Batch 50 Loss 0.2549 Accuracy 0.9297\n",
            "Epoch 71 Batch 100 Loss 0.2635 Accuracy 0.9272\n",
            "Epoch 71 Batch 150 Loss 0.2696 Accuracy 0.9255\n",
            "Epoch 71 Batch 200 Loss 0.2755 Accuracy 0.9243\n",
            "Epoch 71 Batch 250 Loss 0.2831 Accuracy 0.9226\n",
            "Epoch 71 Batch 300 Loss 0.2915 Accuracy 0.9205\n",
            "Epoch 71 Loss 0.2944 Accuracy 0.9197\n",
            "Time taken for 1 epoch: 183.27 secs\n",
            "\n",
            "Epoch 72 Batch 0 Loss 0.2565 Accuracy 0.9265\n",
            "Epoch 72 Batch 50 Loss 0.2568 Accuracy 0.9298\n",
            "Epoch 72 Batch 100 Loss 0.2637 Accuracy 0.9285\n",
            "Epoch 72 Batch 150 Loss 0.2710 Accuracy 0.9265\n",
            "Epoch 72 Batch 200 Loss 0.2780 Accuracy 0.9245\n",
            "Epoch 72 Batch 250 Loss 0.2839 Accuracy 0.9229\n",
            "Epoch 72 Batch 300 Loss 0.2916 Accuracy 0.9209\n",
            "Epoch 72 Loss 0.2943 Accuracy 0.9202\n",
            "Time taken for 1 epoch: 183.09 secs\n",
            "\n",
            "Epoch 73 Batch 0 Loss 0.2687 Accuracy 0.9242\n",
            "Epoch 73 Batch 50 Loss 0.2630 Accuracy 0.9276\n",
            "Epoch 73 Batch 100 Loss 0.2654 Accuracy 0.9270\n",
            "Epoch 73 Batch 150 Loss 0.2722 Accuracy 0.9252\n",
            "Epoch 73 Batch 200 Loss 0.2787 Accuracy 0.9235\n",
            "Epoch 73 Batch 250 Loss 0.2838 Accuracy 0.9224\n",
            "Epoch 73 Batch 300 Loss 0.2901 Accuracy 0.9212\n",
            "Epoch 73 Loss 0.2929 Accuracy 0.9203\n",
            "Time taken for 1 epoch: 183.05 secs\n",
            "\n",
            "Epoch 74 Batch 0 Loss 0.2405 Accuracy 0.9397\n",
            "Epoch 74 Batch 50 Loss 0.2524 Accuracy 0.9323\n",
            "Epoch 74 Batch 100 Loss 0.2595 Accuracy 0.9302\n",
            "Epoch 74 Batch 150 Loss 0.2648 Accuracy 0.9281\n",
            "Epoch 74 Batch 200 Loss 0.2733 Accuracy 0.9257\n",
            "Epoch 74 Batch 250 Loss 0.2805 Accuracy 0.9235\n",
            "Epoch 74 Batch 300 Loss 0.2876 Accuracy 0.9218\n",
            "Epoch 74 Loss 0.2911 Accuracy 0.9210\n",
            "Time taken for 1 epoch: 183.09 secs\n",
            "\n",
            "Epoch 75 Batch 0 Loss 0.2538 Accuracy 0.9348\n",
            "Epoch 75 Batch 50 Loss 0.2630 Accuracy 0.9278\n",
            "Epoch 75 Batch 100 Loss 0.2653 Accuracy 0.9275\n",
            "Epoch 75 Batch 150 Loss 0.2705 Accuracy 0.9263\n",
            "Epoch 75 Batch 200 Loss 0.2763 Accuracy 0.9245\n",
            "Epoch 75 Batch 250 Loss 0.2834 Accuracy 0.9225\n",
            "Epoch 75 Batch 300 Loss 0.2899 Accuracy 0.9209\n",
            "Saving checkpoint for epoch 75 at ./checkpoints/train\\ckpt-135\n",
            "Epoch 75 Loss 0.2926 Accuracy 0.9203\n",
            "Time taken for 1 epoch: 183.64 secs\n",
            "\n",
            "Epoch 76 Batch 0 Loss 0.2617 Accuracy 0.9270\n",
            "Epoch 76 Batch 50 Loss 0.2551 Accuracy 0.9312\n",
            "Epoch 76 Batch 100 Loss 0.2595 Accuracy 0.9298\n",
            "Epoch 76 Batch 150 Loss 0.2663 Accuracy 0.9278\n",
            "Epoch 76 Batch 200 Loss 0.2728 Accuracy 0.9259\n",
            "Epoch 76 Batch 250 Loss 0.2799 Accuracy 0.9241\n",
            "Epoch 76 Batch 300 Loss 0.2873 Accuracy 0.9221\n",
            "Epoch 76 Loss 0.2903 Accuracy 0.9213\n",
            "Time taken for 1 epoch: 182.71 secs\n",
            "\n",
            "Epoch 77 Batch 0 Loss 0.2322 Accuracy 0.9379\n",
            "Epoch 77 Batch 50 Loss 0.2557 Accuracy 0.9299\n",
            "Epoch 77 Batch 100 Loss 0.2609 Accuracy 0.9292\n",
            "Epoch 77 Batch 150 Loss 0.2674 Accuracy 0.9270\n",
            "Epoch 77 Batch 200 Loss 0.2740 Accuracy 0.9251\n",
            "Epoch 77 Batch 250 Loss 0.2806 Accuracy 0.9231\n",
            "Epoch 77 Batch 300 Loss 0.2883 Accuracy 0.9212\n",
            "Epoch 77 Loss 0.2918 Accuracy 0.9204\n",
            "Time taken for 1 epoch: 183.08 secs\n",
            "\n",
            "Epoch 78 Batch 0 Loss 0.2984 Accuracy 0.9165\n",
            "Epoch 78 Batch 50 Loss 0.2600 Accuracy 0.9283\n",
            "Epoch 78 Batch 100 Loss 0.2637 Accuracy 0.9277\n",
            "Epoch 78 Batch 150 Loss 0.2673 Accuracy 0.9268\n",
            "Epoch 78 Batch 200 Loss 0.2726 Accuracy 0.9254\n",
            "Epoch 78 Batch 250 Loss 0.2804 Accuracy 0.9232\n",
            "Epoch 78 Batch 300 Loss 0.2878 Accuracy 0.9217\n",
            "Epoch 78 Loss 0.2909 Accuracy 0.9209\n",
            "Time taken for 1 epoch: 182.43 secs\n",
            "\n",
            "Epoch 79 Batch 0 Loss 0.2763 Accuracy 0.9242\n",
            "Epoch 79 Batch 50 Loss 0.2532 Accuracy 0.9302\n",
            "Epoch 79 Batch 100 Loss 0.2585 Accuracy 0.9295\n",
            "Epoch 79 Batch 150 Loss 0.2642 Accuracy 0.9279\n",
            "Epoch 79 Batch 200 Loss 0.2708 Accuracy 0.9264\n",
            "Epoch 79 Batch 250 Loss 0.2782 Accuracy 0.9242\n",
            "Epoch 79 Batch 300 Loss 0.2848 Accuracy 0.9224\n",
            "Epoch 79 Loss 0.2878 Accuracy 0.9218\n",
            "Time taken for 1 epoch: 182.68 secs\n",
            "\n",
            "Epoch 80 Batch 0 Loss 0.2455 Accuracy 0.9315\n",
            "Epoch 80 Batch 50 Loss 0.2546 Accuracy 0.9306\n",
            "Epoch 80 Batch 100 Loss 0.2601 Accuracy 0.9288\n",
            "Epoch 80 Batch 150 Loss 0.2653 Accuracy 0.9272\n",
            "Epoch 80 Batch 200 Loss 0.2715 Accuracy 0.9255\n",
            "Epoch 80 Batch 250 Loss 0.2786 Accuracy 0.9238\n",
            "Epoch 80 Batch 300 Loss 0.2860 Accuracy 0.9221\n",
            "Saving checkpoint for epoch 80 at ./checkpoints/train\\ckpt-136\n",
            "Epoch 80 Loss 0.2889 Accuracy 0.9214\n",
            "Time taken for 1 epoch: 183.67 secs\n",
            "\n",
            "Epoch 81 Batch 0 Loss 0.2379 Accuracy 0.9374\n",
            "Epoch 81 Batch 50 Loss 0.2519 Accuracy 0.9324\n",
            "Epoch 81 Batch 100 Loss 0.2577 Accuracy 0.9303\n",
            "Epoch 81 Batch 150 Loss 0.2659 Accuracy 0.9277\n",
            "Epoch 81 Batch 200 Loss 0.2719 Accuracy 0.9260\n",
            "Epoch 81 Batch 250 Loss 0.2792 Accuracy 0.9241\n",
            "Epoch 81 Batch 300 Loss 0.2876 Accuracy 0.9223\n",
            "Epoch 81 Loss 0.2907 Accuracy 0.9214\n",
            "Time taken for 1 epoch: 182.84 secs\n",
            "\n",
            "Epoch 82 Batch 0 Loss 0.2219 Accuracy 0.9343\n",
            "Epoch 82 Batch 50 Loss 0.2554 Accuracy 0.9303\n",
            "Epoch 82 Batch 100 Loss 0.2629 Accuracy 0.9281\n",
            "Epoch 82 Batch 150 Loss 0.2658 Accuracy 0.9276\n",
            "Epoch 82 Batch 200 Loss 0.2718 Accuracy 0.9260\n",
            "Epoch 82 Batch 250 Loss 0.2790 Accuracy 0.9240\n",
            "Epoch 82 Batch 300 Loss 0.2865 Accuracy 0.9220\n",
            "Epoch 82 Loss 0.2901 Accuracy 0.9211\n",
            "Time taken for 1 epoch: 182.35 secs\n",
            "\n",
            "Epoch 83 Batch 0 Loss 0.2492 Accuracy 0.9270\n",
            "Epoch 83 Batch 50 Loss 0.2559 Accuracy 0.9309\n",
            "Epoch 83 Batch 100 Loss 0.2593 Accuracy 0.9293\n",
            "Epoch 83 Batch 150 Loss 0.2639 Accuracy 0.9280\n",
            "Epoch 83 Batch 200 Loss 0.2703 Accuracy 0.9264\n",
            "Epoch 83 Batch 250 Loss 0.2777 Accuracy 0.9245\n",
            "Epoch 83 Batch 300 Loss 0.2867 Accuracy 0.9222\n",
            "Epoch 83 Loss 0.2899 Accuracy 0.9213\n",
            "Time taken for 1 epoch: 182.28 secs\n",
            "\n",
            "Epoch 84 Batch 0 Loss 0.2569 Accuracy 0.9317\n",
            "Epoch 84 Batch 50 Loss 0.2535 Accuracy 0.9308\n",
            "Epoch 84 Batch 100 Loss 0.2576 Accuracy 0.9294\n",
            "Epoch 84 Batch 150 Loss 0.2647 Accuracy 0.9274\n",
            "Epoch 84 Batch 200 Loss 0.2725 Accuracy 0.9251\n",
            "Epoch 84 Batch 250 Loss 0.2800 Accuracy 0.9234\n",
            "Epoch 84 Batch 300 Loss 0.2868 Accuracy 0.9217\n",
            "Epoch 84 Loss 0.2899 Accuracy 0.9210\n",
            "Time taken for 1 epoch: 182.49 secs\n",
            "\n",
            "Epoch 85 Batch 0 Loss 0.2305 Accuracy 0.9335\n",
            "Epoch 85 Batch 50 Loss 0.2541 Accuracy 0.9312\n",
            "Epoch 85 Batch 100 Loss 0.2585 Accuracy 0.9294\n",
            "Epoch 85 Batch 150 Loss 0.2642 Accuracy 0.9279\n",
            "Epoch 85 Batch 200 Loss 0.2706 Accuracy 0.9263\n",
            "Epoch 85 Batch 250 Loss 0.2775 Accuracy 0.9245\n",
            "Epoch 85 Batch 300 Loss 0.2850 Accuracy 0.9226\n",
            "Saving checkpoint for epoch 85 at ./checkpoints/train\\ckpt-137\n",
            "Epoch 85 Loss 0.2878 Accuracy 0.9219\n",
            "Time taken for 1 epoch: 183.51 secs\n",
            "\n",
            "Epoch 86 Batch 0 Loss 0.2656 Accuracy 0.9273\n",
            "Epoch 86 Batch 50 Loss 0.2580 Accuracy 0.9293\n",
            "Epoch 86 Batch 100 Loss 0.2593 Accuracy 0.9288\n",
            "Epoch 86 Batch 150 Loss 0.2647 Accuracy 0.9278\n",
            "Epoch 86 Batch 200 Loss 0.2700 Accuracy 0.9261\n",
            "Epoch 86 Batch 250 Loss 0.2769 Accuracy 0.9244\n",
            "Epoch 86 Batch 300 Loss 0.2841 Accuracy 0.9229\n",
            "Epoch 86 Loss 0.2874 Accuracy 0.9221\n",
            "Time taken for 1 epoch: 182.47 secs\n",
            "\n",
            "Epoch 87 Batch 0 Loss 0.2671 Accuracy 0.9314\n",
            "Epoch 87 Batch 50 Loss 0.2552 Accuracy 0.9313\n",
            "Epoch 87 Batch 100 Loss 0.2605 Accuracy 0.9291\n",
            "Epoch 87 Batch 150 Loss 0.2651 Accuracy 0.9283\n",
            "Epoch 87 Batch 200 Loss 0.2723 Accuracy 0.9263\n",
            "Epoch 87 Batch 250 Loss 0.2788 Accuracy 0.9246\n",
            "Epoch 87 Batch 300 Loss 0.2863 Accuracy 0.9227\n",
            "Epoch 87 Loss 0.2893 Accuracy 0.9219\n",
            "Time taken for 1 epoch: 182.39 secs\n",
            "\n",
            "Epoch 88 Batch 0 Loss 0.2395 Accuracy 0.9251\n",
            "Epoch 88 Batch 50 Loss 0.2517 Accuracy 0.9316\n",
            "Epoch 88 Batch 100 Loss 0.2586 Accuracy 0.9294\n",
            "Epoch 88 Batch 150 Loss 0.2631 Accuracy 0.9284\n",
            "Epoch 88 Batch 200 Loss 0.2708 Accuracy 0.9260\n",
            "Epoch 88 Batch 250 Loss 0.2795 Accuracy 0.9237\n",
            "Epoch 88 Batch 300 Loss 0.2867 Accuracy 0.9220\n",
            "Epoch 88 Loss 0.2893 Accuracy 0.9214\n",
            "Time taken for 1 epoch: 182.35 secs\n",
            "\n",
            "Epoch 89 Batch 0 Loss 0.2570 Accuracy 0.9324\n",
            "Epoch 89 Batch 50 Loss 0.2519 Accuracy 0.9308\n",
            "Epoch 89 Batch 100 Loss 0.2551 Accuracy 0.9300\n",
            "Epoch 89 Batch 150 Loss 0.2624 Accuracy 0.9279\n",
            "Epoch 89 Batch 200 Loss 0.2690 Accuracy 0.9264\n",
            "Epoch 89 Batch 250 Loss 0.2755 Accuracy 0.9247\n",
            "Epoch 89 Batch 300 Loss 0.2831 Accuracy 0.9228\n",
            "Epoch 89 Loss 0.2871 Accuracy 0.9219\n",
            "Time taken for 1 epoch: 182.34 secs\n",
            "\n",
            "Epoch 90 Batch 0 Loss 0.2392 Accuracy 0.9346\n",
            "Epoch 90 Batch 50 Loss 0.2524 Accuracy 0.9297\n",
            "Epoch 90 Batch 100 Loss 0.2571 Accuracy 0.9293\n",
            "Epoch 90 Batch 150 Loss 0.2636 Accuracy 0.9275\n",
            "Epoch 90 Batch 200 Loss 0.2690 Accuracy 0.9263\n",
            "Epoch 90 Batch 250 Loss 0.2755 Accuracy 0.9248\n",
            "Epoch 90 Batch 300 Loss 0.2830 Accuracy 0.9232\n",
            "Saving checkpoint for epoch 90 at ./checkpoints/train\\ckpt-138\n",
            "Epoch 90 Loss 0.2862 Accuracy 0.9224\n",
            "Time taken for 1 epoch: 184.12 secs\n",
            "\n",
            "Epoch 91 Batch 0 Loss 0.2244 Accuracy 0.9399\n",
            "Epoch 91 Batch 50 Loss 0.2516 Accuracy 0.9316\n",
            "Epoch 91 Batch 100 Loss 0.2597 Accuracy 0.9297\n",
            "Epoch 91 Batch 150 Loss 0.2655 Accuracy 0.9275\n",
            "Epoch 91 Batch 200 Loss 0.2710 Accuracy 0.9260\n",
            "Epoch 91 Batch 250 Loss 0.2785 Accuracy 0.9238\n",
            "Epoch 91 Batch 300 Loss 0.2846 Accuracy 0.9225\n",
            "Epoch 91 Loss 0.2878 Accuracy 0.9218\n",
            "Time taken for 1 epoch: 182.33 secs\n",
            "\n",
            "Epoch 92 Batch 0 Loss 0.2794 Accuracy 0.9205\n",
            "Epoch 92 Batch 50 Loss 0.2554 Accuracy 0.9294\n",
            "Epoch 92 Batch 100 Loss 0.2584 Accuracy 0.9290\n",
            "Epoch 92 Batch 150 Loss 0.2645 Accuracy 0.9271\n",
            "Epoch 92 Batch 200 Loss 0.2715 Accuracy 0.9254\n",
            "Epoch 92 Batch 250 Loss 0.2785 Accuracy 0.9235\n",
            "Epoch 92 Batch 300 Loss 0.2864 Accuracy 0.9217\n",
            "Epoch 92 Loss 0.2889 Accuracy 0.9213\n",
            "Time taken for 1 epoch: 182.28 secs\n",
            "\n",
            "Epoch 93 Batch 0 Loss 0.2567 Accuracy 0.9317\n",
            "Epoch 93 Batch 50 Loss 0.2534 Accuracy 0.9303\n",
            "Epoch 93 Batch 100 Loss 0.2582 Accuracy 0.9295\n",
            "Epoch 93 Batch 150 Loss 0.2652 Accuracy 0.9280\n",
            "Epoch 93 Batch 200 Loss 0.2705 Accuracy 0.9271\n",
            "Epoch 93 Batch 250 Loss 0.2768 Accuracy 0.9254\n",
            "Epoch 93 Batch 300 Loss 0.2840 Accuracy 0.9234\n",
            "Epoch 93 Loss 0.2872 Accuracy 0.9227\n",
            "Time taken for 1 epoch: 182.28 secs\n",
            "\n",
            "Epoch 94 Batch 0 Loss 0.2257 Accuracy 0.9443\n",
            "Epoch 94 Batch 50 Loss 0.2535 Accuracy 0.9319\n",
            "Epoch 94 Batch 100 Loss 0.2595 Accuracy 0.9300\n",
            "Epoch 94 Batch 150 Loss 0.2646 Accuracy 0.9281\n",
            "Epoch 94 Batch 200 Loss 0.2704 Accuracy 0.9263\n",
            "Epoch 94 Batch 250 Loss 0.2761 Accuracy 0.9248\n",
            "Epoch 94 Batch 300 Loss 0.2835 Accuracy 0.9231\n",
            "Epoch 94 Loss 0.2867 Accuracy 0.9224\n",
            "Time taken for 1 epoch: 182.65 secs\n",
            "\n",
            "Epoch 95 Batch 0 Loss 0.3027 Accuracy 0.9185\n",
            "Epoch 95 Batch 50 Loss 0.2508 Accuracy 0.9310\n",
            "Epoch 95 Batch 100 Loss 0.2585 Accuracy 0.9294\n",
            "Epoch 95 Batch 150 Loss 0.2631 Accuracy 0.9284\n",
            "Epoch 95 Batch 200 Loss 0.2681 Accuracy 0.9270\n",
            "Epoch 95 Batch 250 Loss 0.2746 Accuracy 0.9255\n",
            "Epoch 95 Batch 300 Loss 0.2824 Accuracy 0.9236\n",
            "Saving checkpoint for epoch 95 at ./checkpoints/train\\ckpt-139\n",
            "Epoch 95 Loss 0.2854 Accuracy 0.9228\n",
            "Time taken for 1 epoch: 185.21 secs\n",
            "\n",
            "Epoch 96 Batch 0 Loss 0.2236 Accuracy 0.9375\n",
            "Epoch 96 Batch 50 Loss 0.2543 Accuracy 0.9313\n",
            "Epoch 96 Batch 100 Loss 0.2597 Accuracy 0.9291\n",
            "Epoch 96 Batch 150 Loss 0.2629 Accuracy 0.9282\n",
            "Epoch 96 Batch 200 Loss 0.2703 Accuracy 0.9260\n",
            "Epoch 96 Batch 250 Loss 0.2776 Accuracy 0.9241\n",
            "Epoch 96 Batch 300 Loss 0.2849 Accuracy 0.9225\n",
            "Epoch 96 Loss 0.2875 Accuracy 0.9220\n",
            "Time taken for 1 epoch: 183.08 secs\n",
            "\n",
            "Epoch 97 Batch 0 Loss 0.2230 Accuracy 0.9420\n",
            "Epoch 97 Batch 50 Loss 0.2472 Accuracy 0.9319\n",
            "Epoch 97 Batch 100 Loss 0.2562 Accuracy 0.9300\n",
            "Epoch 97 Batch 150 Loss 0.2611 Accuracy 0.9285\n",
            "Epoch 97 Batch 200 Loss 0.2690 Accuracy 0.9267\n",
            "Epoch 97 Batch 250 Loss 0.2760 Accuracy 0.9247\n",
            "Epoch 97 Batch 300 Loss 0.2837 Accuracy 0.9230\n",
            "Epoch 97 Loss 0.2864 Accuracy 0.9222\n",
            "Time taken for 1 epoch: 182.79 secs\n",
            "\n",
            "Epoch 98 Batch 0 Loss 0.2525 Accuracy 0.9402\n",
            "Epoch 98 Batch 50 Loss 0.2463 Accuracy 0.9338\n",
            "Epoch 98 Batch 100 Loss 0.2552 Accuracy 0.9313\n",
            "Epoch 98 Batch 150 Loss 0.2619 Accuracy 0.9292\n",
            "Epoch 98 Batch 200 Loss 0.2689 Accuracy 0.9271\n",
            "Epoch 98 Batch 250 Loss 0.2759 Accuracy 0.9252\n",
            "Epoch 98 Batch 300 Loss 0.2823 Accuracy 0.9236\n",
            "Epoch 98 Loss 0.2844 Accuracy 0.9230\n",
            "Time taken for 1 epoch: 182.60 secs\n",
            "\n",
            "Epoch 99 Batch 0 Loss 0.2431 Accuracy 0.9350\n",
            "Epoch 99 Batch 50 Loss 0.2544 Accuracy 0.9308\n",
            "Epoch 99 Batch 100 Loss 0.2558 Accuracy 0.9305\n",
            "Epoch 99 Batch 150 Loss 0.2612 Accuracy 0.9295\n",
            "Epoch 99 Batch 200 Loss 0.2664 Accuracy 0.9280\n",
            "Epoch 99 Batch 250 Loss 0.2717 Accuracy 0.9269\n",
            "Epoch 99 Batch 300 Loss 0.2799 Accuracy 0.9246\n",
            "Epoch 99 Loss 0.2830 Accuracy 0.9239\n",
            "Time taken for 1 epoch: 182.58 secs\n",
            "\n",
            "Epoch 100 Batch 0 Loss 0.2111 Accuracy 0.9440\n",
            "Epoch 100 Batch 50 Loss 0.2526 Accuracy 0.9320\n",
            "Epoch 100 Batch 100 Loss 0.2564 Accuracy 0.9308\n",
            "Epoch 100 Batch 150 Loss 0.2596 Accuracy 0.9297\n",
            "Epoch 100 Batch 200 Loss 0.2650 Accuracy 0.9279\n",
            "Epoch 100 Batch 250 Loss 0.2723 Accuracy 0.9259\n",
            "Epoch 100 Batch 300 Loss 0.2809 Accuracy 0.9238\n",
            "Saving checkpoint for epoch 100 at ./checkpoints/train\\ckpt-140\n",
            "Epoch 100 Loss 0.2837 Accuracy 0.9231\n",
            "Time taken for 1 epoch: 183.37 secs\n",
            "\n",
            "Epoch 101 Batch 0 Loss 0.2393 Accuracy 0.9371\n",
            "Epoch 101 Batch 50 Loss 0.2525 Accuracy 0.9306\n",
            "Epoch 101 Batch 100 Loss 0.2574 Accuracy 0.9294\n",
            "Epoch 101 Batch 150 Loss 0.2618 Accuracy 0.9279\n",
            "Epoch 101 Batch 200 Loss 0.2690 Accuracy 0.9261\n",
            "Epoch 101 Batch 250 Loss 0.2753 Accuracy 0.9248\n",
            "Epoch 101 Batch 300 Loss 0.2819 Accuracy 0.9234\n",
            "Epoch 101 Loss 0.2847 Accuracy 0.9228\n",
            "Time taken for 1 epoch: 182.57 secs\n",
            "\n",
            "Epoch 102 Batch 0 Loss 0.2822 Accuracy 0.9276\n",
            "Epoch 102 Batch 50 Loss 0.2526 Accuracy 0.9321\n",
            "Epoch 102 Batch 100 Loss 0.2601 Accuracy 0.9296\n",
            "Epoch 102 Batch 150 Loss 0.2628 Accuracy 0.9287\n",
            "Epoch 102 Batch 200 Loss 0.2687 Accuracy 0.9265\n",
            "Epoch 102 Batch 250 Loss 0.2744 Accuracy 0.9254\n",
            "Epoch 102 Batch 300 Loss 0.2816 Accuracy 0.9236\n",
            "Epoch 102 Loss 0.2851 Accuracy 0.9227\n",
            "Time taken for 1 epoch: 182.45 secs\n",
            "\n",
            "Epoch 103 Batch 0 Loss 0.2454 Accuracy 0.9356\n",
            "Epoch 103 Batch 50 Loss 0.2523 Accuracy 0.9316\n",
            "Epoch 103 Batch 100 Loss 0.2559 Accuracy 0.9305\n",
            "Epoch 103 Batch 150 Loss 0.2629 Accuracy 0.9282\n",
            "Epoch 103 Batch 200 Loss 0.2687 Accuracy 0.9267\n",
            "Epoch 103 Batch 250 Loss 0.2752 Accuracy 0.9250\n",
            "Epoch 103 Batch 300 Loss 0.2828 Accuracy 0.9229\n",
            "Epoch 103 Loss 0.2855 Accuracy 0.9223\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 104 Batch 0 Loss 0.2564 Accuracy 0.9360\n",
            "Epoch 104 Batch 50 Loss 0.2462 Accuracy 0.9338\n",
            "Epoch 104 Batch 100 Loss 0.2551 Accuracy 0.9304\n",
            "Epoch 104 Batch 150 Loss 0.2617 Accuracy 0.9289\n",
            "Epoch 104 Batch 200 Loss 0.2663 Accuracy 0.9275\n",
            "Epoch 104 Batch 250 Loss 0.2726 Accuracy 0.9259\n",
            "Epoch 104 Batch 300 Loss 0.2806 Accuracy 0.9242\n",
            "Epoch 104 Loss 0.2834 Accuracy 0.9236\n",
            "Time taken for 1 epoch: 182.63 secs\n",
            "\n",
            "Epoch 105 Batch 0 Loss 0.2107 Accuracy 0.9423\n",
            "Epoch 105 Batch 50 Loss 0.2444 Accuracy 0.9334\n",
            "Epoch 105 Batch 100 Loss 0.2519 Accuracy 0.9320\n",
            "Epoch 105 Batch 150 Loss 0.2583 Accuracy 0.9296\n",
            "Epoch 105 Batch 200 Loss 0.2667 Accuracy 0.9273\n",
            "Epoch 105 Batch 250 Loss 0.2742 Accuracy 0.9253\n",
            "Epoch 105 Batch 300 Loss 0.2821 Accuracy 0.9232\n",
            "Saving checkpoint for epoch 105 at ./checkpoints/train\\ckpt-141\n",
            "Epoch 105 Loss 0.2851 Accuracy 0.9224\n",
            "Time taken for 1 epoch: 184.30 secs\n",
            "\n",
            "Epoch 106 Batch 0 Loss 0.2190 Accuracy 0.9363\n",
            "Epoch 106 Batch 50 Loss 0.2491 Accuracy 0.9326\n",
            "Epoch 106 Batch 100 Loss 0.2563 Accuracy 0.9300\n",
            "Epoch 106 Batch 150 Loss 0.2622 Accuracy 0.9287\n",
            "Epoch 106 Batch 200 Loss 0.2674 Accuracy 0.9273\n",
            "Epoch 106 Batch 250 Loss 0.2737 Accuracy 0.9258\n",
            "Epoch 106 Batch 300 Loss 0.2796 Accuracy 0.9242\n",
            "Epoch 106 Loss 0.2829 Accuracy 0.9233\n",
            "Time taken for 1 epoch: 183.64 secs\n",
            "\n",
            "Epoch 107 Batch 0 Loss 0.2413 Accuracy 0.9351\n",
            "Epoch 107 Batch 50 Loss 0.2502 Accuracy 0.9333\n",
            "Epoch 107 Batch 100 Loss 0.2539 Accuracy 0.9319\n",
            "Epoch 107 Batch 150 Loss 0.2595 Accuracy 0.9300\n",
            "Epoch 107 Batch 200 Loss 0.2671 Accuracy 0.9280\n",
            "Epoch 107 Batch 250 Loss 0.2727 Accuracy 0.9265\n",
            "Epoch 107 Batch 300 Loss 0.2804 Accuracy 0.9244\n",
            "Epoch 107 Loss 0.2833 Accuracy 0.9236\n",
            "Time taken for 1 epoch: 182.76 secs\n",
            "\n",
            "Epoch 108 Batch 0 Loss 0.2305 Accuracy 0.9423\n",
            "Epoch 108 Batch 50 Loss 0.2516 Accuracy 0.9325\n",
            "Epoch 108 Batch 100 Loss 0.2532 Accuracy 0.9313\n",
            "Epoch 108 Batch 150 Loss 0.2593 Accuracy 0.9293\n",
            "Epoch 108 Batch 200 Loss 0.2663 Accuracy 0.9275\n",
            "Epoch 108 Batch 250 Loss 0.2727 Accuracy 0.9259\n",
            "Epoch 108 Batch 300 Loss 0.2797 Accuracy 0.9242\n",
            "Epoch 108 Loss 0.2822 Accuracy 0.9236\n",
            "Time taken for 1 epoch: 182.46 secs\n",
            "\n",
            "Epoch 109 Batch 0 Loss 0.2454 Accuracy 0.9350\n",
            "Epoch 109 Batch 50 Loss 0.2472 Accuracy 0.9333\n",
            "Epoch 109 Batch 100 Loss 0.2541 Accuracy 0.9311\n",
            "Epoch 109 Batch 150 Loss 0.2611 Accuracy 0.9294\n",
            "Epoch 109 Batch 200 Loss 0.2676 Accuracy 0.9276\n",
            "Epoch 109 Batch 250 Loss 0.2754 Accuracy 0.9253\n",
            "Epoch 109 Batch 300 Loss 0.2825 Accuracy 0.9235\n",
            "Epoch 109 Loss 0.2857 Accuracy 0.9228\n",
            "Time taken for 1 epoch: 182.55 secs\n",
            "\n",
            "Epoch 110 Batch 0 Loss 0.2521 Accuracy 0.9341\n",
            "Epoch 110 Batch 50 Loss 0.2465 Accuracy 0.9333\n",
            "Epoch 110 Batch 100 Loss 0.2530 Accuracy 0.9311\n",
            "Epoch 110 Batch 150 Loss 0.2587 Accuracy 0.9295\n",
            "Epoch 110 Batch 200 Loss 0.2656 Accuracy 0.9276\n",
            "Epoch 110 Batch 250 Loss 0.2721 Accuracy 0.9259\n",
            "Epoch 110 Batch 300 Loss 0.2800 Accuracy 0.9239\n",
            "Saving checkpoint for epoch 110 at ./checkpoints/train\\ckpt-142\n",
            "Epoch 110 Loss 0.2840 Accuracy 0.9230\n",
            "Time taken for 1 epoch: 183.53 secs\n",
            "\n",
            "Epoch 111 Batch 0 Loss 0.2328 Accuracy 0.9417\n",
            "Epoch 111 Batch 50 Loss 0.2497 Accuracy 0.9316\n",
            "Epoch 111 Batch 100 Loss 0.2528 Accuracy 0.9312\n",
            "Epoch 111 Batch 150 Loss 0.2578 Accuracy 0.9299\n",
            "Epoch 111 Batch 200 Loss 0.2645 Accuracy 0.9281\n",
            "Epoch 111 Batch 250 Loss 0.2709 Accuracy 0.9262\n",
            "Epoch 111 Batch 300 Loss 0.2793 Accuracy 0.9242\n",
            "Epoch 111 Loss 0.2831 Accuracy 0.9232\n",
            "Time taken for 1 epoch: 182.58 secs\n",
            "\n",
            "Epoch 112 Batch 0 Loss 0.2252 Accuracy 0.9384\n",
            "Epoch 112 Batch 50 Loss 0.2506 Accuracy 0.9332\n",
            "Epoch 112 Batch 100 Loss 0.2548 Accuracy 0.9318\n",
            "Epoch 112 Batch 150 Loss 0.2614 Accuracy 0.9293\n",
            "Epoch 112 Batch 200 Loss 0.2669 Accuracy 0.9280\n",
            "Epoch 112 Batch 250 Loss 0.2724 Accuracy 0.9264\n",
            "Epoch 112 Batch 300 Loss 0.2800 Accuracy 0.9243\n",
            "Epoch 112 Loss 0.2825 Accuracy 0.9235\n",
            "Time taken for 1 epoch: 182.46 secs\n",
            "\n",
            "Epoch 113 Batch 0 Loss 0.2539 Accuracy 0.9331\n",
            "Epoch 113 Batch 50 Loss 0.2472 Accuracy 0.9342\n",
            "Epoch 113 Batch 100 Loss 0.2528 Accuracy 0.9321\n",
            "Epoch 113 Batch 150 Loss 0.2586 Accuracy 0.9305\n",
            "Epoch 113 Batch 200 Loss 0.2649 Accuracy 0.9285\n",
            "Epoch 113 Batch 250 Loss 0.2714 Accuracy 0.9269\n",
            "Epoch 113 Batch 300 Loss 0.2796 Accuracy 0.9247\n",
            "Epoch 113 Loss 0.2824 Accuracy 0.9240\n",
            "Time taken for 1 epoch: 182.69 secs\n",
            "\n",
            "Epoch 114 Batch 0 Loss 0.2837 Accuracy 0.9214\n",
            "Epoch 114 Batch 50 Loss 0.2520 Accuracy 0.9307\n",
            "Epoch 114 Batch 100 Loss 0.2545 Accuracy 0.9309\n",
            "Epoch 114 Batch 150 Loss 0.2578 Accuracy 0.9299\n",
            "Epoch 114 Batch 200 Loss 0.2636 Accuracy 0.9283\n",
            "Epoch 114 Batch 250 Loss 0.2701 Accuracy 0.9266\n",
            "Epoch 114 Batch 300 Loss 0.2774 Accuracy 0.9248\n",
            "Epoch 114 Loss 0.2806 Accuracy 0.9241\n",
            "Time taken for 1 epoch: 182.56 secs\n",
            "\n",
            "Epoch 115 Batch 0 Loss 0.2446 Accuracy 0.9324\n",
            "Epoch 115 Batch 50 Loss 0.2511 Accuracy 0.9321\n",
            "Epoch 115 Batch 100 Loss 0.2525 Accuracy 0.9321\n",
            "Epoch 115 Batch 150 Loss 0.2573 Accuracy 0.9302\n",
            "Epoch 115 Batch 200 Loss 0.2637 Accuracy 0.9281\n",
            "Epoch 115 Batch 250 Loss 0.2706 Accuracy 0.9261\n",
            "Epoch 115 Batch 300 Loss 0.2785 Accuracy 0.9242\n",
            "Saving checkpoint for epoch 115 at ./checkpoints/train\\ckpt-143\n",
            "Epoch 115 Loss 0.2810 Accuracy 0.9235\n",
            "Time taken for 1 epoch: 183.79 secs\n",
            "\n",
            "Epoch 116 Batch 0 Loss 0.2889 Accuracy 0.9228\n",
            "Epoch 116 Batch 50 Loss 0.2484 Accuracy 0.9325\n",
            "Epoch 116 Batch 100 Loss 0.2525 Accuracy 0.9312\n",
            "Epoch 116 Batch 150 Loss 0.2599 Accuracy 0.9290\n",
            "Epoch 116 Batch 200 Loss 0.2660 Accuracy 0.9276\n",
            "Epoch 116 Batch 250 Loss 0.2734 Accuracy 0.9257\n",
            "Epoch 116 Batch 300 Loss 0.2812 Accuracy 0.9240\n",
            "Epoch 116 Loss 0.2843 Accuracy 0.9232\n",
            "Time taken for 1 epoch: 182.61 secs\n",
            "\n",
            "Epoch 117 Batch 0 Loss 0.2329 Accuracy 0.9387\n",
            "Epoch 117 Batch 50 Loss 0.2448 Accuracy 0.9339\n",
            "Epoch 117 Batch 100 Loss 0.2518 Accuracy 0.9321\n",
            "Epoch 117 Batch 150 Loss 0.2580 Accuracy 0.9298\n",
            "Epoch 117 Batch 200 Loss 0.2649 Accuracy 0.9278\n",
            "Epoch 117 Batch 250 Loss 0.2710 Accuracy 0.9262\n",
            "Epoch 117 Batch 300 Loss 0.2771 Accuracy 0.9247\n",
            "Epoch 117 Loss 0.2803 Accuracy 0.9240\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 118 Batch 0 Loss 0.2277 Accuracy 0.9423\n",
            "Epoch 118 Batch 50 Loss 0.2495 Accuracy 0.9316\n",
            "Epoch 118 Batch 100 Loss 0.2547 Accuracy 0.9302\n",
            "Epoch 118 Batch 150 Loss 0.2602 Accuracy 0.9289\n",
            "Epoch 118 Batch 200 Loss 0.2658 Accuracy 0.9276\n",
            "Epoch 118 Batch 250 Loss 0.2715 Accuracy 0.9262\n",
            "Epoch 118 Batch 300 Loss 0.2787 Accuracy 0.9243\n",
            "Epoch 118 Loss 0.2812 Accuracy 0.9237\n",
            "Time taken for 1 epoch: 182.66 secs\n",
            "\n",
            "Epoch 119 Batch 0 Loss 0.2110 Accuracy 0.9381\n",
            "Epoch 119 Batch 50 Loss 0.2463 Accuracy 0.9329\n",
            "Epoch 119 Batch 100 Loss 0.2510 Accuracy 0.9319\n",
            "Epoch 119 Batch 150 Loss 0.2568 Accuracy 0.9300\n",
            "Epoch 119 Batch 200 Loss 0.2630 Accuracy 0.9283\n",
            "Epoch 119 Batch 250 Loss 0.2698 Accuracy 0.9266\n",
            "Epoch 119 Batch 300 Loss 0.2779 Accuracy 0.9246\n",
            "Epoch 119 Loss 0.2814 Accuracy 0.9237\n",
            "Time taken for 1 epoch: 182.49 secs\n",
            "\n",
            "Epoch 120 Batch 0 Loss 0.2134 Accuracy 0.9437\n",
            "Epoch 120 Batch 50 Loss 0.2460 Accuracy 0.9332\n",
            "Epoch 120 Batch 100 Loss 0.2514 Accuracy 0.9317\n",
            "Epoch 120 Batch 150 Loss 0.2587 Accuracy 0.9294\n",
            "Epoch 120 Batch 200 Loss 0.2656 Accuracy 0.9273\n",
            "Epoch 120 Batch 250 Loss 0.2716 Accuracy 0.9258\n",
            "Epoch 120 Batch 300 Loss 0.2786 Accuracy 0.9243\n",
            "Saving checkpoint for epoch 120 at ./checkpoints/train\\ckpt-144\n",
            "Epoch 120 Loss 0.2813 Accuracy 0.9237\n",
            "Time taken for 1 epoch: 183.79 secs\n",
            "\n",
            "Epoch 121 Batch 0 Loss 0.2328 Accuracy 0.9374\n",
            "Epoch 121 Batch 50 Loss 0.2479 Accuracy 0.9328\n",
            "Epoch 121 Batch 100 Loss 0.2509 Accuracy 0.9317\n",
            "Epoch 121 Batch 150 Loss 0.2560 Accuracy 0.9297\n",
            "Epoch 121 Batch 200 Loss 0.2620 Accuracy 0.9283\n",
            "Epoch 121 Batch 250 Loss 0.2689 Accuracy 0.9265\n",
            "Epoch 121 Batch 300 Loss 0.2764 Accuracy 0.9247\n",
            "Epoch 121 Loss 0.2790 Accuracy 0.9241\n",
            "Time taken for 1 epoch: 182.71 secs\n",
            "\n",
            "Epoch 122 Batch 0 Loss 0.2513 Accuracy 0.9328\n",
            "Epoch 122 Batch 50 Loss 0.2444 Accuracy 0.9339\n",
            "Epoch 122 Batch 100 Loss 0.2508 Accuracy 0.9319\n",
            "Epoch 122 Batch 150 Loss 0.2575 Accuracy 0.9300\n",
            "Epoch 122 Batch 200 Loss 0.2641 Accuracy 0.9283\n",
            "Epoch 122 Batch 250 Loss 0.2707 Accuracy 0.9266\n",
            "Epoch 122 Batch 300 Loss 0.2773 Accuracy 0.9251\n",
            "Epoch 122 Loss 0.2804 Accuracy 0.9242\n",
            "Time taken for 1 epoch: 182.73 secs\n",
            "\n",
            "Epoch 123 Batch 0 Loss 0.2291 Accuracy 0.9405\n",
            "Epoch 123 Batch 50 Loss 0.2496 Accuracy 0.9311\n",
            "Epoch 123 Batch 100 Loss 0.2508 Accuracy 0.9314\n",
            "Epoch 123 Batch 150 Loss 0.2566 Accuracy 0.9295\n",
            "Epoch 123 Batch 200 Loss 0.2622 Accuracy 0.9280\n",
            "Epoch 123 Batch 250 Loss 0.2685 Accuracy 0.9266\n",
            "Epoch 123 Batch 300 Loss 0.2750 Accuracy 0.9250\n",
            "Epoch 123 Loss 0.2784 Accuracy 0.9242\n",
            "Time taken for 1 epoch: 182.53 secs\n",
            "\n",
            "Epoch 124 Batch 0 Loss 0.1934 Accuracy 0.9519\n",
            "Epoch 124 Batch 50 Loss 0.2435 Accuracy 0.9346\n",
            "Epoch 124 Batch 100 Loss 0.2512 Accuracy 0.9324\n",
            "Epoch 124 Batch 150 Loss 0.2549 Accuracy 0.9314\n",
            "Epoch 124 Batch 200 Loss 0.2621 Accuracy 0.9296\n",
            "Epoch 124 Batch 250 Loss 0.2671 Accuracy 0.9279\n",
            "Epoch 124 Batch 300 Loss 0.2750 Accuracy 0.9259\n",
            "Epoch 124 Loss 0.2785 Accuracy 0.9249\n",
            "Time taken for 1 epoch: 183.09 secs\n",
            "\n",
            "Epoch 125 Batch 0 Loss 0.2472 Accuracy 0.9324\n",
            "Epoch 125 Batch 50 Loss 0.2440 Accuracy 0.9342\n",
            "Epoch 125 Batch 100 Loss 0.2518 Accuracy 0.9318\n",
            "Epoch 125 Batch 150 Loss 0.2552 Accuracy 0.9308\n",
            "Epoch 125 Batch 200 Loss 0.2625 Accuracy 0.9286\n",
            "Epoch 125 Batch 250 Loss 0.2694 Accuracy 0.9266\n",
            "Epoch 125 Batch 300 Loss 0.2763 Accuracy 0.9247\n",
            "Saving checkpoint for epoch 125 at ./checkpoints/train\\ckpt-145\n",
            "Epoch 125 Loss 0.2794 Accuracy 0.9239\n",
            "Time taken for 1 epoch: 184.14 secs\n",
            "\n",
            "Epoch 126 Batch 0 Loss 0.2460 Accuracy 0.9338\n",
            "Epoch 126 Batch 50 Loss 0.2426 Accuracy 0.9334\n",
            "Epoch 126 Batch 100 Loss 0.2512 Accuracy 0.9312\n",
            "Epoch 126 Batch 150 Loss 0.2568 Accuracy 0.9303\n",
            "Epoch 126 Batch 200 Loss 0.2631 Accuracy 0.9288\n",
            "Epoch 126 Batch 250 Loss 0.2687 Accuracy 0.9273\n",
            "Epoch 126 Batch 300 Loss 0.2754 Accuracy 0.9257\n",
            "Epoch 126 Loss 0.2776 Accuracy 0.9251\n",
            "Time taken for 1 epoch: 182.56 secs\n",
            "\n",
            "Epoch 127 Batch 0 Loss 0.2909 Accuracy 0.9210\n",
            "Epoch 127 Batch 50 Loss 0.2513 Accuracy 0.9309\n",
            "Epoch 127 Batch 100 Loss 0.2551 Accuracy 0.9305\n",
            "Epoch 127 Batch 150 Loss 0.2597 Accuracy 0.9291\n",
            "Epoch 127 Batch 200 Loss 0.2651 Accuracy 0.9281\n",
            "Epoch 127 Batch 250 Loss 0.2705 Accuracy 0.9264\n",
            "Epoch 127 Batch 300 Loss 0.2778 Accuracy 0.9246\n",
            "Epoch 127 Loss 0.2806 Accuracy 0.9239\n",
            "Time taken for 1 epoch: 182.49 secs\n",
            "\n",
            "Epoch 128 Batch 0 Loss 0.2045 Accuracy 0.9535\n",
            "Epoch 128 Batch 50 Loss 0.2488 Accuracy 0.9325\n",
            "Epoch 128 Batch 100 Loss 0.2507 Accuracy 0.9319\n",
            "Epoch 128 Batch 150 Loss 0.2563 Accuracy 0.9303\n",
            "Epoch 128 Batch 200 Loss 0.2620 Accuracy 0.9286\n",
            "Epoch 128 Batch 250 Loss 0.2689 Accuracy 0.9266\n",
            "Epoch 128 Batch 300 Loss 0.2764 Accuracy 0.9248\n",
            "Epoch 128 Loss 0.2793 Accuracy 0.9242\n",
            "Time taken for 1 epoch: 182.60 secs\n",
            "\n",
            "Epoch 129 Batch 0 Loss 0.2294 Accuracy 0.9397\n",
            "Epoch 129 Batch 50 Loss 0.2411 Accuracy 0.9344\n",
            "Epoch 129 Batch 100 Loss 0.2482 Accuracy 0.9325\n",
            "Epoch 129 Batch 150 Loss 0.2539 Accuracy 0.9308\n",
            "Epoch 129 Batch 200 Loss 0.2600 Accuracy 0.9292\n",
            "Epoch 129 Batch 250 Loss 0.2664 Accuracy 0.9277\n",
            "Epoch 129 Batch 300 Loss 0.2751 Accuracy 0.9256\n",
            "Epoch 129 Loss 0.2786 Accuracy 0.9249\n",
            "Time taken for 1 epoch: 182.48 secs\n",
            "\n",
            "Epoch 130 Batch 0 Loss 0.2609 Accuracy 0.9277\n",
            "Epoch 130 Batch 50 Loss 0.2433 Accuracy 0.9350\n",
            "Epoch 130 Batch 100 Loss 0.2495 Accuracy 0.9335\n",
            "Epoch 130 Batch 150 Loss 0.2566 Accuracy 0.9313\n",
            "Epoch 130 Batch 200 Loss 0.2623 Accuracy 0.9291\n",
            "Epoch 130 Batch 250 Loss 0.2690 Accuracy 0.9273\n",
            "Epoch 130 Batch 300 Loss 0.2764 Accuracy 0.9255\n",
            "Saving checkpoint for epoch 130 at ./checkpoints/train\\ckpt-146\n",
            "Epoch 130 Loss 0.2794 Accuracy 0.9247\n",
            "Time taken for 1 epoch: 183.79 secs\n",
            "\n",
            "Epoch 131 Batch 0 Loss 0.2402 Accuracy 0.9291\n",
            "Epoch 131 Batch 50 Loss 0.2467 Accuracy 0.9320\n",
            "Epoch 131 Batch 100 Loss 0.2503 Accuracy 0.9317\n",
            "Epoch 131 Batch 150 Loss 0.2564 Accuracy 0.9299\n",
            "Epoch 131 Batch 200 Loss 0.2609 Accuracy 0.9289\n",
            "Epoch 131 Batch 250 Loss 0.2678 Accuracy 0.9272\n",
            "Epoch 131 Batch 300 Loss 0.2756 Accuracy 0.9252\n",
            "Epoch 131 Loss 0.2785 Accuracy 0.9245\n",
            "Time taken for 1 epoch: 182.57 secs\n",
            "\n",
            "Epoch 132 Batch 0 Loss 0.2342 Accuracy 0.9381\n",
            "Epoch 132 Batch 50 Loss 0.2384 Accuracy 0.9370\n",
            "Epoch 132 Batch 100 Loss 0.2474 Accuracy 0.9337\n",
            "Epoch 132 Batch 150 Loss 0.2525 Accuracy 0.9314\n",
            "Epoch 132 Batch 200 Loss 0.2583 Accuracy 0.9298\n",
            "Epoch 132 Batch 250 Loss 0.2660 Accuracy 0.9278\n",
            "Epoch 132 Batch 300 Loss 0.2741 Accuracy 0.9257\n",
            "Epoch 132 Loss 0.2771 Accuracy 0.9250\n",
            "Time taken for 1 epoch: 182.97 secs\n",
            "\n",
            "Epoch 133 Batch 0 Loss 0.1911 Accuracy 0.9524\n",
            "Epoch 133 Batch 50 Loss 0.2426 Accuracy 0.9333\n",
            "Epoch 133 Batch 100 Loss 0.2498 Accuracy 0.9323\n",
            "Epoch 133 Batch 150 Loss 0.2557 Accuracy 0.9305\n",
            "Epoch 133 Batch 200 Loss 0.2599 Accuracy 0.9292\n",
            "Epoch 133 Batch 250 Loss 0.2675 Accuracy 0.9271\n",
            "Epoch 133 Batch 300 Loss 0.2749 Accuracy 0.9254\n",
            "Epoch 133 Loss 0.2781 Accuracy 0.9246\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 134 Batch 0 Loss 0.2308 Accuracy 0.9412\n",
            "Epoch 134 Batch 50 Loss 0.2466 Accuracy 0.9328\n",
            "Epoch 134 Batch 100 Loss 0.2506 Accuracy 0.9315\n",
            "Epoch 134 Batch 150 Loss 0.2541 Accuracy 0.9306\n",
            "Epoch 134 Batch 200 Loss 0.2595 Accuracy 0.9291\n",
            "Epoch 134 Batch 250 Loss 0.2669 Accuracy 0.9271\n",
            "Epoch 134 Batch 300 Loss 0.2755 Accuracy 0.9249\n",
            "Epoch 134 Loss 0.2786 Accuracy 0.9242\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 135 Batch 0 Loss 0.2447 Accuracy 0.9371\n",
            "Epoch 135 Batch 50 Loss 0.2407 Accuracy 0.9342\n",
            "Epoch 135 Batch 100 Loss 0.2474 Accuracy 0.9330\n",
            "Epoch 135 Batch 150 Loss 0.2529 Accuracy 0.9315\n",
            "Epoch 135 Batch 200 Loss 0.2584 Accuracy 0.9301\n",
            "Epoch 135 Batch 250 Loss 0.2651 Accuracy 0.9283\n",
            "Epoch 135 Batch 300 Loss 0.2724 Accuracy 0.9263\n",
            "Saving checkpoint for epoch 135 at ./checkpoints/train\\ckpt-147\n",
            "Epoch 135 Loss 0.2763 Accuracy 0.9253\n",
            "Time taken for 1 epoch: 183.84 secs\n",
            "\n",
            "Epoch 136 Batch 0 Loss 0.2175 Accuracy 0.9482\n",
            "Epoch 136 Batch 50 Loss 0.2448 Accuracy 0.9340\n",
            "Epoch 136 Batch 100 Loss 0.2512 Accuracy 0.9320\n",
            "Epoch 136 Batch 150 Loss 0.2552 Accuracy 0.9307\n",
            "Epoch 136 Batch 200 Loss 0.2626 Accuracy 0.9285\n",
            "Epoch 136 Batch 250 Loss 0.2695 Accuracy 0.9267\n",
            "Epoch 136 Batch 300 Loss 0.2759 Accuracy 0.9255\n",
            "Epoch 136 Loss 0.2786 Accuracy 0.9248\n",
            "Time taken for 1 epoch: 182.77 secs\n",
            "\n",
            "Epoch 137 Batch 0 Loss 0.2206 Accuracy 0.9392\n",
            "Epoch 137 Batch 50 Loss 0.2436 Accuracy 0.9338\n",
            "Epoch 137 Batch 100 Loss 0.2487 Accuracy 0.9328\n",
            "Epoch 137 Batch 150 Loss 0.2547 Accuracy 0.9310\n",
            "Epoch 137 Batch 200 Loss 0.2610 Accuracy 0.9292\n",
            "Epoch 137 Batch 250 Loss 0.2668 Accuracy 0.9279\n",
            "Epoch 137 Batch 300 Loss 0.2743 Accuracy 0.9262\n",
            "Epoch 137 Loss 0.2772 Accuracy 0.9255\n",
            "Time taken for 1 epoch: 182.70 secs\n",
            "\n",
            "Epoch 138 Batch 0 Loss 0.2623 Accuracy 0.9350\n",
            "Epoch 138 Batch 50 Loss 0.2424 Accuracy 0.9344\n",
            "Epoch 138 Batch 100 Loss 0.2463 Accuracy 0.9333\n",
            "Epoch 138 Batch 150 Loss 0.2524 Accuracy 0.9312\n",
            "Epoch 138 Batch 200 Loss 0.2593 Accuracy 0.9292\n",
            "Epoch 138 Batch 250 Loss 0.2665 Accuracy 0.9274\n",
            "Epoch 138 Batch 300 Loss 0.2736 Accuracy 0.9259\n",
            "Epoch 138 Loss 0.2763 Accuracy 0.9253\n",
            "Time taken for 1 epoch: 182.66 secs\n",
            "\n",
            "Epoch 139 Batch 0 Loss 0.2187 Accuracy 0.9474\n",
            "Epoch 139 Batch 50 Loss 0.2447 Accuracy 0.9337\n",
            "Epoch 139 Batch 100 Loss 0.2518 Accuracy 0.9315\n",
            "Epoch 139 Batch 150 Loss 0.2555 Accuracy 0.9308\n",
            "Epoch 139 Batch 200 Loss 0.2611 Accuracy 0.9292\n",
            "Epoch 139 Batch 250 Loss 0.2661 Accuracy 0.9279\n",
            "Epoch 139 Batch 300 Loss 0.2734 Accuracy 0.9261\n",
            "Epoch 139 Loss 0.2768 Accuracy 0.9253\n",
            "Time taken for 1 epoch: 182.59 secs\n",
            "\n",
            "Epoch 140 Batch 0 Loss 0.2656 Accuracy 0.9288\n",
            "Epoch 140 Batch 50 Loss 0.2385 Accuracy 0.9359\n",
            "Epoch 140 Batch 100 Loss 0.2442 Accuracy 0.9342\n",
            "Epoch 140 Batch 150 Loss 0.2524 Accuracy 0.9321\n",
            "Epoch 140 Batch 200 Loss 0.2580 Accuracy 0.9303\n",
            "Epoch 140 Batch 250 Loss 0.2660 Accuracy 0.9281\n",
            "Epoch 140 Batch 300 Loss 0.2724 Accuracy 0.9266\n",
            "Saving checkpoint for epoch 140 at ./checkpoints/train\\ckpt-148\n",
            "Epoch 140 Loss 0.2748 Accuracy 0.9260\n",
            "Time taken for 1 epoch: 183.75 secs\n",
            "\n",
            "Epoch 141 Batch 0 Loss 0.2439 Accuracy 0.9343\n",
            "Epoch 141 Batch 50 Loss 0.2453 Accuracy 0.9340\n",
            "Epoch 141 Batch 100 Loss 0.2494 Accuracy 0.9324\n",
            "Epoch 141 Batch 150 Loss 0.2541 Accuracy 0.9315\n",
            "Epoch 141 Batch 200 Loss 0.2603 Accuracy 0.9297\n",
            "Epoch 141 Batch 250 Loss 0.2657 Accuracy 0.9282\n",
            "Epoch 141 Batch 300 Loss 0.2740 Accuracy 0.9262\n",
            "Epoch 141 Loss 0.2771 Accuracy 0.9255\n",
            "Time taken for 1 epoch: 182.52 secs\n",
            "\n",
            "Epoch 142 Batch 0 Loss 0.2408 Accuracy 0.9308\n",
            "Epoch 142 Batch 50 Loss 0.2386 Accuracy 0.9348\n",
            "Epoch 142 Batch 100 Loss 0.2489 Accuracy 0.9324\n",
            "Epoch 142 Batch 150 Loss 0.2542 Accuracy 0.9308\n",
            "Epoch 142 Batch 200 Loss 0.2592 Accuracy 0.9296\n",
            "Epoch 142 Batch 250 Loss 0.2662 Accuracy 0.9276\n",
            "Epoch 142 Batch 300 Loss 0.2739 Accuracy 0.9257\n",
            "Epoch 142 Loss 0.2771 Accuracy 0.9249\n",
            "Time taken for 1 epoch: 182.57 secs\n",
            "\n",
            "Epoch 143 Batch 0 Loss 0.2138 Accuracy 0.9438\n",
            "Epoch 143 Batch 50 Loss 0.2409 Accuracy 0.9354\n",
            "Epoch 143 Batch 100 Loss 0.2439 Accuracy 0.9342\n",
            "Epoch 143 Batch 150 Loss 0.2523 Accuracy 0.9313\n",
            "Epoch 143 Batch 200 Loss 0.2582 Accuracy 0.9302\n",
            "Epoch 143 Batch 250 Loss 0.2647 Accuracy 0.9284\n",
            "Epoch 143 Batch 300 Loss 0.2714 Accuracy 0.9267\n",
            "Epoch 143 Loss 0.2743 Accuracy 0.9262\n",
            "Time taken for 1 epoch: 182.58 secs\n",
            "\n",
            "Epoch 144 Batch 0 Loss 0.2223 Accuracy 0.9476\n",
            "Epoch 144 Batch 50 Loss 0.2450 Accuracy 0.9337\n",
            "Epoch 144 Batch 100 Loss 0.2494 Accuracy 0.9325\n",
            "Epoch 144 Batch 150 Loss 0.2538 Accuracy 0.9314\n",
            "Epoch 144 Batch 200 Loss 0.2598 Accuracy 0.9296\n",
            "Epoch 144 Batch 250 Loss 0.2662 Accuracy 0.9281\n",
            "Epoch 144 Batch 300 Loss 0.2728 Accuracy 0.9262\n",
            "Epoch 144 Loss 0.2753 Accuracy 0.9256\n",
            "Time taken for 1 epoch: 182.46 secs\n",
            "\n",
            "Epoch 145 Batch 0 Loss 0.2634 Accuracy 0.9317\n",
            "Epoch 145 Batch 50 Loss 0.2438 Accuracy 0.9336\n",
            "Epoch 145 Batch 100 Loss 0.2492 Accuracy 0.9326\n",
            "Epoch 145 Batch 150 Loss 0.2527 Accuracy 0.9318\n",
            "Epoch 145 Batch 200 Loss 0.2586 Accuracy 0.9301\n",
            "Epoch 145 Batch 250 Loss 0.2654 Accuracy 0.9283\n",
            "Epoch 145 Batch 300 Loss 0.2723 Accuracy 0.9265\n",
            "Saving checkpoint for epoch 145 at ./checkpoints/train\\ckpt-149\n",
            "Epoch 145 Loss 0.2749 Accuracy 0.9259\n",
            "Time taken for 1 epoch: 183.98 secs\n",
            "\n",
            "Epoch 146 Batch 0 Loss 0.2311 Accuracy 0.9392\n",
            "Epoch 146 Batch 50 Loss 0.2417 Accuracy 0.9348\n",
            "Epoch 146 Batch 100 Loss 0.2458 Accuracy 0.9332\n",
            "Epoch 146 Batch 150 Loss 0.2529 Accuracy 0.9313\n",
            "Epoch 146 Batch 200 Loss 0.2573 Accuracy 0.9305\n",
            "Epoch 146 Batch 250 Loss 0.2640 Accuracy 0.9287\n",
            "Epoch 146 Batch 300 Loss 0.2713 Accuracy 0.9270\n",
            "Epoch 146 Loss 0.2745 Accuracy 0.9263\n",
            "Time taken for 1 epoch: 182.69 secs\n",
            "\n",
            "Epoch 147 Batch 0 Loss 0.2174 Accuracy 0.9403\n",
            "Epoch 147 Batch 50 Loss 0.2409 Accuracy 0.9359\n",
            "Epoch 147 Batch 100 Loss 0.2435 Accuracy 0.9352\n",
            "Epoch 147 Batch 150 Loss 0.2511 Accuracy 0.9326\n",
            "Epoch 147 Batch 200 Loss 0.2555 Accuracy 0.9313\n",
            "Epoch 147 Batch 250 Loss 0.2618 Accuracy 0.9294\n",
            "Epoch 147 Batch 300 Loss 0.2693 Accuracy 0.9275\n",
            "Epoch 147 Loss 0.2728 Accuracy 0.9266\n",
            "Time taken for 1 epoch: 182.71 secs\n",
            "\n",
            "Epoch 148 Batch 0 Loss 0.2091 Accuracy 0.9404\n",
            "Epoch 148 Batch 50 Loss 0.2441 Accuracy 0.9330\n",
            "Epoch 148 Batch 100 Loss 0.2481 Accuracy 0.9326\n",
            "Epoch 148 Batch 150 Loss 0.2527 Accuracy 0.9317\n",
            "Epoch 148 Batch 200 Loss 0.2579 Accuracy 0.9305\n",
            "Epoch 148 Batch 250 Loss 0.2638 Accuracy 0.9290\n",
            "Epoch 148 Batch 300 Loss 0.2711 Accuracy 0.9271\n",
            "Epoch 148 Loss 0.2743 Accuracy 0.9263\n",
            "Time taken for 1 epoch: 182.63 secs\n",
            "\n",
            "Epoch 149 Batch 0 Loss 0.2104 Accuracy 0.9476\n",
            "Epoch 149 Batch 50 Loss 0.2408 Accuracy 0.9358\n",
            "Epoch 149 Batch 100 Loss 0.2472 Accuracy 0.9335\n",
            "Epoch 149 Batch 150 Loss 0.2527 Accuracy 0.9317\n",
            "Epoch 149 Batch 200 Loss 0.2581 Accuracy 0.9301\n",
            "Epoch 149 Batch 250 Loss 0.2645 Accuracy 0.9286\n",
            "Epoch 149 Batch 300 Loss 0.2709 Accuracy 0.9270\n",
            "Epoch 149 Loss 0.2740 Accuracy 0.9262\n",
            "Time taken for 1 epoch: 182.58 secs\n",
            "\n",
            "Epoch 150 Batch 0 Loss 0.2453 Accuracy 0.9342\n",
            "Epoch 150 Batch 50 Loss 0.2361 Accuracy 0.9363\n",
            "Epoch 150 Batch 100 Loss 0.2426 Accuracy 0.9345\n",
            "Epoch 150 Batch 150 Loss 0.2479 Accuracy 0.9329\n",
            "Epoch 150 Batch 200 Loss 0.2562 Accuracy 0.9305\n",
            "Epoch 150 Batch 250 Loss 0.2632 Accuracy 0.9286\n",
            "Epoch 150 Batch 300 Loss 0.2698 Accuracy 0.9269\n",
            "Saving checkpoint for epoch 150 at ./checkpoints/train\\ckpt-150\n",
            "Epoch 150 Loss 0.2727 Accuracy 0.9262\n",
            "Time taken for 1 epoch: 183.68 secs\n",
            "\n",
            "Epoch 151 Batch 0 Loss 0.2408 Accuracy 0.9413\n",
            "Epoch 151 Batch 50 Loss 0.2372 Accuracy 0.9367\n",
            "Epoch 151 Batch 100 Loss 0.2440 Accuracy 0.9346\n",
            "Epoch 151 Batch 150 Loss 0.2490 Accuracy 0.9326\n",
            "Epoch 151 Batch 200 Loss 0.2555 Accuracy 0.9308\n",
            "Epoch 151 Batch 250 Loss 0.2625 Accuracy 0.9288\n",
            "Epoch 151 Batch 300 Loss 0.2704 Accuracy 0.9268\n",
            "Epoch 151 Loss 0.2731 Accuracy 0.9262\n",
            "Time taken for 1 epoch: 182.61 secs\n",
            "\n",
            "Epoch 152 Batch 0 Loss 0.2195 Accuracy 0.9399\n",
            "Epoch 152 Batch 50 Loss 0.2408 Accuracy 0.9350\n",
            "Epoch 152 Batch 100 Loss 0.2439 Accuracy 0.9344\n",
            "Epoch 152 Batch 150 Loss 0.2496 Accuracy 0.9328\n",
            "Epoch 152 Batch 200 Loss 0.2559 Accuracy 0.9309\n",
            "Epoch 152 Batch 250 Loss 0.2628 Accuracy 0.9292\n",
            "Epoch 152 Batch 300 Loss 0.2692 Accuracy 0.9277\n",
            "Epoch 152 Loss 0.2721 Accuracy 0.9270\n",
            "Time taken for 1 epoch: 182.57 secs\n",
            "\n",
            "Epoch 153 Batch 0 Loss 0.2112 Accuracy 0.9408\n",
            "Epoch 153 Batch 50 Loss 0.2406 Accuracy 0.9340\n",
            "Epoch 153 Batch 100 Loss 0.2440 Accuracy 0.9332\n",
            "Epoch 153 Batch 150 Loss 0.2490 Accuracy 0.9322\n",
            "Epoch 153 Batch 200 Loss 0.2543 Accuracy 0.9309\n",
            "Epoch 153 Batch 250 Loss 0.2615 Accuracy 0.9290\n",
            "Epoch 153 Batch 300 Loss 0.2690 Accuracy 0.9273\n",
            "Epoch 153 Loss 0.2730 Accuracy 0.9263\n",
            "Time taken for 1 epoch: 182.55 secs\n",
            "\n",
            "Epoch 154 Batch 0 Loss 0.2371 Accuracy 0.9416\n",
            "Epoch 154 Batch 50 Loss 0.2453 Accuracy 0.9331\n",
            "Epoch 154 Batch 100 Loss 0.2459 Accuracy 0.9330\n",
            "Epoch 154 Batch 150 Loss 0.2490 Accuracy 0.9324\n",
            "Epoch 154 Batch 200 Loss 0.2542 Accuracy 0.9312\n",
            "Epoch 154 Batch 250 Loss 0.2608 Accuracy 0.9295\n",
            "Epoch 154 Batch 300 Loss 0.2688 Accuracy 0.9275\n",
            "Epoch 154 Loss 0.2713 Accuracy 0.9269\n",
            "Time taken for 1 epoch: 182.99 secs\n",
            "\n",
            "Epoch 155 Batch 0 Loss 0.2030 Accuracy 0.9466\n",
            "Epoch 155 Batch 50 Loss 0.2439 Accuracy 0.9338\n",
            "Epoch 155 Batch 100 Loss 0.2465 Accuracy 0.9334\n",
            "Epoch 155 Batch 150 Loss 0.2500 Accuracy 0.9322\n",
            "Epoch 155 Batch 200 Loss 0.2555 Accuracy 0.9308\n",
            "Epoch 155 Batch 250 Loss 0.2624 Accuracy 0.9289\n",
            "Epoch 155 Batch 300 Loss 0.2693 Accuracy 0.9272\n",
            "Saving checkpoint for epoch 155 at ./checkpoints/train\\ckpt-151\n",
            "Epoch 155 Loss 0.2727 Accuracy 0.9264\n",
            "Time taken for 1 epoch: 183.85 secs\n",
            "\n",
            "Epoch 156 Batch 0 Loss 0.2540 Accuracy 0.9300\n",
            "Epoch 156 Batch 50 Loss 0.2428 Accuracy 0.9344\n",
            "Epoch 156 Batch 100 Loss 0.2478 Accuracy 0.9332\n",
            "Epoch 156 Batch 150 Loss 0.2521 Accuracy 0.9320\n",
            "Epoch 156 Batch 200 Loss 0.2574 Accuracy 0.9304\n",
            "Epoch 156 Batch 250 Loss 0.2640 Accuracy 0.9288\n",
            "Epoch 156 Batch 300 Loss 0.2701 Accuracy 0.9273\n",
            "Epoch 156 Loss 0.2727 Accuracy 0.9267\n",
            "Time taken for 1 epoch: 182.61 secs\n",
            "\n",
            "Epoch 157 Batch 0 Loss 0.2065 Accuracy 0.9406\n",
            "Epoch 157 Batch 50 Loss 0.2368 Accuracy 0.9361\n",
            "Epoch 157 Batch 100 Loss 0.2427 Accuracy 0.9344\n",
            "Epoch 157 Batch 150 Loss 0.2481 Accuracy 0.9327\n",
            "Epoch 157 Batch 200 Loss 0.2542 Accuracy 0.9309\n",
            "Epoch 157 Batch 250 Loss 0.2609 Accuracy 0.9291\n",
            "Epoch 157 Batch 300 Loss 0.2694 Accuracy 0.9269\n",
            "Epoch 157 Loss 0.2725 Accuracy 0.9262\n",
            "Time taken for 1 epoch: 182.62 secs\n",
            "\n",
            "Epoch 158 Batch 0 Loss 0.2548 Accuracy 0.9286\n",
            "Epoch 158 Batch 50 Loss 0.2400 Accuracy 0.9350\n",
            "Epoch 158 Batch 100 Loss 0.2431 Accuracy 0.9344\n",
            "Epoch 158 Batch 150 Loss 0.2482 Accuracy 0.9326\n",
            "Epoch 158 Batch 200 Loss 0.2532 Accuracy 0.9312\n",
            "Epoch 158 Batch 250 Loss 0.2601 Accuracy 0.9293\n",
            "Epoch 158 Batch 300 Loss 0.2684 Accuracy 0.9274\n",
            "Epoch 158 Loss 0.2714 Accuracy 0.9267\n",
            "Time taken for 1 epoch: 182.65 secs\n",
            "\n",
            "Epoch 159 Batch 0 Loss 0.2180 Accuracy 0.9409\n",
            "Epoch 159 Batch 50 Loss 0.2344 Accuracy 0.9359\n",
            "Epoch 159 Batch 100 Loss 0.2410 Accuracy 0.9343\n",
            "Epoch 159 Batch 150 Loss 0.2474 Accuracy 0.9327\n",
            "Epoch 159 Batch 200 Loss 0.2528 Accuracy 0.9315\n",
            "Epoch 159 Batch 250 Loss 0.2597 Accuracy 0.9295\n",
            "Epoch 159 Batch 300 Loss 0.2674 Accuracy 0.9277\n",
            "Epoch 159 Loss 0.2702 Accuracy 0.9272\n",
            "Time taken for 1 epoch: 182.58 secs\n",
            "\n",
            "Epoch 160 Batch 0 Loss 0.2174 Accuracy 0.9431\n",
            "Epoch 160 Batch 50 Loss 0.2416 Accuracy 0.9350\n",
            "Epoch 160 Batch 100 Loss 0.2416 Accuracy 0.9345\n",
            "Epoch 160 Batch 150 Loss 0.2478 Accuracy 0.9331\n",
            "Epoch 160 Batch 200 Loss 0.2550 Accuracy 0.9313\n",
            "Epoch 160 Batch 250 Loss 0.2609 Accuracy 0.9298\n",
            "Epoch 160 Batch 300 Loss 0.2687 Accuracy 0.9279\n",
            "Saving checkpoint for epoch 160 at ./checkpoints/train\\ckpt-152\n",
            "Epoch 160 Loss 0.2710 Accuracy 0.9274\n",
            "Time taken for 1 epoch: 183.98 secs\n",
            "\n",
            "Epoch 161 Batch 0 Loss 0.2253 Accuracy 0.9432\n",
            "Epoch 161 Batch 50 Loss 0.2353 Accuracy 0.9360\n",
            "Epoch 161 Batch 100 Loss 0.2402 Accuracy 0.9350\n",
            "Epoch 161 Batch 150 Loss 0.2461 Accuracy 0.9333\n",
            "Epoch 161 Batch 200 Loss 0.2530 Accuracy 0.9313\n",
            "Epoch 161 Batch 250 Loss 0.2595 Accuracy 0.9298\n",
            "Epoch 161 Batch 300 Loss 0.2671 Accuracy 0.9280\n",
            "Epoch 161 Loss 0.2702 Accuracy 0.9272\n",
            "Time taken for 1 epoch: 182.60 secs\n",
            "\n",
            "Epoch 162 Batch 0 Loss 0.2129 Accuracy 0.9436\n",
            "Epoch 162 Batch 50 Loss 0.2413 Accuracy 0.9346\n",
            "Epoch 162 Batch 100 Loss 0.2476 Accuracy 0.9332\n",
            "Epoch 162 Batch 150 Loss 0.2505 Accuracy 0.9320\n",
            "Epoch 162 Batch 200 Loss 0.2553 Accuracy 0.9309\n",
            "Epoch 162 Batch 250 Loss 0.2626 Accuracy 0.9290\n",
            "Epoch 162 Batch 300 Loss 0.2689 Accuracy 0.9276\n",
            "Epoch 162 Loss 0.2717 Accuracy 0.9270\n",
            "Time taken for 1 epoch: 182.62 secs\n",
            "\n",
            "Epoch 163 Batch 0 Loss 0.2421 Accuracy 0.9265\n",
            "Epoch 163 Batch 50 Loss 0.2379 Accuracy 0.9348\n",
            "Epoch 163 Batch 100 Loss 0.2416 Accuracy 0.9352\n",
            "Epoch 163 Batch 150 Loss 0.2458 Accuracy 0.9338\n",
            "Epoch 163 Batch 200 Loss 0.2513 Accuracy 0.9324\n",
            "Epoch 163 Batch 250 Loss 0.2585 Accuracy 0.9306\n",
            "Epoch 163 Batch 300 Loss 0.2661 Accuracy 0.9288\n",
            "Epoch 163 Loss 0.2689 Accuracy 0.9282\n",
            "Time taken for 1 epoch: 182.86 secs\n",
            "\n",
            "Epoch 164 Batch 0 Loss 0.2247 Accuracy 0.9475\n",
            "Epoch 164 Batch 50 Loss 0.2367 Accuracy 0.9355\n",
            "Epoch 164 Batch 100 Loss 0.2426 Accuracy 0.9339\n",
            "Epoch 164 Batch 150 Loss 0.2485 Accuracy 0.9327\n",
            "Epoch 164 Batch 200 Loss 0.2541 Accuracy 0.9310\n",
            "Epoch 164 Batch 250 Loss 0.2618 Accuracy 0.9288\n",
            "Epoch 164 Batch 300 Loss 0.2684 Accuracy 0.9270\n",
            "Epoch 164 Loss 0.2712 Accuracy 0.9263\n",
            "Time taken for 1 epoch: 182.62 secs\n",
            "\n",
            "Epoch 165 Batch 0 Loss 0.2295 Accuracy 0.9331\n",
            "Epoch 165 Batch 50 Loss 0.2365 Accuracy 0.9365\n",
            "Epoch 165 Batch 100 Loss 0.2424 Accuracy 0.9353\n",
            "Epoch 165 Batch 150 Loss 0.2487 Accuracy 0.9334\n",
            "Epoch 165 Batch 200 Loss 0.2545 Accuracy 0.9317\n",
            "Epoch 165 Batch 250 Loss 0.2608 Accuracy 0.9297\n",
            "Epoch 165 Batch 300 Loss 0.2666 Accuracy 0.9284\n",
            "Saving checkpoint for epoch 165 at ./checkpoints/train\\ckpt-153\n",
            "Epoch 165 Loss 0.2695 Accuracy 0.9276\n",
            "Time taken for 1 epoch: 183.78 secs\n",
            "\n",
            "Epoch 166 Batch 0 Loss 0.2189 Accuracy 0.9382\n",
            "Epoch 166 Batch 50 Loss 0.2347 Accuracy 0.9372\n",
            "Epoch 166 Batch 100 Loss 0.2431 Accuracy 0.9343\n",
            "Epoch 166 Batch 150 Loss 0.2485 Accuracy 0.9330\n",
            "Epoch 166 Batch 200 Loss 0.2537 Accuracy 0.9313\n",
            "Epoch 166 Batch 250 Loss 0.2600 Accuracy 0.9298\n",
            "Epoch 166 Batch 300 Loss 0.2661 Accuracy 0.9282\n",
            "Epoch 166 Loss 0.2687 Accuracy 0.9276\n",
            "Time taken for 1 epoch: 182.58 secs\n",
            "\n",
            "Epoch 167 Batch 0 Loss 0.2219 Accuracy 0.9376\n",
            "Epoch 167 Batch 50 Loss 0.2341 Accuracy 0.9361\n",
            "Epoch 167 Batch 100 Loss 0.2446 Accuracy 0.9335\n",
            "Epoch 167 Batch 150 Loss 0.2494 Accuracy 0.9318\n",
            "Epoch 167 Batch 200 Loss 0.2554 Accuracy 0.9304\n",
            "Epoch 167 Batch 250 Loss 0.2617 Accuracy 0.9289\n",
            "Epoch 167 Batch 300 Loss 0.2681 Accuracy 0.9272\n",
            "Epoch 167 Loss 0.2705 Accuracy 0.9266\n",
            "Time taken for 1 epoch: 182.51 secs\n",
            "\n",
            "Epoch 168 Batch 0 Loss 0.2324 Accuracy 0.9266\n",
            "Epoch 168 Batch 50 Loss 0.2371 Accuracy 0.9359\n",
            "Epoch 168 Batch 100 Loss 0.2384 Accuracy 0.9362\n",
            "Epoch 168 Batch 150 Loss 0.2447 Accuracy 0.9340\n",
            "Epoch 168 Batch 200 Loss 0.2498 Accuracy 0.9326\n",
            "Epoch 168 Batch 250 Loss 0.2576 Accuracy 0.9306\n",
            "Epoch 168 Batch 300 Loss 0.2646 Accuracy 0.9288\n",
            "Epoch 168 Loss 0.2674 Accuracy 0.9283\n",
            "Time taken for 1 epoch: 182.60 secs\n",
            "\n",
            "Epoch 169 Batch 0 Loss 0.2482 Accuracy 0.9391\n",
            "Epoch 169 Batch 50 Loss 0.2332 Accuracy 0.9371\n",
            "Epoch 169 Batch 100 Loss 0.2386 Accuracy 0.9351\n",
            "Epoch 169 Batch 150 Loss 0.2448 Accuracy 0.9334\n",
            "Epoch 169 Batch 200 Loss 0.2508 Accuracy 0.9317\n",
            "Epoch 169 Batch 250 Loss 0.2583 Accuracy 0.9297\n",
            "Epoch 169 Batch 300 Loss 0.2655 Accuracy 0.9282\n",
            "Epoch 169 Loss 0.2684 Accuracy 0.9275\n",
            "Time taken for 1 epoch: 182.53 secs\n",
            "\n",
            "Epoch 170 Batch 0 Loss 0.2443 Accuracy 0.9352\n",
            "Epoch 170 Batch 50 Loss 0.2371 Accuracy 0.9371\n",
            "Epoch 170 Batch 100 Loss 0.2420 Accuracy 0.9347\n",
            "Epoch 170 Batch 150 Loss 0.2472 Accuracy 0.9334\n",
            "Epoch 170 Batch 200 Loss 0.2548 Accuracy 0.9315\n",
            "Epoch 170 Batch 250 Loss 0.2609 Accuracy 0.9296\n",
            "Epoch 170 Batch 300 Loss 0.2673 Accuracy 0.9281\n",
            "Saving checkpoint for epoch 170 at ./checkpoints/train\\ckpt-154\n",
            "Epoch 170 Loss 0.2700 Accuracy 0.9277\n",
            "Time taken for 1 epoch: 183.92 secs\n",
            "\n",
            "Epoch 171 Batch 0 Loss 0.2267 Accuracy 0.9358\n",
            "Epoch 171 Batch 50 Loss 0.2375 Accuracy 0.9359\n",
            "Epoch 171 Batch 100 Loss 0.2403 Accuracy 0.9349\n",
            "Epoch 171 Batch 150 Loss 0.2463 Accuracy 0.9329\n",
            "Epoch 171 Batch 200 Loss 0.2526 Accuracy 0.9313\n",
            "Epoch 171 Batch 250 Loss 0.2603 Accuracy 0.9292\n",
            "Epoch 171 Batch 300 Loss 0.2657 Accuracy 0.9278\n",
            "Epoch 171 Loss 0.2687 Accuracy 0.9271\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 172 Batch 0 Loss 0.2100 Accuracy 0.9410\n",
            "Epoch 172 Batch 50 Loss 0.2362 Accuracy 0.9361\n",
            "Epoch 172 Batch 100 Loss 0.2419 Accuracy 0.9344\n",
            "Epoch 172 Batch 150 Loss 0.2468 Accuracy 0.9332\n",
            "Epoch 172 Batch 200 Loss 0.2534 Accuracy 0.9314\n",
            "Epoch 172 Batch 250 Loss 0.2598 Accuracy 0.9299\n",
            "Epoch 172 Batch 300 Loss 0.2665 Accuracy 0.9284\n",
            "Epoch 172 Loss 0.2694 Accuracy 0.9277\n",
            "Time taken for 1 epoch: 182.55 secs\n",
            "\n",
            "Epoch 173 Batch 0 Loss 0.2551 Accuracy 0.9294\n",
            "Epoch 173 Batch 50 Loss 0.2354 Accuracy 0.9356\n",
            "Epoch 173 Batch 100 Loss 0.2425 Accuracy 0.9339\n",
            "Epoch 173 Batch 150 Loss 0.2487 Accuracy 0.9326\n",
            "Epoch 173 Batch 200 Loss 0.2532 Accuracy 0.9314\n",
            "Epoch 173 Batch 250 Loss 0.2601 Accuracy 0.9296\n",
            "Epoch 173 Batch 300 Loss 0.2678 Accuracy 0.9276\n",
            "Epoch 173 Loss 0.2708 Accuracy 0.9268\n",
            "Time taken for 1 epoch: 182.53 secs\n",
            "\n",
            "Epoch 174 Batch 0 Loss 0.2491 Accuracy 0.9350\n",
            "Epoch 174 Batch 50 Loss 0.2346 Accuracy 0.9359\n",
            "Epoch 174 Batch 100 Loss 0.2402 Accuracy 0.9349\n",
            "Epoch 174 Batch 150 Loss 0.2458 Accuracy 0.9335\n",
            "Epoch 174 Batch 200 Loss 0.2519 Accuracy 0.9317\n",
            "Epoch 174 Batch 250 Loss 0.2598 Accuracy 0.9296\n",
            "Epoch 174 Batch 300 Loss 0.2665 Accuracy 0.9279\n",
            "Epoch 174 Loss 0.2694 Accuracy 0.9272\n",
            "Time taken for 1 epoch: 182.51 secs\n",
            "\n",
            "Epoch 175 Batch 0 Loss 0.2075 Accuracy 0.9476\n",
            "Epoch 175 Batch 50 Loss 0.2354 Accuracy 0.9374\n",
            "Epoch 175 Batch 100 Loss 0.2441 Accuracy 0.9343\n",
            "Epoch 175 Batch 150 Loss 0.2490 Accuracy 0.9328\n",
            "Epoch 175 Batch 200 Loss 0.2545 Accuracy 0.9311\n",
            "Epoch 175 Batch 250 Loss 0.2611 Accuracy 0.9295\n",
            "Epoch 175 Batch 300 Loss 0.2678 Accuracy 0.9278\n",
            "Saving checkpoint for epoch 175 at ./checkpoints/train\\ckpt-155\n",
            "Epoch 175 Loss 0.2708 Accuracy 0.9270\n",
            "Time taken for 1 epoch: 183.67 secs\n",
            "\n",
            "Epoch 176 Batch 0 Loss 0.2411 Accuracy 0.9292\n",
            "Epoch 176 Batch 50 Loss 0.2315 Accuracy 0.9370\n",
            "Epoch 176 Batch 100 Loss 0.2391 Accuracy 0.9358\n",
            "Epoch 176 Batch 150 Loss 0.2453 Accuracy 0.9340\n",
            "Epoch 176 Batch 200 Loss 0.2512 Accuracy 0.9323\n",
            "Epoch 176 Batch 250 Loss 0.2586 Accuracy 0.9302\n",
            "Epoch 176 Batch 300 Loss 0.2643 Accuracy 0.9290\n",
            "Epoch 176 Loss 0.2663 Accuracy 0.9284\n",
            "Time taken for 1 epoch: 182.73 secs\n",
            "\n",
            "Epoch 177 Batch 0 Loss 0.2570 Accuracy 0.9345\n",
            "Epoch 177 Batch 50 Loss 0.2374 Accuracy 0.9364\n",
            "Epoch 177 Batch 100 Loss 0.2407 Accuracy 0.9351\n",
            "Epoch 177 Batch 150 Loss 0.2461 Accuracy 0.9333\n",
            "Epoch 177 Batch 200 Loss 0.2515 Accuracy 0.9322\n",
            "Epoch 177 Batch 250 Loss 0.2584 Accuracy 0.9306\n",
            "Epoch 177 Batch 300 Loss 0.2655 Accuracy 0.9288\n",
            "Epoch 177 Loss 0.2685 Accuracy 0.9280\n",
            "Time taken for 1 epoch: 182.57 secs\n",
            "\n",
            "Epoch 178 Batch 0 Loss 0.1943 Accuracy 0.9480\n",
            "Epoch 178 Batch 50 Loss 0.2328 Accuracy 0.9376\n",
            "Epoch 178 Batch 100 Loss 0.2374 Accuracy 0.9366\n",
            "Epoch 178 Batch 150 Loss 0.2446 Accuracy 0.9341\n",
            "Epoch 178 Batch 200 Loss 0.2510 Accuracy 0.9323\n",
            "Epoch 178 Batch 250 Loss 0.2581 Accuracy 0.9304\n",
            "Epoch 178 Batch 300 Loss 0.2649 Accuracy 0.9288\n",
            "Epoch 178 Loss 0.2683 Accuracy 0.9278\n",
            "Time taken for 1 epoch: 182.59 secs\n",
            "\n",
            "Epoch 179 Batch 0 Loss 0.2227 Accuracy 0.9374\n",
            "Epoch 179 Batch 50 Loss 0.2321 Accuracy 0.9380\n",
            "Epoch 179 Batch 100 Loss 0.2388 Accuracy 0.9362\n",
            "Epoch 179 Batch 150 Loss 0.2431 Accuracy 0.9346\n",
            "Epoch 179 Batch 200 Loss 0.2496 Accuracy 0.9325\n",
            "Epoch 179 Batch 250 Loss 0.2565 Accuracy 0.9309\n",
            "Epoch 179 Batch 300 Loss 0.2635 Accuracy 0.9291\n",
            "Epoch 179 Loss 0.2660 Accuracy 0.9286\n",
            "Time taken for 1 epoch: 182.57 secs\n",
            "\n",
            "Epoch 180 Batch 0 Loss 0.2455 Accuracy 0.9400\n",
            "Epoch 180 Batch 50 Loss 0.2320 Accuracy 0.9364\n",
            "Epoch 180 Batch 100 Loss 0.2360 Accuracy 0.9359\n",
            "Epoch 180 Batch 150 Loss 0.2422 Accuracy 0.9346\n",
            "Epoch 180 Batch 200 Loss 0.2473 Accuracy 0.9334\n",
            "Epoch 180 Batch 250 Loss 0.2536 Accuracy 0.9314\n",
            "Epoch 180 Batch 300 Loss 0.2610 Accuracy 0.9296\n",
            "Saving checkpoint for epoch 180 at ./checkpoints/train\\ckpt-156\n",
            "Epoch 180 Loss 0.2641 Accuracy 0.9289\n",
            "Time taken for 1 epoch: 183.58 secs\n",
            "\n",
            "Epoch 181 Batch 0 Loss 0.2225 Accuracy 0.9436\n",
            "Epoch 181 Batch 50 Loss 0.2369 Accuracy 0.9370\n",
            "Epoch 181 Batch 100 Loss 0.2389 Accuracy 0.9360\n",
            "Epoch 181 Batch 150 Loss 0.2452 Accuracy 0.9340\n",
            "Epoch 181 Batch 200 Loss 0.2490 Accuracy 0.9328\n",
            "Epoch 181 Batch 250 Loss 0.2569 Accuracy 0.9308\n",
            "Epoch 181 Batch 300 Loss 0.2633 Accuracy 0.9293\n",
            "Epoch 181 Loss 0.2667 Accuracy 0.9284\n",
            "Time taken for 1 epoch: 182.56 secs\n",
            "\n",
            "Epoch 182 Batch 0 Loss 0.2233 Accuracy 0.9378\n",
            "Epoch 182 Batch 50 Loss 0.2336 Accuracy 0.9360\n",
            "Epoch 182 Batch 100 Loss 0.2395 Accuracy 0.9349\n",
            "Epoch 182 Batch 150 Loss 0.2449 Accuracy 0.9333\n",
            "Epoch 182 Batch 200 Loss 0.2501 Accuracy 0.9321\n",
            "Epoch 182 Batch 250 Loss 0.2559 Accuracy 0.9305\n",
            "Epoch 182 Batch 300 Loss 0.2632 Accuracy 0.9291\n",
            "Epoch 182 Loss 0.2665 Accuracy 0.9283\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 183 Batch 0 Loss 0.2322 Accuracy 0.9358\n",
            "Epoch 183 Batch 50 Loss 0.2329 Accuracy 0.9366\n",
            "Epoch 183 Batch 100 Loss 0.2398 Accuracy 0.9350\n",
            "Epoch 183 Batch 150 Loss 0.2432 Accuracy 0.9343\n",
            "Epoch 183 Batch 200 Loss 0.2499 Accuracy 0.9326\n",
            "Epoch 183 Batch 250 Loss 0.2552 Accuracy 0.9313\n",
            "Epoch 183 Batch 300 Loss 0.2616 Accuracy 0.9297\n",
            "Epoch 183 Loss 0.2645 Accuracy 0.9290\n",
            "Time taken for 1 epoch: 182.67 secs\n",
            "\n",
            "Epoch 184 Batch 0 Loss 0.2393 Accuracy 0.9361\n",
            "Epoch 184 Batch 50 Loss 0.2306 Accuracy 0.9393\n",
            "Epoch 184 Batch 100 Loss 0.2383 Accuracy 0.9365\n",
            "Epoch 184 Batch 150 Loss 0.2452 Accuracy 0.9342\n",
            "Epoch 184 Batch 200 Loss 0.2507 Accuracy 0.9328\n",
            "Epoch 184 Batch 250 Loss 0.2571 Accuracy 0.9310\n",
            "Epoch 184 Batch 300 Loss 0.2639 Accuracy 0.9293\n",
            "Epoch 184 Loss 0.2670 Accuracy 0.9285\n",
            "Time taken for 1 epoch: 182.49 secs\n",
            "\n",
            "Epoch 185 Batch 0 Loss 0.2586 Accuracy 0.9266\n",
            "Epoch 185 Batch 50 Loss 0.2332 Accuracy 0.9366\n",
            "Epoch 185 Batch 100 Loss 0.2378 Accuracy 0.9359\n",
            "Epoch 185 Batch 150 Loss 0.2423 Accuracy 0.9347\n",
            "Epoch 185 Batch 200 Loss 0.2476 Accuracy 0.9333\n",
            "Epoch 185 Batch 250 Loss 0.2538 Accuracy 0.9314\n",
            "Epoch 185 Batch 300 Loss 0.2620 Accuracy 0.9294\n",
            "Saving checkpoint for epoch 185 at ./checkpoints/train\\ckpt-157\n",
            "Epoch 185 Loss 0.2651 Accuracy 0.9287\n",
            "Time taken for 1 epoch: 183.96 secs\n",
            "\n",
            "Epoch 186 Batch 0 Loss 0.2175 Accuracy 0.9356\n",
            "Epoch 186 Batch 50 Loss 0.2403 Accuracy 0.9356\n",
            "Epoch 186 Batch 100 Loss 0.2462 Accuracy 0.9341\n",
            "Epoch 186 Batch 150 Loss 0.2494 Accuracy 0.9331\n",
            "Epoch 186 Batch 200 Loss 0.2540 Accuracy 0.9317\n",
            "Epoch 186 Batch 250 Loss 0.2594 Accuracy 0.9301\n",
            "Epoch 186 Batch 300 Loss 0.2662 Accuracy 0.9285\n",
            "Epoch 186 Loss 0.2687 Accuracy 0.9279\n",
            "Time taken for 1 epoch: 182.48 secs\n",
            "\n",
            "Epoch 187 Batch 0 Loss 0.2486 Accuracy 0.9273\n",
            "Epoch 187 Batch 50 Loss 0.2297 Accuracy 0.9394\n",
            "Epoch 187 Batch 100 Loss 0.2386 Accuracy 0.9364\n",
            "Epoch 187 Batch 150 Loss 0.2433 Accuracy 0.9348\n",
            "Epoch 187 Batch 200 Loss 0.2493 Accuracy 0.9329\n",
            "Epoch 187 Batch 250 Loss 0.2551 Accuracy 0.9316\n",
            "Epoch 187 Batch 300 Loss 0.2620 Accuracy 0.9298\n",
            "Epoch 187 Loss 0.2652 Accuracy 0.9289\n",
            "Time taken for 1 epoch: 182.56 secs\n",
            "\n",
            "Epoch 188 Batch 0 Loss 0.2033 Accuracy 0.9476\n",
            "Epoch 188 Batch 50 Loss 0.2351 Accuracy 0.9360\n",
            "Epoch 188 Batch 100 Loss 0.2402 Accuracy 0.9353\n",
            "Epoch 188 Batch 150 Loss 0.2460 Accuracy 0.9337\n",
            "Epoch 188 Batch 200 Loss 0.2510 Accuracy 0.9325\n",
            "Epoch 188 Batch 250 Loss 0.2568 Accuracy 0.9310\n",
            "Epoch 188 Batch 300 Loss 0.2641 Accuracy 0.9291\n",
            "Epoch 188 Loss 0.2668 Accuracy 0.9284\n",
            "Time taken for 1 epoch: 182.51 secs\n",
            "\n",
            "Epoch 189 Batch 0 Loss 0.2354 Accuracy 0.9286\n",
            "Epoch 189 Batch 50 Loss 0.2330 Accuracy 0.9377\n",
            "Epoch 189 Batch 100 Loss 0.2384 Accuracy 0.9359\n",
            "Epoch 189 Batch 150 Loss 0.2444 Accuracy 0.9342\n",
            "Epoch 189 Batch 200 Loss 0.2509 Accuracy 0.9325\n",
            "Epoch 189 Batch 250 Loss 0.2569 Accuracy 0.9309\n",
            "Epoch 189 Batch 300 Loss 0.2644 Accuracy 0.9291\n",
            "Epoch 189 Loss 0.2675 Accuracy 0.9284\n",
            "Time taken for 1 epoch: 183.36 secs\n",
            "\n",
            "Epoch 190 Batch 0 Loss 0.2168 Accuracy 0.9360\n",
            "Epoch 190 Batch 50 Loss 0.2330 Accuracy 0.9358\n",
            "Epoch 190 Batch 100 Loss 0.2364 Accuracy 0.9358\n",
            "Epoch 190 Batch 150 Loss 0.2421 Accuracy 0.9342\n",
            "Epoch 190 Batch 200 Loss 0.2488 Accuracy 0.9327\n",
            "Epoch 190 Batch 250 Loss 0.2548 Accuracy 0.9314\n",
            "Epoch 190 Batch 300 Loss 0.2626 Accuracy 0.9293\n",
            "Saving checkpoint for epoch 190 at ./checkpoints/train\\ckpt-158\n",
            "Epoch 190 Loss 0.2646 Accuracy 0.9288\n",
            "Time taken for 1 epoch: 183.73 secs\n",
            "\n",
            "Epoch 191 Batch 0 Loss 0.2436 Accuracy 0.9376\n",
            "Epoch 191 Batch 50 Loss 0.2359 Accuracy 0.9368\n",
            "Epoch 191 Batch 100 Loss 0.2365 Accuracy 0.9363\n",
            "Epoch 191 Batch 150 Loss 0.2431 Accuracy 0.9342\n",
            "Epoch 191 Batch 200 Loss 0.2493 Accuracy 0.9327\n",
            "Epoch 191 Batch 250 Loss 0.2564 Accuracy 0.9308\n",
            "Epoch 191 Batch 300 Loss 0.2637 Accuracy 0.9290\n",
            "Epoch 191 Loss 0.2659 Accuracy 0.9285\n",
            "Time taken for 1 epoch: 182.55 secs\n",
            "\n",
            "Epoch 192 Batch 0 Loss 0.2474 Accuracy 0.9363\n",
            "Epoch 192 Batch 50 Loss 0.2283 Accuracy 0.9393\n",
            "Epoch 192 Batch 100 Loss 0.2354 Accuracy 0.9366\n",
            "Epoch 192 Batch 150 Loss 0.2411 Accuracy 0.9350\n",
            "Epoch 192 Batch 200 Loss 0.2483 Accuracy 0.9331\n",
            "Epoch 192 Batch 250 Loss 0.2553 Accuracy 0.9314\n",
            "Epoch 192 Batch 300 Loss 0.2630 Accuracy 0.9296\n",
            "Epoch 192 Loss 0.2659 Accuracy 0.9289\n",
            "Time taken for 1 epoch: 183.29 secs\n",
            "\n",
            "Epoch 193 Batch 0 Loss 0.1919 Accuracy 0.9453\n",
            "Epoch 193 Batch 50 Loss 0.2350 Accuracy 0.9361\n",
            "Epoch 193 Batch 100 Loss 0.2405 Accuracy 0.9346\n",
            "Epoch 193 Batch 150 Loss 0.2422 Accuracy 0.9344\n",
            "Epoch 193 Batch 200 Loss 0.2490 Accuracy 0.9324\n",
            "Epoch 193 Batch 250 Loss 0.2560 Accuracy 0.9308\n",
            "Epoch 193 Batch 300 Loss 0.2633 Accuracy 0.9292\n",
            "Epoch 193 Loss 0.2661 Accuracy 0.9286\n",
            "Time taken for 1 epoch: 183.61 secs\n",
            "\n",
            "Epoch 194 Batch 0 Loss 0.2296 Accuracy 0.9448\n",
            "Epoch 194 Batch 50 Loss 0.2299 Accuracy 0.9381\n",
            "Epoch 194 Batch 100 Loss 0.2359 Accuracy 0.9368\n",
            "Epoch 194 Batch 150 Loss 0.2421 Accuracy 0.9350\n",
            "Epoch 194 Batch 200 Loss 0.2475 Accuracy 0.9330\n",
            "Epoch 194 Batch 250 Loss 0.2544 Accuracy 0.9312\n",
            "Epoch 194 Batch 300 Loss 0.2610 Accuracy 0.9294\n",
            "Epoch 194 Loss 0.2645 Accuracy 0.9287\n",
            "Time taken for 1 epoch: 183.37 secs\n",
            "\n",
            "Epoch 195 Batch 0 Loss 0.1989 Accuracy 0.9482\n",
            "Epoch 195 Batch 50 Loss 0.2319 Accuracy 0.9378\n",
            "Epoch 195 Batch 100 Loss 0.2373 Accuracy 0.9366\n",
            "Epoch 195 Batch 150 Loss 0.2422 Accuracy 0.9347\n",
            "Epoch 195 Batch 200 Loss 0.2478 Accuracy 0.9334\n",
            "Epoch 195 Batch 250 Loss 0.2542 Accuracy 0.9318\n",
            "Epoch 195 Batch 300 Loss 0.2614 Accuracy 0.9299\n",
            "Saving checkpoint for epoch 195 at ./checkpoints/train\\ckpt-159\n",
            "Epoch 195 Loss 0.2638 Accuracy 0.9293\n",
            "Time taken for 1 epoch: 183.84 secs\n",
            "\n",
            "Epoch 196 Batch 0 Loss 0.2247 Accuracy 0.9450\n",
            "Epoch 196 Batch 50 Loss 0.2342 Accuracy 0.9369\n",
            "Epoch 196 Batch 100 Loss 0.2374 Accuracy 0.9355\n",
            "Epoch 196 Batch 150 Loss 0.2445 Accuracy 0.9338\n",
            "Epoch 196 Batch 200 Loss 0.2506 Accuracy 0.9323\n",
            "Epoch 196 Batch 250 Loss 0.2559 Accuracy 0.9309\n",
            "Epoch 196 Batch 300 Loss 0.2619 Accuracy 0.9296\n",
            "Epoch 196 Loss 0.2644 Accuracy 0.9289\n",
            "Time taken for 1 epoch: 182.52 secs\n",
            "\n",
            "Epoch 197 Batch 0 Loss 0.1956 Accuracy 0.9520\n",
            "Epoch 197 Batch 50 Loss 0.2286 Accuracy 0.9391\n",
            "Epoch 197 Batch 100 Loss 0.2344 Accuracy 0.9370\n",
            "Epoch 197 Batch 150 Loss 0.2408 Accuracy 0.9352\n",
            "Epoch 197 Batch 200 Loss 0.2478 Accuracy 0.9330\n",
            "Epoch 197 Batch 250 Loss 0.2535 Accuracy 0.9314\n",
            "Epoch 197 Batch 300 Loss 0.2612 Accuracy 0.9297\n",
            "Epoch 197 Loss 0.2637 Accuracy 0.9290\n",
            "Time taken for 1 epoch: 182.62 secs\n",
            "\n",
            "Epoch 198 Batch 0 Loss 0.2340 Accuracy 0.9390\n",
            "Epoch 198 Batch 50 Loss 0.2251 Accuracy 0.9408\n",
            "Epoch 198 Batch 100 Loss 0.2357 Accuracy 0.9369\n",
            "Epoch 198 Batch 150 Loss 0.2416 Accuracy 0.9353\n",
            "Epoch 198 Batch 200 Loss 0.2474 Accuracy 0.9339\n",
            "Epoch 198 Batch 250 Loss 0.2543 Accuracy 0.9320\n",
            "Epoch 198 Batch 300 Loss 0.2613 Accuracy 0.9302\n",
            "Epoch 198 Loss 0.2640 Accuracy 0.9295\n",
            "Time taken for 1 epoch: 182.55 secs\n",
            "\n",
            "Epoch 199 Batch 0 Loss 0.2076 Accuracy 0.9429\n",
            "Epoch 199 Batch 50 Loss 0.2311 Accuracy 0.9373\n",
            "Epoch 199 Batch 100 Loss 0.2354 Accuracy 0.9362\n",
            "Epoch 199 Batch 150 Loss 0.2419 Accuracy 0.9345\n",
            "Epoch 199 Batch 200 Loss 0.2472 Accuracy 0.9332\n",
            "Epoch 199 Batch 250 Loss 0.2537 Accuracy 0.9314\n",
            "Epoch 199 Batch 300 Loss 0.2610 Accuracy 0.9296\n",
            "Epoch 199 Loss 0.2641 Accuracy 0.9289\n",
            "Time taken for 1 epoch: 182.67 secs\n",
            "\n",
            "Epoch 200 Batch 0 Loss 0.2163 Accuracy 0.9331\n",
            "Epoch 200 Batch 50 Loss 0.2320 Accuracy 0.9364\n",
            "Epoch 200 Batch 100 Loss 0.2378 Accuracy 0.9349\n",
            "Epoch 200 Batch 150 Loss 0.2418 Accuracy 0.9339\n",
            "Epoch 200 Batch 200 Loss 0.2452 Accuracy 0.9332\n",
            "Epoch 200 Batch 250 Loss 0.2520 Accuracy 0.9316\n",
            "Epoch 200 Batch 300 Loss 0.2585 Accuracy 0.9300\n",
            "Saving checkpoint for epoch 200 at ./checkpoints/train\\ckpt-160\n",
            "Epoch 200 Loss 0.2613 Accuracy 0.9294\n",
            "Time taken for 1 epoch: 183.41 secs\n",
            "\n",
            "Epoch 201 Batch 0 Loss 0.2283 Accuracy 0.9337\n",
            "Epoch 201 Batch 50 Loss 0.2323 Accuracy 0.9373\n",
            "Epoch 201 Batch 100 Loss 0.2343 Accuracy 0.9377\n",
            "Epoch 201 Batch 150 Loss 0.2393 Accuracy 0.9359\n",
            "Epoch 201 Batch 200 Loss 0.2457 Accuracy 0.9343\n",
            "Epoch 201 Batch 250 Loss 0.2523 Accuracy 0.9322\n",
            "Epoch 201 Batch 300 Loss 0.2598 Accuracy 0.9303\n",
            "Epoch 201 Loss 0.2625 Accuracy 0.9296\n",
            "Time taken for 1 epoch: 183.39 secs\n",
            "\n",
            "Epoch 202 Batch 0 Loss 0.2169 Accuracy 0.9479\n",
            "Epoch 202 Batch 50 Loss 0.2331 Accuracy 0.9370\n",
            "Epoch 202 Batch 100 Loss 0.2379 Accuracy 0.9359\n",
            "Epoch 202 Batch 150 Loss 0.2414 Accuracy 0.9345\n",
            "Epoch 202 Batch 200 Loss 0.2479 Accuracy 0.9330\n",
            "Epoch 202 Batch 250 Loss 0.2543 Accuracy 0.9313\n",
            "Epoch 202 Batch 300 Loss 0.2609 Accuracy 0.9295\n",
            "Epoch 202 Loss 0.2635 Accuracy 0.9289\n",
            "Time taken for 1 epoch: 182.62 secs\n",
            "\n",
            "Epoch 203 Batch 0 Loss 0.2004 Accuracy 0.9430\n",
            "Epoch 203 Batch 50 Loss 0.2313 Accuracy 0.9369\n",
            "Epoch 203 Batch 100 Loss 0.2346 Accuracy 0.9367\n",
            "Epoch 203 Batch 150 Loss 0.2408 Accuracy 0.9347\n",
            "Epoch 203 Batch 200 Loss 0.2457 Accuracy 0.9339\n",
            "Epoch 203 Batch 250 Loss 0.2505 Accuracy 0.9326\n",
            "Epoch 203 Batch 300 Loss 0.2583 Accuracy 0.9305\n",
            "Epoch 203 Loss 0.2613 Accuracy 0.9298\n",
            "Time taken for 1 epoch: 183.12 secs\n",
            "\n",
            "Epoch 204 Batch 0 Loss 0.1936 Accuracy 0.9445\n",
            "Epoch 204 Batch 50 Loss 0.2398 Accuracy 0.9358\n",
            "Epoch 204 Batch 100 Loss 0.2394 Accuracy 0.9352\n",
            "Epoch 204 Batch 150 Loss 0.2419 Accuracy 0.9347\n",
            "Epoch 204 Batch 200 Loss 0.2471 Accuracy 0.9332\n",
            "Epoch 204 Batch 250 Loss 0.2529 Accuracy 0.9320\n",
            "Epoch 204 Batch 300 Loss 0.2590 Accuracy 0.9304\n",
            "Epoch 204 Loss 0.2620 Accuracy 0.9297\n",
            "Time taken for 1 epoch: 182.58 secs\n",
            "\n",
            "Epoch 205 Batch 0 Loss 0.2205 Accuracy 0.9344\n",
            "Epoch 205 Batch 50 Loss 0.2352 Accuracy 0.9362\n",
            "Epoch 205 Batch 100 Loss 0.2361 Accuracy 0.9361\n",
            "Epoch 205 Batch 150 Loss 0.2412 Accuracy 0.9346\n",
            "Epoch 205 Batch 200 Loss 0.2467 Accuracy 0.9332\n",
            "Epoch 205 Batch 250 Loss 0.2527 Accuracy 0.9317\n",
            "Epoch 205 Batch 300 Loss 0.2597 Accuracy 0.9300\n",
            "Saving checkpoint for epoch 205 at ./checkpoints/train\\ckpt-161\n",
            "Epoch 205 Loss 0.2632 Accuracy 0.9291\n",
            "Time taken for 1 epoch: 183.91 secs\n",
            "\n",
            "Epoch 206 Batch 0 Loss 0.1926 Accuracy 0.9527\n",
            "Epoch 206 Batch 50 Loss 0.2256 Accuracy 0.9390\n",
            "Epoch 206 Batch 100 Loss 0.2345 Accuracy 0.9364\n",
            "Epoch 206 Batch 150 Loss 0.2410 Accuracy 0.9347\n",
            "Epoch 206 Batch 200 Loss 0.2471 Accuracy 0.9331\n",
            "Epoch 206 Batch 250 Loss 0.2548 Accuracy 0.9309\n",
            "Epoch 206 Batch 300 Loss 0.2612 Accuracy 0.9293\n",
            "Epoch 206 Loss 0.2635 Accuracy 0.9288\n",
            "Time taken for 1 epoch: 182.56 secs\n",
            "\n",
            "Epoch 207 Batch 0 Loss 0.2194 Accuracy 0.9444\n",
            "Epoch 207 Batch 50 Loss 0.2330 Accuracy 0.9365\n",
            "Epoch 207 Batch 100 Loss 0.2364 Accuracy 0.9356\n",
            "Epoch 207 Batch 150 Loss 0.2410 Accuracy 0.9342\n",
            "Epoch 207 Batch 200 Loss 0.2475 Accuracy 0.9324\n",
            "Epoch 207 Batch 250 Loss 0.2537 Accuracy 0.9311\n",
            "Epoch 207 Batch 300 Loss 0.2597 Accuracy 0.9297\n",
            "Epoch 207 Loss 0.2621 Accuracy 0.9290\n",
            "Time taken for 1 epoch: 182.52 secs\n",
            "\n",
            "Epoch 208 Batch 0 Loss 0.2675 Accuracy 0.9260\n",
            "Epoch 208 Batch 50 Loss 0.2304 Accuracy 0.9389\n",
            "Epoch 208 Batch 100 Loss 0.2346 Accuracy 0.9381\n",
            "Epoch 208 Batch 150 Loss 0.2404 Accuracy 0.9360\n",
            "Epoch 208 Batch 200 Loss 0.2468 Accuracy 0.9341\n",
            "Epoch 208 Batch 250 Loss 0.2533 Accuracy 0.9324\n",
            "Epoch 208 Batch 300 Loss 0.2598 Accuracy 0.9310\n",
            "Epoch 208 Loss 0.2625 Accuracy 0.9303\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 209 Batch 0 Loss 0.2040 Accuracy 0.9456\n",
            "Epoch 209 Batch 50 Loss 0.2339 Accuracy 0.9360\n",
            "Epoch 209 Batch 100 Loss 0.2391 Accuracy 0.9349\n",
            "Epoch 209 Batch 150 Loss 0.2441 Accuracy 0.9338\n",
            "Epoch 209 Batch 200 Loss 0.2489 Accuracy 0.9327\n",
            "Epoch 209 Batch 250 Loss 0.2557 Accuracy 0.9311\n",
            "Epoch 209 Batch 300 Loss 0.2621 Accuracy 0.9297\n",
            "Epoch 209 Loss 0.2650 Accuracy 0.9290\n",
            "Time taken for 1 epoch: 182.46 secs\n",
            "\n",
            "Epoch 210 Batch 0 Loss 0.2188 Accuracy 0.9457\n",
            "Epoch 210 Batch 50 Loss 0.2274 Accuracy 0.9376\n",
            "Epoch 210 Batch 100 Loss 0.2341 Accuracy 0.9364\n",
            "Epoch 210 Batch 150 Loss 0.2390 Accuracy 0.9352\n",
            "Epoch 210 Batch 200 Loss 0.2458 Accuracy 0.9331\n",
            "Epoch 210 Batch 250 Loss 0.2503 Accuracy 0.9323\n",
            "Epoch 210 Batch 300 Loss 0.2577 Accuracy 0.9304\n",
            "Saving checkpoint for epoch 210 at ./checkpoints/train\\ckpt-162\n",
            "Epoch 210 Loss 0.2605 Accuracy 0.9297\n",
            "Time taken for 1 epoch: 183.79 secs\n",
            "\n",
            "Epoch 211 Batch 0 Loss 0.2262 Accuracy 0.9425\n",
            "Epoch 211 Batch 50 Loss 0.2317 Accuracy 0.9385\n",
            "Epoch 211 Batch 100 Loss 0.2357 Accuracy 0.9369\n",
            "Epoch 211 Batch 150 Loss 0.2412 Accuracy 0.9353\n",
            "Epoch 211 Batch 200 Loss 0.2473 Accuracy 0.9338\n",
            "Epoch 211 Batch 250 Loss 0.2521 Accuracy 0.9327\n",
            "Epoch 211 Batch 300 Loss 0.2592 Accuracy 0.9309\n",
            "Epoch 211 Loss 0.2616 Accuracy 0.9302\n",
            "Time taken for 1 epoch: 182.52 secs\n",
            "\n",
            "Epoch 212 Batch 0 Loss 0.2033 Accuracy 0.9475\n",
            "Epoch 212 Batch 50 Loss 0.2319 Accuracy 0.9374\n",
            "Epoch 212 Batch 100 Loss 0.2372 Accuracy 0.9356\n",
            "Epoch 212 Batch 150 Loss 0.2412 Accuracy 0.9345\n",
            "Epoch 212 Batch 200 Loss 0.2469 Accuracy 0.9329\n",
            "Epoch 212 Batch 250 Loss 0.2523 Accuracy 0.9317\n",
            "Epoch 212 Batch 300 Loss 0.2575 Accuracy 0.9304\n",
            "Epoch 212 Loss 0.2605 Accuracy 0.9297\n",
            "Time taken for 1 epoch: 182.52 secs\n",
            "\n",
            "Epoch 213 Batch 0 Loss 0.2340 Accuracy 0.9333\n",
            "Epoch 213 Batch 50 Loss 0.2226 Accuracy 0.9406\n",
            "Epoch 213 Batch 100 Loss 0.2299 Accuracy 0.9378\n",
            "Epoch 213 Batch 150 Loss 0.2367 Accuracy 0.9365\n",
            "Epoch 213 Batch 200 Loss 0.2437 Accuracy 0.9342\n",
            "Epoch 213 Batch 250 Loss 0.2509 Accuracy 0.9323\n",
            "Epoch 213 Batch 300 Loss 0.2579 Accuracy 0.9305\n",
            "Epoch 213 Loss 0.2605 Accuracy 0.9299\n",
            "Time taken for 1 epoch: 182.52 secs\n",
            "\n",
            "Epoch 214 Batch 0 Loss 0.2109 Accuracy 0.9505\n",
            "Epoch 214 Batch 50 Loss 0.2286 Accuracy 0.9383\n",
            "Epoch 214 Batch 100 Loss 0.2337 Accuracy 0.9367\n",
            "Epoch 214 Batch 150 Loss 0.2398 Accuracy 0.9354\n",
            "Epoch 214 Batch 200 Loss 0.2461 Accuracy 0.9333\n",
            "Epoch 214 Batch 250 Loss 0.2521 Accuracy 0.9319\n",
            "Epoch 214 Batch 300 Loss 0.2591 Accuracy 0.9304\n",
            "Epoch 214 Loss 0.2616 Accuracy 0.9300\n",
            "Time taken for 1 epoch: 182.54 secs\n",
            "\n",
            "Epoch 215 Batch 0 Loss 0.2021 Accuracy 0.9507\n",
            "Epoch 215 Batch 50 Loss 0.2262 Accuracy 0.9395\n",
            "Epoch 215 Batch 100 Loss 0.2337 Accuracy 0.9369\n",
            "Epoch 215 Batch 150 Loss 0.2385 Accuracy 0.9358\n",
            "Epoch 215 Batch 200 Loss 0.2440 Accuracy 0.9341\n",
            "Epoch 215 Batch 250 Loss 0.2509 Accuracy 0.9323\n",
            "Epoch 215 Batch 300 Loss 0.2579 Accuracy 0.9304\n",
            "Saving checkpoint for epoch 215 at ./checkpoints/train\\ckpt-163\n",
            "Epoch 215 Loss 0.2615 Accuracy 0.9297\n",
            "Time taken for 1 epoch: 183.81 secs\n",
            "\n",
            "Epoch 216 Batch 0 Loss 0.2044 Accuracy 0.9361\n",
            "Epoch 216 Batch 50 Loss 0.2284 Accuracy 0.9389\n",
            "Epoch 216 Batch 100 Loss 0.2358 Accuracy 0.9365\n",
            "Epoch 216 Batch 150 Loss 0.2403 Accuracy 0.9351\n",
            "Epoch 216 Batch 200 Loss 0.2468 Accuracy 0.9333\n",
            "Epoch 216 Batch 250 Loss 0.2524 Accuracy 0.9320\n",
            "Epoch 216 Batch 300 Loss 0.2599 Accuracy 0.9302\n",
            "Epoch 216 Loss 0.2627 Accuracy 0.9297\n",
            "Time taken for 1 epoch: 183.15 secs\n",
            "\n",
            "Epoch 217 Batch 0 Loss 0.2325 Accuracy 0.9371\n",
            "Epoch 217 Batch 50 Loss 0.2286 Accuracy 0.9387\n",
            "Epoch 217 Batch 100 Loss 0.2330 Accuracy 0.9382\n",
            "Epoch 217 Batch 150 Loss 0.2388 Accuracy 0.9360\n",
            "Epoch 217 Batch 200 Loss 0.2436 Accuracy 0.9348\n",
            "Epoch 217 Batch 250 Loss 0.2506 Accuracy 0.9328\n",
            "Epoch 217 Batch 300 Loss 0.2584 Accuracy 0.9306\n",
            "Epoch 217 Loss 0.2621 Accuracy 0.9298\n",
            "Time taken for 1 epoch: 182.50 secs\n",
            "\n",
            "Epoch 218 Batch 0 Loss 0.1784 Accuracy 0.9593\n",
            "Epoch 218 Batch 50 Loss 0.2304 Accuracy 0.9386\n",
            "Epoch 218 Batch 100 Loss 0.2336 Accuracy 0.9375\n",
            "Epoch 218 Batch 150 Loss 0.2372 Accuracy 0.9363\n",
            "Epoch 218 Batch 200 Loss 0.2436 Accuracy 0.9345\n",
            "Epoch 218 Batch 250 Loss 0.2507 Accuracy 0.9326\n",
            "Epoch 218 Batch 300 Loss 0.2567 Accuracy 0.9310\n",
            "Epoch 218 Loss 0.2590 Accuracy 0.9305\n",
            "Time taken for 1 epoch: 186.84 secs\n",
            "\n",
            "Epoch 219 Batch 0 Loss 0.2335 Accuracy 0.9411\n",
            "Epoch 219 Batch 50 Loss 0.2313 Accuracy 0.9383\n",
            "Epoch 219 Batch 100 Loss 0.2361 Accuracy 0.9366\n",
            "Epoch 219 Batch 150 Loss 0.2384 Accuracy 0.9356\n",
            "Epoch 219 Batch 200 Loss 0.2452 Accuracy 0.9340\n",
            "Epoch 219 Batch 250 Loss 0.2516 Accuracy 0.9323\n",
            "Epoch 219 Batch 300 Loss 0.2589 Accuracy 0.9306\n",
            "Epoch 219 Loss 0.2615 Accuracy 0.9301\n",
            "Time taken for 1 epoch: 187.17 secs\n",
            "\n",
            "Epoch 220 Batch 0 Loss 0.2215 Accuracy 0.9418\n",
            "Epoch 220 Batch 50 Loss 0.2211 Accuracy 0.9413\n",
            "Epoch 220 Batch 100 Loss 0.2305 Accuracy 0.9385\n",
            "Epoch 220 Batch 150 Loss 0.2368 Accuracy 0.9367\n",
            "Epoch 220 Batch 200 Loss 0.2426 Accuracy 0.9352\n",
            "Epoch 220 Batch 250 Loss 0.2487 Accuracy 0.9334\n",
            "Epoch 220 Batch 300 Loss 0.2552 Accuracy 0.9319\n",
            "Saving checkpoint for epoch 220 at ./checkpoints/train\\ckpt-164\n",
            "Epoch 220 Loss 0.2583 Accuracy 0.9310\n",
            "Time taken for 1 epoch: 188.76 secs\n",
            "\n",
            "Epoch 221 Batch 0 Loss 0.2309 Accuracy 0.9366\n",
            "Epoch 221 Batch 50 Loss 0.2309 Accuracy 0.9381\n",
            "Epoch 221 Batch 100 Loss 0.2326 Accuracy 0.9374\n",
            "Epoch 221 Batch 150 Loss 0.2363 Accuracy 0.9363\n",
            "Epoch 221 Batch 200 Loss 0.2431 Accuracy 0.9347\n",
            "Epoch 221 Batch 250 Loss 0.2490 Accuracy 0.9330\n",
            "Epoch 221 Batch 300 Loss 0.2564 Accuracy 0.9314\n",
            "Epoch 221 Loss 0.2597 Accuracy 0.9306\n",
            "Time taken for 1 epoch: 187.52 secs\n",
            "\n",
            "Epoch 222 Batch 0 Loss 0.2227 Accuracy 0.9363\n",
            "Epoch 222 Batch 50 Loss 0.2292 Accuracy 0.9385\n",
            "Epoch 222 Batch 100 Loss 0.2360 Accuracy 0.9367\n",
            "Epoch 222 Batch 150 Loss 0.2401 Accuracy 0.9353\n",
            "Epoch 222 Batch 200 Loss 0.2460 Accuracy 0.9336\n",
            "Epoch 222 Batch 250 Loss 0.2527 Accuracy 0.9318\n",
            "Epoch 222 Batch 300 Loss 0.2590 Accuracy 0.9303\n",
            "Epoch 222 Loss 0.2621 Accuracy 0.9295\n",
            "Time taken for 1 epoch: 187.32 secs\n",
            "\n",
            "Epoch 223 Batch 0 Loss 0.2277 Accuracy 0.9445\n",
            "Epoch 223 Batch 50 Loss 0.2269 Accuracy 0.9399\n",
            "Epoch 223 Batch 100 Loss 0.2307 Accuracy 0.9383\n",
            "Epoch 223 Batch 150 Loss 0.2354 Accuracy 0.9373\n",
            "Epoch 223 Batch 200 Loss 0.2425 Accuracy 0.9352\n",
            "Epoch 223 Batch 250 Loss 0.2489 Accuracy 0.9333\n",
            "Epoch 223 Batch 300 Loss 0.2564 Accuracy 0.9316\n",
            "Epoch 223 Loss 0.2587 Accuracy 0.9310\n",
            "Time taken for 1 epoch: 184.34 secs\n",
            "\n",
            "Epoch 224 Batch 0 Loss 0.1950 Accuracy 0.9492\n",
            "Epoch 224 Batch 50 Loss 0.2307 Accuracy 0.9374\n",
            "Epoch 224 Batch 100 Loss 0.2361 Accuracy 0.9358\n",
            "Epoch 224 Batch 150 Loss 0.2421 Accuracy 0.9344\n",
            "Epoch 224 Batch 200 Loss 0.2472 Accuracy 0.9333\n",
            "Epoch 224 Batch 250 Loss 0.2518 Accuracy 0.9321\n",
            "Epoch 224 Batch 300 Loss 0.2578 Accuracy 0.9306\n",
            "Epoch 224 Loss 0.2608 Accuracy 0.9300\n",
            "Time taken for 1 epoch: 184.40 secs\n",
            "\n",
            "Epoch 225 Batch 0 Loss 0.2218 Accuracy 0.9409\n",
            "Epoch 225 Batch 50 Loss 0.2298 Accuracy 0.9381\n",
            "Epoch 225 Batch 100 Loss 0.2308 Accuracy 0.9381\n",
            "Epoch 225 Batch 150 Loss 0.2360 Accuracy 0.9369\n",
            "Epoch 225 Batch 200 Loss 0.2420 Accuracy 0.9350\n",
            "Epoch 225 Batch 250 Loss 0.2495 Accuracy 0.9329\n",
            "Epoch 225 Batch 300 Loss 0.2562 Accuracy 0.9313\n",
            "Saving checkpoint for epoch 225 at ./checkpoints/train\\ckpt-165\n",
            "Epoch 225 Loss 0.2593 Accuracy 0.9306\n",
            "Time taken for 1 epoch: 188.50 secs\n",
            "\n",
            "Epoch 226 Batch 0 Loss 0.2640 Accuracy 0.9201\n",
            "Epoch 226 Batch 50 Loss 0.2334 Accuracy 0.9357\n",
            "Epoch 226 Batch 100 Loss 0.2353 Accuracy 0.9354\n",
            "Epoch 226 Batch 150 Loss 0.2402 Accuracy 0.9340\n",
            "Epoch 226 Batch 200 Loss 0.2468 Accuracy 0.9325\n",
            "Epoch 226 Batch 250 Loss 0.2517 Accuracy 0.9315\n",
            "Epoch 226 Batch 300 Loss 0.2578 Accuracy 0.9301\n",
            "Epoch 226 Loss 0.2603 Accuracy 0.9295\n",
            "Time taken for 1 epoch: 185.79 secs\n",
            "\n",
            "Epoch 227 Batch 0 Loss 0.2020 Accuracy 0.9471\n",
            "Epoch 227 Batch 50 Loss 0.2280 Accuracy 0.9383\n",
            "Epoch 227 Batch 100 Loss 0.2317 Accuracy 0.9374\n",
            "Epoch 227 Batch 150 Loss 0.2352 Accuracy 0.9365\n",
            "Epoch 227 Batch 200 Loss 0.2428 Accuracy 0.9344\n",
            "Epoch 227 Batch 250 Loss 0.2477 Accuracy 0.9331\n",
            "Epoch 227 Batch 300 Loss 0.2548 Accuracy 0.9314\n",
            "Epoch 227 Loss 0.2581 Accuracy 0.9306\n",
            "Time taken for 1 epoch: 183.12 secs\n",
            "\n",
            "Epoch 228 Batch 0 Loss 0.1935 Accuracy 0.9497\n",
            "Epoch 228 Batch 50 Loss 0.2189 Accuracy 0.9424\n",
            "Epoch 228 Batch 100 Loss 0.2299 Accuracy 0.9386\n",
            "Epoch 228 Batch 150 Loss 0.2347 Accuracy 0.9370\n",
            "Epoch 228 Batch 200 Loss 0.2413 Accuracy 0.9352\n",
            "Epoch 228 Batch 250 Loss 0.2484 Accuracy 0.9332\n",
            "Epoch 228 Batch 300 Loss 0.2543 Accuracy 0.9317\n",
            "Epoch 228 Loss 0.2569 Accuracy 0.9309\n",
            "Time taken for 1 epoch: 183.29 secs\n",
            "\n",
            "Epoch 229 Batch 0 Loss 0.1985 Accuracy 0.9524\n",
            "Epoch 229 Batch 50 Loss 0.2279 Accuracy 0.9389\n",
            "Epoch 229 Batch 100 Loss 0.2346 Accuracy 0.9372\n",
            "Epoch 229 Batch 150 Loss 0.2390 Accuracy 0.9357\n",
            "Epoch 229 Batch 200 Loss 0.2445 Accuracy 0.9341\n",
            "Epoch 229 Batch 250 Loss 0.2500 Accuracy 0.9327\n",
            "Epoch 229 Batch 300 Loss 0.2572 Accuracy 0.9311\n",
            "Epoch 229 Loss 0.2594 Accuracy 0.9306\n",
            "Time taken for 1 epoch: 182.95 secs\n",
            "\n",
            "Epoch 230 Batch 0 Loss 0.2390 Accuracy 0.9401\n",
            "Epoch 230 Batch 50 Loss 0.2281 Accuracy 0.9382\n",
            "Epoch 230 Batch 100 Loss 0.2322 Accuracy 0.9376\n",
            "Epoch 230 Batch 150 Loss 0.2376 Accuracy 0.9358\n",
            "Epoch 230 Batch 200 Loss 0.2435 Accuracy 0.9342\n",
            "Epoch 230 Batch 250 Loss 0.2505 Accuracy 0.9325\n",
            "Epoch 230 Batch 300 Loss 0.2566 Accuracy 0.9309\n",
            "Saving checkpoint for epoch 230 at ./checkpoints/train\\ckpt-166\n",
            "Epoch 230 Loss 0.2590 Accuracy 0.9303\n",
            "Time taken for 1 epoch: 184.20 secs\n",
            "\n",
            "Epoch 231 Batch 0 Loss 0.2545 Accuracy 0.9351\n",
            "Epoch 231 Batch 50 Loss 0.2275 Accuracy 0.9378\n",
            "Epoch 231 Batch 100 Loss 0.2319 Accuracy 0.9371\n",
            "Epoch 231 Batch 150 Loss 0.2354 Accuracy 0.9361\n",
            "Epoch 231 Batch 200 Loss 0.2426 Accuracy 0.9344\n",
            "Epoch 231 Batch 250 Loss 0.2495 Accuracy 0.9327\n",
            "Epoch 231 Batch 300 Loss 0.2566 Accuracy 0.9312\n",
            "Epoch 231 Loss 0.2596 Accuracy 0.9305\n",
            "Time taken for 1 epoch: 182.90 secs\n",
            "\n",
            "Epoch 232 Batch 0 Loss 0.2122 Accuracy 0.9483\n",
            "Epoch 232 Batch 50 Loss 0.2247 Accuracy 0.9399\n",
            "Epoch 232 Batch 100 Loss 0.2303 Accuracy 0.9384\n",
            "Epoch 232 Batch 150 Loss 0.2357 Accuracy 0.9368\n",
            "Epoch 232 Batch 200 Loss 0.2413 Accuracy 0.9353\n",
            "Epoch 232 Batch 250 Loss 0.2474 Accuracy 0.9338\n",
            "Epoch 232 Batch 300 Loss 0.2544 Accuracy 0.9321\n",
            "Epoch 232 Loss 0.2570 Accuracy 0.9315\n",
            "Time taken for 1 epoch: 183.04 secs\n",
            "\n",
            "Epoch 233 Batch 0 Loss 0.2446 Accuracy 0.9322\n",
            "Epoch 233 Batch 50 Loss 0.2246 Accuracy 0.9394\n",
            "Epoch 233 Batch 100 Loss 0.2287 Accuracy 0.9388\n",
            "Epoch 233 Batch 150 Loss 0.2346 Accuracy 0.9369\n",
            "Epoch 233 Batch 200 Loss 0.2396 Accuracy 0.9354\n",
            "Epoch 233 Batch 250 Loss 0.2452 Accuracy 0.9340\n",
            "Epoch 233 Batch 300 Loss 0.2524 Accuracy 0.9320\n",
            "Epoch 233 Loss 0.2551 Accuracy 0.9314\n",
            "Time taken for 1 epoch: 183.28 secs\n",
            "\n",
            "Epoch 234 Batch 0 Loss 0.2058 Accuracy 0.9407\n",
            "Epoch 234 Batch 50 Loss 0.2332 Accuracy 0.9377\n",
            "Epoch 234 Batch 100 Loss 0.2350 Accuracy 0.9368\n",
            "Epoch 234 Batch 150 Loss 0.2393 Accuracy 0.9352\n",
            "Epoch 234 Batch 200 Loss 0.2433 Accuracy 0.9339\n",
            "Epoch 234 Batch 250 Loss 0.2488 Accuracy 0.9325\n",
            "Epoch 234 Batch 300 Loss 0.2555 Accuracy 0.9309\n",
            "Epoch 234 Loss 0.2585 Accuracy 0.9302\n",
            "Time taken for 1 epoch: 182.66 secs\n",
            "\n",
            "Epoch 235 Batch 0 Loss 0.2157 Accuracy 0.9475\n",
            "Epoch 235 Batch 50 Loss 0.2204 Accuracy 0.9408\n",
            "Epoch 235 Batch 100 Loss 0.2286 Accuracy 0.9379\n",
            "Epoch 235 Batch 150 Loss 0.2346 Accuracy 0.9360\n",
            "Epoch 235 Batch 200 Loss 0.2409 Accuracy 0.9345\n",
            "Epoch 235 Batch 250 Loss 0.2459 Accuracy 0.9335\n",
            "Epoch 235 Batch 300 Loss 0.2534 Accuracy 0.9316\n",
            "Saving checkpoint for epoch 235 at ./checkpoints/train\\ckpt-167\n",
            "Epoch 235 Loss 0.2554 Accuracy 0.9312\n",
            "Time taken for 1 epoch: 184.40 secs\n",
            "\n",
            "Epoch 236 Batch 0 Loss 0.2152 Accuracy 0.9463\n",
            "Epoch 236 Batch 50 Loss 0.2274 Accuracy 0.9396\n",
            "Epoch 236 Batch 100 Loss 0.2323 Accuracy 0.9374\n",
            "Epoch 236 Batch 150 Loss 0.2362 Accuracy 0.9365\n",
            "Epoch 236 Batch 200 Loss 0.2412 Accuracy 0.9349\n",
            "Epoch 236 Batch 250 Loss 0.2474 Accuracy 0.9330\n",
            "Epoch 236 Batch 300 Loss 0.2544 Accuracy 0.9312\n",
            "Epoch 236 Loss 0.2569 Accuracy 0.9307\n",
            "Time taken for 1 epoch: 183.09 secs\n",
            "\n",
            "Epoch 237 Batch 0 Loss 0.2105 Accuracy 0.9497\n",
            "Epoch 237 Batch 50 Loss 0.2340 Accuracy 0.9378\n",
            "Epoch 237 Batch 100 Loss 0.2342 Accuracy 0.9375\n",
            "Epoch 237 Batch 150 Loss 0.2385 Accuracy 0.9355\n",
            "Epoch 237 Batch 200 Loss 0.2436 Accuracy 0.9343\n",
            "Epoch 237 Batch 250 Loss 0.2490 Accuracy 0.9330\n",
            "Epoch 237 Batch 300 Loss 0.2559 Accuracy 0.9314\n",
            "Epoch 237 Loss 0.2591 Accuracy 0.9306\n",
            "Time taken for 1 epoch: 184.18 secs\n",
            "\n",
            "Epoch 238 Batch 0 Loss 0.1976 Accuracy 0.9463\n",
            "Epoch 238 Batch 50 Loss 0.2206 Accuracy 0.9422\n",
            "Epoch 238 Batch 100 Loss 0.2266 Accuracy 0.9399\n",
            "Epoch 238 Batch 150 Loss 0.2321 Accuracy 0.9384\n",
            "Epoch 238 Batch 200 Loss 0.2387 Accuracy 0.9363\n",
            "Epoch 238 Batch 250 Loss 0.2462 Accuracy 0.9344\n",
            "Epoch 238 Batch 300 Loss 0.2541 Accuracy 0.9323\n",
            "Epoch 238 Loss 0.2570 Accuracy 0.9315\n",
            "Time taken for 1 epoch: 183.04 secs\n",
            "\n",
            "Epoch 239 Batch 0 Loss 0.2155 Accuracy 0.9525\n",
            "Epoch 239 Batch 50 Loss 0.2254 Accuracy 0.9394\n",
            "Epoch 239 Batch 100 Loss 0.2271 Accuracy 0.9392\n",
            "Epoch 239 Batch 150 Loss 0.2344 Accuracy 0.9370\n",
            "Epoch 239 Batch 200 Loss 0.2405 Accuracy 0.9355\n",
            "Epoch 239 Batch 250 Loss 0.2463 Accuracy 0.9338\n",
            "Epoch 239 Batch 300 Loss 0.2534 Accuracy 0.9320\n",
            "Epoch 239 Loss 0.2565 Accuracy 0.9311\n",
            "Time taken for 1 epoch: 182.93 secs\n",
            "\n",
            "Epoch 240 Batch 0 Loss 0.2442 Accuracy 0.9315\n",
            "Epoch 240 Batch 50 Loss 0.2255 Accuracy 0.9393\n",
            "Epoch 240 Batch 100 Loss 0.2324 Accuracy 0.9375\n",
            "Epoch 240 Batch 150 Loss 0.2381 Accuracy 0.9357\n",
            "Epoch 240 Batch 200 Loss 0.2422 Accuracy 0.9349\n",
            "Epoch 240 Batch 250 Loss 0.2482 Accuracy 0.9332\n",
            "Epoch 240 Batch 300 Loss 0.2537 Accuracy 0.9320\n",
            "Saving checkpoint for epoch 240 at ./checkpoints/train\\ckpt-168\n",
            "Epoch 240 Loss 0.2562 Accuracy 0.9313\n",
            "Time taken for 1 epoch: 183.91 secs\n",
            "\n",
            "Epoch 241 Batch 0 Loss 0.2210 Accuracy 0.9361\n",
            "Epoch 241 Batch 50 Loss 0.2236 Accuracy 0.9399\n",
            "Epoch 241 Batch 100 Loss 0.2296 Accuracy 0.9386\n",
            "Epoch 241 Batch 150 Loss 0.2357 Accuracy 0.9369\n",
            "Epoch 241 Batch 200 Loss 0.2404 Accuracy 0.9358\n",
            "Epoch 241 Batch 250 Loss 0.2459 Accuracy 0.9343\n",
            "Epoch 241 Batch 300 Loss 0.2545 Accuracy 0.9322\n",
            "Epoch 241 Loss 0.2577 Accuracy 0.9315\n",
            "Time taken for 1 epoch: 183.77 secs\n",
            "\n",
            "Epoch 242 Batch 0 Loss 0.1804 Accuracy 0.9565\n",
            "Epoch 242 Batch 50 Loss 0.2272 Accuracy 0.9381\n",
            "Epoch 242 Batch 100 Loss 0.2324 Accuracy 0.9369\n",
            "Epoch 242 Batch 150 Loss 0.2379 Accuracy 0.9351\n",
            "Epoch 242 Batch 200 Loss 0.2431 Accuracy 0.9339\n",
            "Epoch 242 Batch 250 Loss 0.2496 Accuracy 0.9325\n",
            "Epoch 242 Batch 300 Loss 0.2550 Accuracy 0.9313\n",
            "Epoch 242 Loss 0.2578 Accuracy 0.9305\n",
            "Time taken for 1 epoch: 182.51 secs\n",
            "\n",
            "Epoch 243 Batch 0 Loss 0.2508 Accuracy 0.9296\n",
            "Epoch 243 Batch 50 Loss 0.2283 Accuracy 0.9386\n",
            "Epoch 243 Batch 100 Loss 0.2321 Accuracy 0.9380\n",
            "Epoch 243 Batch 150 Loss 0.2387 Accuracy 0.9357\n",
            "Epoch 243 Batch 200 Loss 0.2435 Accuracy 0.9345\n",
            "Epoch 243 Batch 250 Loss 0.2484 Accuracy 0.9332\n",
            "Epoch 243 Batch 300 Loss 0.2550 Accuracy 0.9315\n",
            "Epoch 243 Loss 0.2576 Accuracy 0.9310\n",
            "Time taken for 1 epoch: 182.66 secs\n",
            "\n",
            "Epoch 244 Batch 0 Loss 0.2226 Accuracy 0.9388\n",
            "Epoch 244 Batch 50 Loss 0.2266 Accuracy 0.9382\n",
            "Epoch 244 Batch 100 Loss 0.2325 Accuracy 0.9373\n",
            "Epoch 244 Batch 150 Loss 0.2373 Accuracy 0.9364\n",
            "Epoch 244 Batch 200 Loss 0.2419 Accuracy 0.9354\n",
            "Epoch 244 Batch 250 Loss 0.2482 Accuracy 0.9335\n",
            "Epoch 244 Batch 300 Loss 0.2549 Accuracy 0.9320\n",
            "Epoch 244 Loss 0.2579 Accuracy 0.9313\n",
            "Time taken for 1 epoch: 182.26 secs\n",
            "\n",
            "Epoch 245 Batch 0 Loss 0.2587 Accuracy 0.9461\n",
            "Epoch 245 Batch 50 Loss 0.2264 Accuracy 0.9394\n",
            "Epoch 245 Batch 100 Loss 0.2330 Accuracy 0.9371\n",
            "Epoch 245 Batch 150 Loss 0.2385 Accuracy 0.9354\n",
            "Epoch 245 Batch 200 Loss 0.2430 Accuracy 0.9346\n",
            "Epoch 245 Batch 250 Loss 0.2482 Accuracy 0.9333\n",
            "Epoch 245 Batch 300 Loss 0.2542 Accuracy 0.9319\n",
            "Saving checkpoint for epoch 245 at ./checkpoints/train\\ckpt-169\n",
            "Epoch 245 Loss 0.2567 Accuracy 0.9314\n",
            "Time taken for 1 epoch: 184.26 secs\n",
            "\n",
            "Epoch 246 Batch 0 Loss 0.1649 Accuracy 0.9614\n",
            "Epoch 246 Batch 50 Loss 0.2239 Accuracy 0.9395\n",
            "Epoch 246 Batch 100 Loss 0.2274 Accuracy 0.9384\n",
            "Epoch 246 Batch 150 Loss 0.2341 Accuracy 0.9364\n",
            "Epoch 246 Batch 200 Loss 0.2404 Accuracy 0.9350\n",
            "Epoch 246 Batch 250 Loss 0.2465 Accuracy 0.9335\n",
            "Epoch 246 Batch 300 Loss 0.2528 Accuracy 0.9321\n",
            "Epoch 246 Loss 0.2555 Accuracy 0.9314\n",
            "Time taken for 1 epoch: 183.34 secs\n",
            "\n",
            "Epoch 247 Batch 0 Loss 0.2423 Accuracy 0.9270\n",
            "Epoch 247 Batch 50 Loss 0.2253 Accuracy 0.9383\n",
            "Epoch 247 Batch 100 Loss 0.2293 Accuracy 0.9376\n",
            "Epoch 247 Batch 150 Loss 0.2338 Accuracy 0.9366\n",
            "Epoch 247 Batch 200 Loss 0.2396 Accuracy 0.9357\n",
            "Epoch 247 Batch 250 Loss 0.2456 Accuracy 0.9341\n",
            "Epoch 247 Batch 300 Loss 0.2520 Accuracy 0.9324\n",
            "Epoch 247 Loss 0.2551 Accuracy 0.9317\n",
            "Time taken for 1 epoch: 182.78 secs\n",
            "\n",
            "Epoch 248 Batch 0 Loss 0.2182 Accuracy 0.9438\n",
            "Epoch 248 Batch 50 Loss 0.2261 Accuracy 0.9390\n",
            "Epoch 248 Batch 100 Loss 0.2287 Accuracy 0.9384\n",
            "Epoch 248 Batch 150 Loss 0.2341 Accuracy 0.9368\n",
            "Epoch 248 Batch 200 Loss 0.2396 Accuracy 0.9353\n",
            "Epoch 248 Batch 250 Loss 0.2460 Accuracy 0.9338\n",
            "Epoch 248 Batch 300 Loss 0.2525 Accuracy 0.9322\n",
            "Epoch 248 Loss 0.2552 Accuracy 0.9314\n",
            "Time taken for 1 epoch: 182.71 secs\n",
            "\n",
            "Epoch 249 Batch 0 Loss 0.2386 Accuracy 0.9310\n",
            "Epoch 249 Batch 50 Loss 0.2263 Accuracy 0.9388\n",
            "Epoch 249 Batch 100 Loss 0.2314 Accuracy 0.9372\n",
            "Epoch 249 Batch 150 Loss 0.2360 Accuracy 0.9361\n",
            "Epoch 249 Batch 200 Loss 0.2389 Accuracy 0.9354\n",
            "Epoch 249 Batch 250 Loss 0.2456 Accuracy 0.9338\n",
            "Epoch 249 Batch 300 Loss 0.2523 Accuracy 0.9321\n",
            "Epoch 249 Loss 0.2559 Accuracy 0.9313\n",
            "Time taken for 1 epoch: 183.29 secs\n",
            "\n",
            "Epoch 250 Batch 0 Loss 0.2579 Accuracy 0.9327\n",
            "Epoch 250 Batch 50 Loss 0.2258 Accuracy 0.9388\n",
            "Epoch 250 Batch 100 Loss 0.2307 Accuracy 0.9372\n",
            "Epoch 250 Batch 150 Loss 0.2372 Accuracy 0.9355\n",
            "Epoch 250 Batch 200 Loss 0.2418 Accuracy 0.9345\n",
            "Epoch 250 Batch 250 Loss 0.2475 Accuracy 0.9332\n",
            "Epoch 250 Batch 300 Loss 0.2537 Accuracy 0.9318\n",
            "Saving checkpoint for epoch 250 at ./checkpoints/train\\ckpt-170\n",
            "Epoch 250 Loss 0.2558 Accuracy 0.9313\n",
            "Time taken for 1 epoch: 184.92 secs\n",
            "\n",
            "Epoch 251 Batch 0 Loss 0.2110 Accuracy 0.9415\n",
            "Epoch 251 Batch 50 Loss 0.2265 Accuracy 0.9389\n",
            "Epoch 251 Batch 100 Loss 0.2297 Accuracy 0.9380\n",
            "Epoch 251 Batch 150 Loss 0.2343 Accuracy 0.9368\n",
            "Epoch 251 Batch 200 Loss 0.2393 Accuracy 0.9355\n",
            "Epoch 251 Batch 250 Loss 0.2452 Accuracy 0.9339\n",
            "Epoch 251 Batch 300 Loss 0.2520 Accuracy 0.9322\n",
            "Epoch 251 Loss 0.2555 Accuracy 0.9315\n",
            "Time taken for 1 epoch: 182.61 secs\n",
            "\n",
            "Epoch 252 Batch 0 Loss 0.2460 Accuracy 0.9277\n",
            "Epoch 252 Batch 50 Loss 0.2258 Accuracy 0.9392\n",
            "Epoch 252 Batch 100 Loss 0.2300 Accuracy 0.9382\n",
            "Epoch 252 Batch 150 Loss 0.2333 Accuracy 0.9373\n",
            "Epoch 252 Batch 200 Loss 0.2405 Accuracy 0.9353\n",
            "Epoch 252 Batch 250 Loss 0.2459 Accuracy 0.9339\n",
            "Epoch 252 Batch 300 Loss 0.2528 Accuracy 0.9322\n",
            "Epoch 252 Loss 0.2557 Accuracy 0.9315\n",
            "Time taken for 1 epoch: 182.61 secs\n",
            "\n",
            "Epoch 253 Batch 0 Loss 0.2042 Accuracy 0.9490\n",
            "Epoch 253 Batch 50 Loss 0.2195 Accuracy 0.9398\n",
            "Epoch 253 Batch 100 Loss 0.2279 Accuracy 0.9377\n",
            "Epoch 253 Batch 150 Loss 0.2326 Accuracy 0.9368\n",
            "Epoch 253 Batch 200 Loss 0.2394 Accuracy 0.9348\n",
            "Epoch 253 Batch 250 Loss 0.2445 Accuracy 0.9339\n",
            "Epoch 253 Batch 300 Loss 0.2510 Accuracy 0.9325\n",
            "Epoch 253 Loss 0.2552 Accuracy 0.9314\n",
            "Time taken for 1 epoch: 182.43 secs\n",
            "\n",
            "Epoch 254 Batch 0 Loss 0.1643 Accuracy 0.9524\n",
            "Epoch 254 Batch 50 Loss 0.2188 Accuracy 0.9418\n",
            "Epoch 254 Batch 100 Loss 0.2266 Accuracy 0.9399\n",
            "Epoch 254 Batch 150 Loss 0.2304 Accuracy 0.9383\n",
            "Epoch 254 Batch 200 Loss 0.2356 Accuracy 0.9364\n",
            "Epoch 254 Batch 250 Loss 0.2423 Accuracy 0.9346\n",
            "Epoch 254 Batch 300 Loss 0.2497 Accuracy 0.9329\n",
            "Epoch 254 Loss 0.2530 Accuracy 0.9321\n",
            "Time taken for 1 epoch: 182.50 secs\n",
            "\n",
            "Epoch 255 Batch 0 Loss 0.1978 Accuracy 0.9480\n",
            "Epoch 255 Batch 50 Loss 0.2227 Accuracy 0.9402\n",
            "Epoch 255 Batch 100 Loss 0.2255 Accuracy 0.9394\n",
            "Epoch 255 Batch 150 Loss 0.2308 Accuracy 0.9376\n",
            "Epoch 255 Batch 200 Loss 0.2358 Accuracy 0.9360\n",
            "Epoch 255 Batch 250 Loss 0.2421 Accuracy 0.9347\n",
            "Epoch 255 Batch 300 Loss 0.2495 Accuracy 0.9330\n",
            "Saving checkpoint for epoch 255 at ./checkpoints/train\\ckpt-171\n",
            "Epoch 255 Loss 0.2525 Accuracy 0.9323\n",
            "Time taken for 1 epoch: 184.19 secs\n",
            "\n",
            "Epoch 256 Batch 0 Loss 0.2122 Accuracy 0.9461\n",
            "Epoch 256 Batch 50 Loss 0.2277 Accuracy 0.9397\n",
            "Epoch 256 Batch 100 Loss 0.2279 Accuracy 0.9393\n",
            "Epoch 256 Batch 150 Loss 0.2341 Accuracy 0.9374\n",
            "Epoch 256 Batch 200 Loss 0.2385 Accuracy 0.9360\n",
            "Epoch 256 Batch 250 Loss 0.2454 Accuracy 0.9344\n",
            "Epoch 256 Batch 300 Loss 0.2508 Accuracy 0.9330\n",
            "Epoch 256 Loss 0.2541 Accuracy 0.9321\n",
            "Time taken for 1 epoch: 182.75 secs\n",
            "\n",
            "Epoch 257 Batch 0 Loss 0.2148 Accuracy 0.9464\n",
            "Epoch 257 Batch 50 Loss 0.2222 Accuracy 0.9404\n",
            "Epoch 257 Batch 100 Loss 0.2264 Accuracy 0.9396\n",
            "Epoch 257 Batch 150 Loss 0.2305 Accuracy 0.9383\n",
            "Epoch 257 Batch 200 Loss 0.2361 Accuracy 0.9367\n",
            "Epoch 257 Batch 250 Loss 0.2424 Accuracy 0.9351\n",
            "Epoch 257 Batch 300 Loss 0.2486 Accuracy 0.9335\n",
            "Epoch 257 Loss 0.2517 Accuracy 0.9328\n",
            "Time taken for 1 epoch: 183.37 secs\n",
            "\n",
            "Epoch 258 Batch 0 Loss 0.2223 Accuracy 0.9426\n",
            "Epoch 258 Batch 50 Loss 0.2175 Accuracy 0.9419\n",
            "Epoch 258 Batch 100 Loss 0.2241 Accuracy 0.9398\n",
            "Epoch 258 Batch 150 Loss 0.2306 Accuracy 0.9382\n",
            "Epoch 258 Batch 200 Loss 0.2359 Accuracy 0.9369\n",
            "Epoch 258 Batch 250 Loss 0.2415 Accuracy 0.9354\n",
            "Epoch 258 Batch 300 Loss 0.2483 Accuracy 0.9337\n",
            "Epoch 258 Loss 0.2502 Accuracy 0.9332\n",
            "Time taken for 1 epoch: 183.06 secs\n",
            "\n",
            "Epoch 259 Batch 0 Loss 0.2401 Accuracy 0.9323\n",
            "Epoch 259 Batch 50 Loss 0.2217 Accuracy 0.9406\n",
            "Epoch 259 Batch 100 Loss 0.2271 Accuracy 0.9388\n",
            "Epoch 259 Batch 150 Loss 0.2326 Accuracy 0.9373\n",
            "Epoch 259 Batch 200 Loss 0.2370 Accuracy 0.9364\n",
            "Epoch 259 Batch 250 Loss 0.2436 Accuracy 0.9346\n",
            "Epoch 259 Batch 300 Loss 0.2497 Accuracy 0.9330\n",
            "Epoch 259 Loss 0.2524 Accuracy 0.9324\n",
            "Time taken for 1 epoch: 191.25 secs\n",
            "\n",
            "Epoch 260 Batch 0 Loss 0.1841 Accuracy 0.9493\n",
            "Epoch 260 Batch 50 Loss 0.2209 Accuracy 0.9416\n",
            "Epoch 260 Batch 100 Loss 0.2265 Accuracy 0.9395\n",
            "Epoch 260 Batch 150 Loss 0.2307 Accuracy 0.9380\n",
            "Epoch 260 Batch 200 Loss 0.2370 Accuracy 0.9362\n",
            "Epoch 260 Batch 250 Loss 0.2438 Accuracy 0.9344\n",
            "Epoch 260 Batch 300 Loss 0.2505 Accuracy 0.9327\n",
            "Saving checkpoint for epoch 260 at ./checkpoints/train\\ckpt-172\n",
            "Epoch 260 Loss 0.2533 Accuracy 0.9320\n",
            "Time taken for 1 epoch: 195.17 secs\n",
            "\n",
            "Epoch 261 Batch 0 Loss 0.1861 Accuracy 0.9485\n",
            "Epoch 261 Batch 50 Loss 0.2218 Accuracy 0.9404\n",
            "Epoch 261 Batch 100 Loss 0.2254 Accuracy 0.9393\n",
            "Epoch 261 Batch 150 Loss 0.2306 Accuracy 0.9378\n",
            "Epoch 261 Batch 200 Loss 0.2343 Accuracy 0.9368\n",
            "Epoch 261 Batch 250 Loss 0.2407 Accuracy 0.9351\n",
            "Epoch 261 Batch 300 Loss 0.2472 Accuracy 0.9337\n",
            "Epoch 261 Loss 0.2505 Accuracy 0.9328\n",
            "Time taken for 1 epoch: 195.55 secs\n",
            "\n",
            "Epoch 262 Batch 0 Loss 0.2023 Accuracy 0.9494\n",
            "Epoch 262 Batch 50 Loss 0.2190 Accuracy 0.9414\n",
            "Epoch 262 Batch 100 Loss 0.2253 Accuracy 0.9393\n",
            "Epoch 262 Batch 150 Loss 0.2280 Accuracy 0.9389\n",
            "Epoch 262 Batch 200 Loss 0.2339 Accuracy 0.9373\n",
            "Epoch 262 Batch 250 Loss 0.2408 Accuracy 0.9354\n",
            "Epoch 262 Batch 300 Loss 0.2492 Accuracy 0.9331\n",
            "Epoch 262 Loss 0.2521 Accuracy 0.9325\n",
            "Time taken for 1 epoch: 195.34 secs\n",
            "\n",
            "Epoch 263 Batch 0 Loss 0.1940 Accuracy 0.9455\n",
            "Epoch 263 Batch 50 Loss 0.2246 Accuracy 0.9396\n",
            "Epoch 263 Batch 100 Loss 0.2260 Accuracy 0.9389\n",
            "Epoch 263 Batch 150 Loss 0.2323 Accuracy 0.9372\n",
            "Epoch 263 Batch 200 Loss 0.2372 Accuracy 0.9363\n",
            "Epoch 263 Batch 250 Loss 0.2427 Accuracy 0.9349\n",
            "Epoch 263 Batch 300 Loss 0.2505 Accuracy 0.9330\n",
            "Epoch 263 Loss 0.2528 Accuracy 0.9326\n",
            "Time taken for 1 epoch: 195.27 secs\n",
            "\n",
            "Epoch 264 Batch 0 Loss 0.2456 Accuracy 0.9295\n",
            "Epoch 264 Batch 50 Loss 0.2201 Accuracy 0.9407\n",
            "Epoch 264 Batch 100 Loss 0.2248 Accuracy 0.9397\n",
            "Epoch 264 Batch 150 Loss 0.2281 Accuracy 0.9386\n",
            "Epoch 264 Batch 200 Loss 0.2344 Accuracy 0.9368\n",
            "Epoch 264 Batch 250 Loss 0.2423 Accuracy 0.9347\n",
            "Epoch 264 Batch 300 Loss 0.2494 Accuracy 0.9331\n",
            "Epoch 264 Loss 0.2523 Accuracy 0.9325\n",
            "Time taken for 1 epoch: 187.11 secs\n",
            "\n",
            "Epoch 265 Batch 0 Loss 0.2229 Accuracy 0.9499\n",
            "Epoch 265 Batch 50 Loss 0.2249 Accuracy 0.9396\n",
            "Epoch 265 Batch 100 Loss 0.2282 Accuracy 0.9385\n",
            "Epoch 265 Batch 150 Loss 0.2330 Accuracy 0.9372\n",
            "Epoch 265 Batch 200 Loss 0.2388 Accuracy 0.9356\n",
            "Epoch 265 Batch 250 Loss 0.2451 Accuracy 0.9342\n",
            "Epoch 265 Batch 300 Loss 0.2512 Accuracy 0.9328\n",
            "Saving checkpoint for epoch 265 at ./checkpoints/train\\ckpt-173\n",
            "Epoch 265 Loss 0.2536 Accuracy 0.9321\n",
            "Time taken for 1 epoch: 195.55 secs\n",
            "\n",
            "Epoch 266 Batch 0 Loss 0.2163 Accuracy 0.9401\n",
            "Epoch 266 Batch 50 Loss 0.2195 Accuracy 0.9405\n",
            "Epoch 266 Batch 100 Loss 0.2262 Accuracy 0.9389\n",
            "Epoch 266 Batch 150 Loss 0.2295 Accuracy 0.9375\n",
            "Epoch 266 Batch 200 Loss 0.2353 Accuracy 0.9361\n",
            "Epoch 266 Batch 250 Loss 0.2421 Accuracy 0.9345\n",
            "Epoch 266 Batch 300 Loss 0.2483 Accuracy 0.9332\n",
            "Epoch 266 Loss 0.2506 Accuracy 0.9328\n",
            "Time taken for 1 epoch: 195.68 secs\n",
            "\n",
            "Epoch 267 Batch 0 Loss 0.2187 Accuracy 0.9463\n",
            "Epoch 267 Batch 50 Loss 0.2192 Accuracy 0.9426\n",
            "Epoch 267 Batch 100 Loss 0.2252 Accuracy 0.9406\n",
            "Epoch 267 Batch 150 Loss 0.2292 Accuracy 0.9394\n",
            "Epoch 267 Batch 200 Loss 0.2358 Accuracy 0.9373\n",
            "Epoch 267 Batch 250 Loss 0.2415 Accuracy 0.9356\n",
            "Epoch 267 Batch 300 Loss 0.2487 Accuracy 0.9338\n",
            "Epoch 267 Loss 0.2513 Accuracy 0.9332\n",
            "Time taken for 1 epoch: 197.67 secs\n",
            "\n",
            "Epoch 268 Batch 0 Loss 0.2156 Accuracy 0.9464\n",
            "Epoch 268 Batch 50 Loss 0.2198 Accuracy 0.9407\n",
            "Epoch 268 Batch 100 Loss 0.2273 Accuracy 0.9391\n",
            "Epoch 268 Batch 150 Loss 0.2316 Accuracy 0.9377\n",
            "Epoch 268 Batch 200 Loss 0.2365 Accuracy 0.9366\n",
            "Epoch 268 Batch 250 Loss 0.2426 Accuracy 0.9351\n",
            "Epoch 268 Batch 300 Loss 0.2494 Accuracy 0.9333\n",
            "Epoch 268 Loss 0.2526 Accuracy 0.9326\n",
            "Time taken for 1 epoch: 190.21 secs\n",
            "\n",
            "Epoch 269 Batch 0 Loss 0.1804 Accuracy 0.9513\n",
            "Epoch 269 Batch 50 Loss 0.2167 Accuracy 0.9416\n",
            "Epoch 269 Batch 100 Loss 0.2233 Accuracy 0.9398\n",
            "Epoch 269 Batch 150 Loss 0.2299 Accuracy 0.9380\n",
            "Epoch 269 Batch 200 Loss 0.2370 Accuracy 0.9360\n",
            "Epoch 269 Batch 250 Loss 0.2422 Accuracy 0.9345\n",
            "Epoch 269 Batch 300 Loss 0.2480 Accuracy 0.9332\n",
            "Epoch 269 Loss 0.2505 Accuracy 0.9326\n",
            "Time taken for 1 epoch: 196.80 secs\n",
            "\n",
            "Epoch 270 Batch 0 Loss 0.2120 Accuracy 0.9370\n",
            "Epoch 270 Batch 50 Loss 0.2204 Accuracy 0.9400\n",
            "Epoch 270 Batch 100 Loss 0.2248 Accuracy 0.9394\n",
            "Epoch 270 Batch 150 Loss 0.2296 Accuracy 0.9379\n",
            "Epoch 270 Batch 200 Loss 0.2348 Accuracy 0.9365\n",
            "Epoch 270 Batch 250 Loss 0.2416 Accuracy 0.9348\n",
            "Epoch 270 Batch 300 Loss 0.2492 Accuracy 0.9333\n",
            "Saving checkpoint for epoch 270 at ./checkpoints/train\\ckpt-174\n",
            "Epoch 270 Loss 0.2517 Accuracy 0.9327\n",
            "Time taken for 1 epoch: 194.08 secs\n",
            "\n",
            "Epoch 271 Batch 0 Loss 0.2211 Accuracy 0.9391\n",
            "Epoch 271 Batch 50 Loss 0.2244 Accuracy 0.9389\n",
            "Epoch 271 Batch 100 Loss 0.2276 Accuracy 0.9380\n",
            "Epoch 271 Batch 150 Loss 0.2320 Accuracy 0.9368\n",
            "Epoch 271 Batch 200 Loss 0.2361 Accuracy 0.9360\n",
            "Epoch 271 Batch 250 Loss 0.2428 Accuracy 0.9346\n",
            "Epoch 271 Batch 300 Loss 0.2486 Accuracy 0.9331\n",
            "Epoch 271 Loss 0.2516 Accuracy 0.9324\n",
            "Time taken for 1 epoch: 182.68 secs\n",
            "\n",
            "Epoch 272 Batch 0 Loss 0.1931 Accuracy 0.9382\n",
            "Epoch 272 Batch 50 Loss 0.2179 Accuracy 0.9405\n",
            "Epoch 272 Batch 100 Loss 0.2230 Accuracy 0.9399\n",
            "Epoch 272 Batch 150 Loss 0.2263 Accuracy 0.9388\n",
            "Epoch 272 Batch 200 Loss 0.2317 Accuracy 0.9372\n",
            "Epoch 272 Batch 250 Loss 0.2387 Accuracy 0.9355\n",
            "Epoch 272 Batch 300 Loss 0.2451 Accuracy 0.9339\n",
            "Epoch 272 Loss 0.2474 Accuracy 0.9335\n",
            "Time taken for 1 epoch: 182.61 secs\n",
            "\n",
            "Epoch 273 Batch 0 Loss 0.2162 Accuracy 0.9442\n",
            "Epoch 273 Batch 50 Loss 0.2215 Accuracy 0.9406\n",
            "Epoch 273 Batch 100 Loss 0.2235 Accuracy 0.9406\n",
            "Epoch 273 Batch 150 Loss 0.2289 Accuracy 0.9390\n",
            "Epoch 273 Batch 200 Loss 0.2345 Accuracy 0.9375\n",
            "Epoch 273 Batch 250 Loss 0.2400 Accuracy 0.9359\n",
            "Epoch 273 Batch 300 Loss 0.2460 Accuracy 0.9346\n",
            "Epoch 273 Loss 0.2487 Accuracy 0.9340\n",
            "Time taken for 1 epoch: 191.22 secs\n",
            "\n",
            "Epoch 274 Batch 0 Loss 0.2048 Accuracy 0.9443\n",
            "Epoch 274 Batch 50 Loss 0.2211 Accuracy 0.9405\n",
            "Epoch 274 Batch 100 Loss 0.2227 Accuracy 0.9396\n",
            "Epoch 274 Batch 150 Loss 0.2275 Accuracy 0.9386\n",
            "Epoch 274 Batch 200 Loss 0.2332 Accuracy 0.9372\n",
            "Epoch 274 Batch 250 Loss 0.2408 Accuracy 0.9351\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD7LrJckMH9c"
      },
      "source": [
        "Translation"
      ],
      "id": "BD7LrJckMH9c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkR0h2yxMH9c"
      },
      "source": [
        "#The transformer functions by building an output \n",
        "#word-by-word, starting from the <start> token\n",
        "#that signifies the beginning of a sentence.\n",
        "#There is a significant number of unused content\n",
        "#in this method. We were not able to implement\n",
        "#these before the D3 submission deadline.\n",
        "def evaluate(sentence):\n",
        "    sentence = tokenizer.texts_to_sequences([sentence])\n",
        "    difference = max_length - len(sentence[0])\n",
        "    while difference > 0:\n",
        "        sentence[0].append(0)\n",
        "        difference = difference - 1\n",
        "    encoder_input = tf.convert_to_tensor(sentence)\n",
        "    encoder_input = tf.cast(encoder_input, tf.int32)\n",
        "    start = 2\n",
        "    end = 3\n",
        "    output = [2]\n",
        "    output = tf.convert_to_tensor([output])\n",
        "    output = tf.cast(output, tf.int32)\n",
        "    for i in range(max_length):\n",
        "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
        "        predictions, attention_weights = transformer(encoder_input, \n",
        "                                                     output,\n",
        "                                                     False,\n",
        "                                                     enc_padding_mask,\n",
        "                                                     combined_mask,\n",
        "                                                     dec_padding_mask)\n",
        "        predictions = predictions[: ,-1:, :]\n",
        "        predicted_id = tf.argmax(predictions, axis=-1)\n",
        "        predicted_id = tf.cast(predicted_id, tf.int32)\n",
        "        output = tf.cast(output, tf.int32)\n",
        "        output = tf.concat([output, predicted_id], axis=-1)\n",
        "        if predicted_id == end:\n",
        "            break\n",
        "    arrayn = output.numpy()\n",
        "    text = tokenizer.sequences_to_texts(arrayn)\n",
        "\n",
        "    return text, attention_weights"
      ],
      "id": "MkR0h2yxMH9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elE95pEAMH9d"
      },
      "source": [
        "def plot_attention_head(in_tokens, translated_tokens, attention):\n",
        "  # The plot is of the attention when a token was generated.\n",
        "  # The model didn't generate `<START>` in the output. Skip it. \n",
        "    translated_tokens = translated_tokens[1:]\n",
        "\n",
        "    ax = plt.gca()\n",
        "    ax.matshow(attention)\n",
        "    ax.set_xticks(range(len(in_tokens)))\n",
        "    ax.set_yticks(range(len(translated_tokens)))\n",
        "\n",
        "\n",
        "    labels = [label.decode('utf-8') for label in in_tokens.numpy()]\n",
        "    ax.set_xticklabels(labels, rotation=90)\n",
        "\n",
        "    labels = [label.decode('utf-8') for label in translated_tokens.numpy()]\n",
        "    ax.set_yticklabels(labels)\n"
      ],
      "id": "elE95pEAMH9d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kThgGy8FMH9e"
      },
      "source": [
        "sentence = \"<start> he leaves me to my own contentment ,  but thats the one thing i cannot find . <end>\"\n",
        "\n",
        "translated_text, attention_weights = evaluate(sentence)\n",
        "#print(attention_weights.shape)"
      ],
      "id": "kThgGy8FMH9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYP-GADIMH9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93e7d953-6edf-4ff7-a3b3-45bc1bfe59cf"
      },
      "source": [
        "print(translated_text)"
      ],
      "id": "wYP-GADIMH9e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start> give him a humble hand , call him back . <end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD_MeEGhMH9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d5276ac-9576-4eea-eea1-46835c39b597"
      },
      "source": [
        "#sentence = \"<start> i swear to god , i am exceedingly tired . <end>\"\n",
        "\n",
        "#translated_text, attention_weights = evaluate(sentence)\n",
        "#print(translated_text)\n",
        "head = 0\n",
        "# shape: (batch=1, num_heads, seq_len_q, seq_len_k)\n",
        "attention_heads = tf.squeeze(\n",
        "  attention_weights['decoder_layer3_block2'], 0)\n",
        "attention = attention_heads[head]\n",
        "attention.shape\n",
        "#print(attention)"
      ],
      "id": "dD_MeEGhMH9e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([11, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfX6fBxbMH9e"
      },
      "source": [
        "experiment = ['one','two','three','four','five','six','seven','eight','nine','ten','eleven','twelve','thirteen','fourteen','fifteen']\n",
        "experiment = tf.convert_to_tensor(experiment)"
      ],
      "id": "kfX6fBxbMH9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "86bmkFJqMH9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "21df1419-a683-4ed5-c326-6662512c823e"
      },
      "source": [
        "plot_attention_head(experiment, experiment, attention)"
      ],
      "id": "86bmkFJqMH9f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moGnrrz2MH9f"
      },
      "source": [
        "def plot_attention_weights(sentence, translated_tokens, attention_heads):\n",
        "\n",
        "    fig = plt.figure(figsize=(16, 8))\n",
        "\n",
        "    for h, head in enumerate(attention_heads):\n",
        "        ax = fig.add_subplot(2, 4, h+1)\n",
        "\n",
        "        plot_attention_head(translated_tokens, translated_tokens, head)\n",
        "\n",
        "        ax.set_xlabel('Head {}'.format(h+1))\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "moGnrrz2MH9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfNp4qm8MH9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 798
        },
        "outputId": "b6ddfb1c-9aeb-4ec9-93ce-cfde13479e2d"
      },
      "source": [
        "plot_attention_weights(sentence, experiment,attention_weights['decoder_layer6_block1'][0])"
      ],
      "id": "JfNp4qm8MH9f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-151c158cd5b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_attention_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattention_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decoder_layer6_block1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-43-2ae5af739604>\u001b[0m in \u001b[0;36mplot_attention_weights\u001b[0;34m(sentence, translated_tokens, attention_heads)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mplot_attention_head\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslated_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1417\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1419\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1421\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_axes_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     raise ValueError(\n\u001b[0;32m---> 66\u001b[0;31m                         f\"num must be 1 <= num <= {rows*cols}, not {num}\")\n\u001b[0m\u001b[1;32m     67\u001b[0m                 self._subplotspec = GridSpec(\n\u001b[1;32m     68\u001b[0m                         rows, cols, figure=self.figure)[int(num) - 1]\n",
            "\u001b[0;31mValueError\u001b[0m: num must be 1 <= num <= 8, not 9"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1152x576 with 8 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYnkaCmFMH9f"
      },
      "source": [
        "Sample outputs"
      ],
      "id": "PYnkaCmFMH9f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKnGmHlRMH9f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "522e6f15-d713-4ec2-e470-c01bf9c89457"
      },
      "source": [
        "sentence = \"<start> i jbdfghfgdhdfghdfgh <end>\"\n",
        "print(evaluate(sentence)[0])"
      ],
      "id": "LKnGmHlRMH9f",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['<start> i am resolved on a court . <end>'], {'decoder_layer1_block1': <tf.Tensor: shape=(1, 16, 8, 8), dtype=float32, numpy=\n",
            "array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.91705871e-01, 1.08294122e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.27031922e-01, 1.77565858e-01, 1.95402205e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [3.10916513e-01, 9.46637839e-02, 1.64046615e-01, ...,\n",
            "          1.12720788e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [3.05294156e-01, 2.00240955e-01, 8.62307250e-02, ...,\n",
            "          1.19285569e-01, 3.52751054e-02, 0.00000000e+00],\n",
            "         [9.75426137e-01, 6.78591838e-04, 1.08500745e-03, ...,\n",
            "          1.25490071e-03, 1.03272228e-04, 6.38555181e-13]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.93299627e-01, 4.06700343e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.71809119e-01, 4.96983171e-01, 3.12076844e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [3.06973904e-01, 9.14216116e-02, 4.81953584e-02, ...,\n",
            "          2.36329585e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.47501647e-01, 5.07615283e-02, 1.84648558e-02, ...,\n",
            "          2.94326603e-01, 1.45228237e-01, 0.00000000e+00],\n",
            "         [9.34065223e-01, 4.05329250e-04, 1.08902040e-03, ...,\n",
            "          6.37095869e-02, 4.44442732e-04, 5.67209391e-09]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.10533428e-01, 8.94666165e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.79315782e-01, 1.08831674e-01, 3.11852574e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [2.54818559e-01, 7.82256126e-02, 1.68972239e-01, ...,\n",
            "          1.53508112e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.59698918e-01, 8.50665793e-02, 1.27577677e-01, ...,\n",
            "          1.16544873e-01, 7.35374391e-02, 0.00000000e+00],\n",
            "         [9.90312457e-01, 4.85376164e-04, 9.94232832e-04, ...,\n",
            "          1.13143655e-03, 4.07277420e-03, 5.11377630e-06]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.05432057e-01, 2.94567913e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.07499087e-01, 1.97141483e-01, 9.53594893e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [3.32035214e-01, 1.59730315e-01, 9.80397090e-02, ...,\n",
            "          1.46721750e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.55157799e-01, 9.49053690e-02, 3.40737216e-02, ...,\n",
            "          1.62458748e-01, 1.61935776e-01, 0.00000000e+00],\n",
            "         [9.98533726e-01, 4.18348820e-04, 6.17341837e-04, ...,\n",
            "          1.11785579e-04, 1.92376956e-06, 2.18832980e-17]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.73861623e-01, 3.26138347e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.52222496e-01, 3.54358554e-01, 1.93418905e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [2.91181535e-01, 2.23538175e-01, 1.03832059e-01, ...,\n",
            "          1.72569364e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [3.86332661e-01, 8.29230100e-02, 2.29821280e-02, ...,\n",
            "          1.26586094e-01, 2.20517322e-01, 0.00000000e+00],\n",
            "         [9.84848797e-01, 1.49886537e-05, 4.62295202e-06, ...,\n",
            "          6.90440402e-06, 3.56912778e-07, 2.23603754e-16]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.49916339e-01, 1.50083691e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [3.61642420e-01, 5.91011167e-01, 4.73463759e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.77111238e-01, 1.72799557e-01, 1.39977261e-01, ...,\n",
            "          1.43722087e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.41446668e-02, 8.17126632e-01, 5.13984868e-03, ...,\n",
            "          2.83325166e-02, 4.77609923e-03, 0.00000000e+00],\n",
            "         [8.18832099e-01, 7.50936815e-05, 1.16041698e-03, ...,\n",
            "          8.68789777e-02, 3.04256734e-02, 4.34464999e-11]]]],\n",
            "      dtype=float32)>, 'decoder_layer1_block2': <tf.Tensor: shape=(1, 16, 8, 100), dtype=float32, numpy=\n",
            "array([[[[0.01000068, 0.00998713, 0.00999002, ..., 0.01000088,\n",
            "          0.01000079, 0.01000089],\n",
            "         [0.01000043, 0.00996542, 0.00998013, ..., 0.01000231,\n",
            "          0.01000186, 0.01000171],\n",
            "         [0.00999699, 0.01002465, 0.0100146 , ..., 0.00999725,\n",
            "          0.00999684, 0.0099966 ],\n",
            "         ...,\n",
            "         [0.01000148, 0.01001419, 0.00999323, ..., 0.01000015,\n",
            "          0.0100002 , 0.01000038],\n",
            "         [0.01000455, 0.01001568, 0.00998566, ..., 0.0100021 ,\n",
            "          0.01000245, 0.01000281],\n",
            "         [0.01000486, 0.0099843 , 0.00996026, ..., 0.0100035 ,\n",
            "          0.0100035 , 0.01000399]],\n",
            "\n",
            "        [[0.0099972 , 0.01001715, 0.01001829, ..., 0.00999788,\n",
            "          0.0099976 , 0.00999707],\n",
            "         [0.00999861, 0.00999136, 0.01000058, ..., 0.00999906,\n",
            "          0.00999876, 0.00999834],\n",
            "         [0.00999694, 0.01000912, 0.0100122 , ..., 0.00999733,\n",
            "          0.00999713, 0.00999673],\n",
            "         ...,\n",
            "         [0.01000061, 0.00999653, 0.00999262, ..., 0.00999921,\n",
            "          0.00999962, 0.01000008],\n",
            "         [0.01000253, 0.00998002, 0.00997989, ..., 0.01000125,\n",
            "          0.0100016 , 0.01000214],\n",
            "         [0.00999932, 0.00997488, 0.0099885 , ..., 0.00999994,\n",
            "          0.00999968, 0.00999942]],\n",
            "\n",
            "        [[0.00999945, 0.01000721, 0.0100095 , ..., 0.00999933,\n",
            "          0.00999952, 0.00999954],\n",
            "         [0.00999483, 0.01002974, 0.0100455 , ..., 0.0099966 ,\n",
            "          0.00999617, 0.0099953 ],\n",
            "         [0.00999967, 0.01000043, 0.0100024 , ..., 0.01000028,\n",
            "          0.0100001 , 0.01000006],\n",
            "         ...,\n",
            "         [0.00999644, 0.00999295, 0.01001907, ..., 0.00999781,\n",
            "          0.00999751, 0.00999686],\n",
            "         [0.00999926, 0.00997508, 0.01000506, ..., 0.01000102,\n",
            "          0.01000067, 0.01000029],\n",
            "         [0.01000081, 0.0099695 , 0.00998696, ..., 0.01000116,\n",
            "          0.01000103, 0.01000072]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.0100003 , 0.01003285, 0.01001332, ..., 0.00999859,\n",
            "          0.00999895, 0.00999913],\n",
            "         [0.00999962, 0.00999195, 0.0099975 , ..., 0.01000032,\n",
            "          0.01000003, 0.00999973],\n",
            "         [0.01000079, 0.01001451, 0.01001345, ..., 0.00999992,\n",
            "          0.01000008, 0.01000011],\n",
            "         ...,\n",
            "         [0.01000145, 0.01001542, 0.00999675, ..., 0.00999993,\n",
            "          0.01000017, 0.01000047],\n",
            "         [0.01000139, 0.00999777, 0.00999361, ..., 0.01000097,\n",
            "          0.01000099, 0.01000089],\n",
            "         [0.01000829, 0.01002299, 0.00998884, ..., 0.01000309,\n",
            "          0.01000358, 0.01000401]],\n",
            "\n",
            "        [[0.01000062, 0.00998937, 0.00999519, ..., 0.01000009,\n",
            "          0.0100004 , 0.01000063],\n",
            "         [0.00999579, 0.00999049, 0.01001619, ..., 0.00999844,\n",
            "          0.00999811, 0.00999759],\n",
            "         [0.01000467, 0.00999028, 0.0099781 , ..., 0.0100032 ,\n",
            "          0.01000344, 0.01000381],\n",
            "         ...,\n",
            "         [0.01000182, 0.0099785 , 0.00998401, ..., 0.01000245,\n",
            "          0.0100021 , 0.0100018 ],\n",
            "         [0.00999978, 0.01002319, 0.01000453, ..., 0.00999931,\n",
            "          0.009999  , 0.00999857],\n",
            "         [0.01000182, 0.00998076, 0.00998781, ..., 0.01000189,\n",
            "          0.01000144, 0.01000097]],\n",
            "\n",
            "        [[0.00999976, 0.00998858, 0.009988  , ..., 0.01000087,\n",
            "          0.01000071, 0.0100006 ],\n",
            "         [0.01000163, 0.01001335, 0.00999582, ..., 0.01000109,\n",
            "          0.01000094, 0.01000091],\n",
            "         [0.010003  , 0.01001117, 0.00999812, ..., 0.01000052,\n",
            "          0.01000092, 0.01000144],\n",
            "         ...,\n",
            "         [0.01000304, 0.00998558, 0.00998782, ..., 0.0100027 ,\n",
            "          0.01000267, 0.01000262],\n",
            "         [0.00999993, 0.0099832 , 0.00999495, ..., 0.01000062,\n",
            "          0.01000046, 0.01000024],\n",
            "         [0.01000741, 0.01001786, 0.00999066, ..., 0.01000293,\n",
            "          0.01000356, 0.01000428]]]], dtype=float32)>, 'decoder_layer2_block1': <tf.Tensor: shape=(1, 16, 8, 8), dtype=float32, numpy=\n",
            "array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.93285453e-01, 1.06714517e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.45394993e-01, 1.23043256e-02, 4.23006155e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [3.85860711e-01, 1.33890035e-02, 2.51292735e-02, ...,\n",
            "          2.35894285e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.87314248e-01, 2.74395663e-02, 2.13882960e-02, ...,\n",
            "          3.73804532e-02, 6.68728426e-02, 0.00000000e+00],\n",
            "         [7.79479027e-01, 3.06769665e-02, 1.28548555e-02, ...,\n",
            "          2.05551740e-02, 3.62996198e-02, 6.19896427e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.49610519e-01, 3.50389510e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.29327703e-01, 9.73812118e-02, 7.32910708e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [2.93677181e-01, 6.83659315e-02, 5.29177673e-02, ...,\n",
            "          9.87959430e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.26291704e-01, 8.84070061e-03, 7.14575127e-03, ...,\n",
            "          1.60329249e-02, 2.08571535e-02, 0.00000000e+00],\n",
            "         [9.49691415e-01, 1.09560899e-02, 3.75979161e-03, ...,\n",
            "          2.12696823e-03, 8.10758444e-04, 2.70054676e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.10344005e-01, 8.96559581e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.54368007e-01, 1.44460574e-01, 1.01171449e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [2.42510304e-01, 8.39252695e-02, 1.66864946e-01, ...,\n",
            "          1.07353657e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.15793359e-01, 7.96945766e-02, 1.65051505e-01, ...,\n",
            "          9.71088782e-02, 4.35820408e-02, 0.00000000e+00],\n",
            "         [8.57080340e-01, 1.42012921e-03, 1.40440632e-02, ...,\n",
            "          1.14954691e-02, 1.05913701e-02, 8.58817175e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.38929021e-01, 6.10709861e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.81561494e-01, 8.86095241e-02, 1.29829019e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [2.06061959e-01, 2.20259745e-02, 3.02081943e-01, ...,\n",
            "          5.91226146e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.55745786e-01, 5.69811799e-02, 6.34522289e-02, ...,\n",
            "          9.07033384e-02, 7.20766261e-02, 0.00000000e+00],\n",
            "         [8.15056562e-01, 3.97558995e-02, 1.70530230e-02, ...,\n",
            "          1.82805303e-02, 7.53924483e-03, 4.22107875e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.19070899e-01, 8.09291005e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.47964704e-01, 9.44514424e-02, 5.75838648e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [3.07691157e-01, 6.04687519e-02, 2.65826769e-02, ...,\n",
            "          5.21050952e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.36251074e-01, 4.14293446e-02, 2.65672058e-02, ...,\n",
            "          6.41764998e-02, 1.02358006e-01, 0.00000000e+00],\n",
            "         [8.92741203e-01, 6.37440477e-03, 8.23519938e-03, ...,\n",
            "          2.41902322e-02, 7.04236701e-03, 3.98805849e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.11837637e-01, 8.81623849e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.34253436e-01, 1.88942462e-01, 5.76804161e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [2.67304480e-01, 8.96549374e-02, 1.02438591e-01, ...,\n",
            "          4.92671505e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.07325472e-01, 1.81104243e-02, 2.70597152e-02, ...,\n",
            "          2.15410534e-02, 9.62058827e-02, 0.00000000e+00],\n",
            "         [9.54145730e-01, 1.06491689e-02, 1.32537046e-02, ...,\n",
            "          6.75116573e-03, 8.23743700e-04, 8.87392275e-03]]]],\n",
            "      dtype=float32)>, 'decoder_layer2_block2': <tf.Tensor: shape=(1, 16, 8, 100), dtype=float32, numpy=\n",
            "array([[[[0.009996  , 0.01001866, 0.01003181, ..., 0.00999671,\n",
            "          0.00999661, 0.00999601],\n",
            "         [0.01000064, 0.010014  , 0.00999841, ..., 0.00999913,\n",
            "          0.00999929, 0.00999906],\n",
            "         [0.01000445, 0.01000145, 0.00998782, ..., 0.01000243,\n",
            "          0.01000304, 0.01000345],\n",
            "         ...,\n",
            "         [0.01000091, 0.00999703, 0.00998542, ..., 0.01000065,\n",
            "          0.01000051, 0.0100006 ],\n",
            "         [0.01000859, 0.00996698, 0.00994531, ..., 0.01000656,\n",
            "          0.01000648, 0.01000663],\n",
            "         [0.01000905, 0.00999299, 0.0099584 , ..., 0.01000541,\n",
            "          0.01000629, 0.01000717]],\n",
            "\n",
            "        [[0.01000004, 0.00997756, 0.01000214, ..., 0.00999953,\n",
            "          0.01000029, 0.01000119],\n",
            "         [0.00999947, 0.00999807, 0.01000274, ..., 0.00999994,\n",
            "          0.01000006, 0.01000009],\n",
            "         [0.01000256, 0.00999884, 0.01000046, ..., 0.01000177,\n",
            "          0.01000174, 0.01000184],\n",
            "         ...,\n",
            "         [0.00999753, 0.00998141, 0.00999824, ..., 0.00999974,\n",
            "          0.00999914, 0.00999848],\n",
            "         [0.01000028, 0.01001955, 0.00999877, ..., 0.01000001,\n",
            "          0.00999951, 0.00999911],\n",
            "         [0.00999917, 0.01003307, 0.01001221, ..., 0.00999932,\n",
            "          0.00999872, 0.009998  ]],\n",
            "\n",
            "        [[0.01000057, 0.00999915, 0.00998508, ..., 0.01000059,\n",
            "          0.01000052, 0.01000085],\n",
            "         [0.01000315, 0.01001264, 0.00997751, ..., 0.01000065,\n",
            "          0.01000105, 0.01000183],\n",
            "         [0.0100024 , 0.00999891, 0.00997605, ..., 0.01000091,\n",
            "          0.01000115, 0.01000162],\n",
            "         ...,\n",
            "         [0.00999954, 0.00999846, 0.00999   , ..., 0.01000005,\n",
            "          0.00999986, 0.00999991],\n",
            "         [0.01000238, 0.00999842, 0.00999367, ..., 0.01000182,\n",
            "          0.01000187, 0.01000183],\n",
            "         [0.01000109, 0.00996262, 0.00998591, ..., 0.0100035 ,\n",
            "          0.01000289, 0.01000222]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01000866, 0.01002153, 0.00998237, ..., 0.01000209,\n",
            "          0.01000326, 0.0100043 ],\n",
            "         [0.00999784, 0.01002118, 0.01003607, ..., 0.00999683,\n",
            "          0.00999683, 0.00999619],\n",
            "         [0.00999898, 0.009985  , 0.01000804, ..., 0.00999964,\n",
            "          0.00999937, 0.00999894],\n",
            "         ...,\n",
            "         [0.00999556, 0.01000153, 0.01002943, ..., 0.00999685,\n",
            "          0.0099964 , 0.00999562],\n",
            "         [0.00999095, 0.00998009, 0.01003293, ..., 0.00999586,\n",
            "          0.0099947 , 0.00999347],\n",
            "         [0.00999514, 0.01003357, 0.01003228, ..., 0.00999565,\n",
            "          0.00999532, 0.00999477]],\n",
            "\n",
            "        [[0.01000338, 0.01006074, 0.01002864, ..., 0.00999847,\n",
            "          0.01000006, 0.01000134],\n",
            "         [0.01000089, 0.01004251, 0.0100279 , ..., 0.01000009,\n",
            "          0.01000029, 0.00999998],\n",
            "         [0.01000278, 0.01002688, 0.01000604, ..., 0.01000034,\n",
            "          0.0100012 , 0.01000182],\n",
            "         ...,\n",
            "         [0.0099996 , 0.01001497, 0.0100146 , ..., 0.00999926,\n",
            "          0.00999931, 0.00999933],\n",
            "         [0.00999946, 0.00999469, 0.01000063, ..., 0.00999931,\n",
            "          0.00999931, 0.00999939],\n",
            "         [0.00999769, 0.00994683, 0.0099855 , ..., 0.01000124,\n",
            "          0.01000033, 0.00999923]],\n",
            "\n",
            "        [[0.00999976, 0.01008145, 0.01004353, ..., 0.00999708,\n",
            "          0.00999741, 0.0099971 ],\n",
            "         [0.00999921, 0.01001499, 0.01001083, ..., 0.00999961,\n",
            "          0.00999952, 0.00999913],\n",
            "         [0.01000571, 0.01002652, 0.01000839, ..., 0.01000268,\n",
            "          0.01000353, 0.01000402],\n",
            "         ...,\n",
            "         [0.01000494, 0.01002791, 0.01000189, ..., 0.01000095,\n",
            "          0.01000168, 0.01000199],\n",
            "         [0.0100041 , 0.00997329, 0.00997049, ..., 0.01000266,\n",
            "          0.01000301, 0.01000357],\n",
            "         [0.01000681, 0.00995246, 0.00995161, ..., 0.01000472,\n",
            "          0.01000527, 0.01000607]]]], dtype=float32)>, 'decoder_layer3_block1': <tf.Tensor: shape=(1, 16, 8, 8), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9244116 , 0.07558842, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.7981865 , 0.0537373 , 0.14807619, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.42005014, 0.02201712, 0.17293599, ..., 0.12173942,\n",
            "          0.        , 0.        ],\n",
            "         [0.29702488, 0.05668667, 0.0800147 , ..., 0.17599897,\n",
            "          0.16128108, 0.        ],\n",
            "         [0.7642489 , 0.02632488, 0.03950074, ..., 0.03173399,\n",
            "          0.02198058, 0.08920754]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8313045 , 0.16869552, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.45026943, 0.3570961 , 0.19263445, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.07375231, 0.00566773, 0.03206754, ..., 0.01154608,\n",
            "          0.        , 0.        ],\n",
            "         [0.34244987, 0.02878505, 0.07850235, ..., 0.1054223 ,\n",
            "          0.17570634, 0.        ],\n",
            "         [0.9446106 , 0.0019753 , 0.00766261, ..., 0.00241445,\n",
            "          0.00247467, 0.02162731]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8371697 , 0.16283032, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.79903424, 0.10823897, 0.09272683, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.5789874 , 0.0044664 , 0.02271706, ..., 0.0091633 ,\n",
            "          0.        , 0.        ],\n",
            "         [0.70495665, 0.01165952, 0.02323527, ..., 0.0209798 ,\n",
            "          0.04851709, 0.        ],\n",
            "         [0.8280724 , 0.00603885, 0.03915972, ..., 0.01628894,\n",
            "          0.01536521, 0.03477451]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.78448385, 0.21551615, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.714748  , 0.15928362, 0.12596835, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.30391657, 0.10375518, 0.08212293, ..., 0.08030248,\n",
            "          0.        , 0.        ],\n",
            "         [0.6022415 , 0.03487949, 0.02308568, ..., 0.00993229,\n",
            "          0.03735485, 0.        ],\n",
            "         [0.8691458 , 0.0087463 , 0.01459375, ..., 0.03576393,\n",
            "          0.04044548, 0.01059758]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9503254 , 0.04967464, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.79646313, 0.01821268, 0.1853241 , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.35776496, 0.06364654, 0.03981362, ..., 0.01176141,\n",
            "          0.        , 0.        ],\n",
            "         [0.88250697, 0.01009763, 0.01797178, ..., 0.00376482,\n",
            "          0.01021475, 0.        ],\n",
            "         [0.8094853 , 0.01427046, 0.0605189 , ..., 0.01189524,\n",
            "          0.00410896, 0.08601015]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.6927509 , 0.30724916, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9562094 , 0.02642654, 0.01736404, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.48085523, 0.00807815, 0.01431952, ..., 0.07493012,\n",
            "          0.        , 0.        ],\n",
            "         [0.5653454 , 0.00801751, 0.0041173 , ..., 0.07583393,\n",
            "          0.00784327, 0.        ],\n",
            "         [0.81398004, 0.02021331, 0.01486743, ..., 0.075888  ,\n",
            "          0.0316234 , 0.01728918]]]], dtype=float32)>, 'decoder_layer3_block2': <tf.Tensor: shape=(1, 16, 8, 100), dtype=float32, numpy=\n",
            "array([[[[0.00999776, 0.00997521, 0.00999752, ..., 0.00999933,\n",
            "          0.00999924, 0.00999907],\n",
            "         [0.00999924, 0.01001825, 0.01001783, ..., 0.00999841,\n",
            "          0.00999862, 0.00999859],\n",
            "         [0.0099963 , 0.00999025, 0.01001395, ..., 0.009998  ,\n",
            "          0.00999773, 0.0099972 ],\n",
            "         ...,\n",
            "         [0.00999474, 0.00997539, 0.01000713, ..., 0.0099979 ,\n",
            "          0.00999736, 0.00999661],\n",
            "         [0.0099938 , 0.00998048, 0.01001374, ..., 0.00999725,\n",
            "          0.00999654, 0.00999557],\n",
            "         [0.00999884, 0.01002766, 0.01001339, ..., 0.00999877,\n",
            "          0.00999844, 0.00999802]],\n",
            "\n",
            "        [[0.01000006, 0.01002868, 0.01000977, ..., 0.00999775,\n",
            "          0.00999848, 0.00999891],\n",
            "         [0.00999308, 0.01003433, 0.01004515, ..., 0.00999366,\n",
            "          0.00999356, 0.00999308],\n",
            "         [0.01000243, 0.01001506, 0.01000325, ..., 0.01000095,\n",
            "          0.0100015 , 0.01000202],\n",
            "         ...,\n",
            "         [0.01000225, 0.01002663, 0.00999993, ..., 0.00999953,\n",
            "          0.01      , 0.01000033],\n",
            "         [0.01000463, 0.01000947, 0.0099819 , ..., 0.01000166,\n",
            "          0.01000228, 0.01000291],\n",
            "         [0.00999689, 0.01003229, 0.01002909, ..., 0.00999727,\n",
            "          0.00999721, 0.00999687]],\n",
            "\n",
            "        [[0.01000461, 0.00995377, 0.00994668, ..., 0.01000452,\n",
            "          0.01000441, 0.01000444],\n",
            "         [0.01000092, 0.00999524, 0.00999513, ..., 0.0100006 ,\n",
            "          0.01000062, 0.01000068],\n",
            "         [0.00999913, 0.01001687, 0.01001352, ..., 0.00999939,\n",
            "          0.00999923, 0.00999901],\n",
            "         ...,\n",
            "         [0.01000126, 0.00997133, 0.00996633, ..., 0.01000225,\n",
            "          0.01000209, 0.01000222],\n",
            "         [0.01000444, 0.00996548, 0.00995509, ..., 0.01000377,\n",
            "          0.0100039 , 0.01000438],\n",
            "         [0.01000425, 0.00995529, 0.00994618, ..., 0.01000457,\n",
            "          0.01000436, 0.01000498]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01000293, 0.01000372, 0.00999307, ..., 0.01000154,\n",
            "          0.01000206, 0.01000256],\n",
            "         [0.00999637, 0.01000656, 0.01001932, ..., 0.00999756,\n",
            "          0.00999681, 0.0099961 ],\n",
            "         [0.00999989, 0.00996844, 0.00998592, ..., 0.01000159,\n",
            "          0.01000122, 0.01000094],\n",
            "         ...,\n",
            "         [0.01000203, 0.00995404, 0.00996553, ..., 0.01000343,\n",
            "          0.0100032 , 0.01000313],\n",
            "         [0.00999459, 0.01001536, 0.01002433, ..., 0.00999676,\n",
            "          0.00999605, 0.00999532],\n",
            "         [0.00999711, 0.00998606, 0.01000185, ..., 0.00999893,\n",
            "          0.00999836, 0.00999782]],\n",
            "\n",
            "        [[0.00999948, 0.01000496, 0.01000573, ..., 0.00999958,\n",
            "          0.00999951, 0.00999937],\n",
            "         [0.01000237, 0.00999783, 0.0099948 , ..., 0.00999993,\n",
            "          0.0100003 , 0.01000075],\n",
            "         [0.01000797, 0.00999916, 0.00998199, ..., 0.01000486,\n",
            "          0.01000554, 0.01000648],\n",
            "         ...,\n",
            "         [0.01000224, 0.00999255, 0.00998491, ..., 0.01000257,\n",
            "          0.01000255, 0.01000269],\n",
            "         [0.00999727, 0.00998404, 0.01000469, ..., 0.00999953,\n",
            "          0.00999905, 0.0099986 ],\n",
            "         [0.00999546, 0.0099774 , 0.01000811, ..., 0.00999893,\n",
            "          0.00999819, 0.00999764]],\n",
            "\n",
            "        [[0.01000124, 0.01000598, 0.00999981, ..., 0.01000016,\n",
            "          0.0100007 , 0.01000108],\n",
            "         [0.00999571, 0.01002648, 0.01003708, ..., 0.00999666,\n",
            "          0.00999664, 0.00999627],\n",
            "         [0.01000268, 0.01002049, 0.00999674, ..., 0.00999989,\n",
            "          0.01000066, 0.01000134],\n",
            "         ...,\n",
            "         [0.01000051, 0.01001342, 0.00999254, ..., 0.00999976,\n",
            "          0.00999953, 0.0099995 ],\n",
            "         [0.01000288, 0.00998567, 0.00996525, ..., 0.01000255,\n",
            "          0.01000232, 0.01000249],\n",
            "         [0.00999625, 0.00995578, 0.00997905, ..., 0.01000154,\n",
            "          0.0100001 , 0.00999908]]]], dtype=float32)>, 'decoder_layer4_block1': <tf.Tensor: shape=(1, 16, 8, 8), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8520836 , 0.14791645, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.695791  , 0.2751483 , 0.02906066, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.0623412 , 0.01312232, 0.04913497, ..., 0.06291708,\n",
            "          0.        , 0.        ],\n",
            "         [0.47116476, 0.11038589, 0.04770772, ..., 0.07889888,\n",
            "          0.05699581, 0.        ],\n",
            "         [0.6274649 , 0.01761748, 0.08793061, ..., 0.02651257,\n",
            "          0.04018763, 0.04325161]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.7731411 , 0.22685893, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9211343 , 0.07487685, 0.00398889, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.12364356, 0.03335065, 0.01428872, ..., 0.05133085,\n",
            "          0.        , 0.        ],\n",
            "         [0.42247692, 0.05244222, 0.05459231, ..., 0.02509181,\n",
            "          0.06386117, 0.        ],\n",
            "         [0.8624971 , 0.00588401, 0.00855509, ..., 0.00596635,\n",
            "          0.00727498, 0.07999294]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9418532 , 0.0581468 , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8061797 , 0.158084  , 0.03573631, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.13668004, 0.11273669, 0.05491656, ..., 0.20602128,\n",
            "          0.        , 0.        ],\n",
            "         [0.68021625, 0.03358981, 0.1305048 , ..., 0.03364459,\n",
            "          0.01588008, 0.        ],\n",
            "         [0.86079407, 0.00940804, 0.05571233, ..., 0.01426077,\n",
            "          0.00217096, 0.03653192]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9181176 , 0.08188241, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.72082436, 0.12400378, 0.1551719 , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.32370156, 0.15792134, 0.01971197, ..., 0.11750415,\n",
            "          0.        , 0.        ],\n",
            "         [0.5742736 , 0.17031083, 0.01435098, ..., 0.06962854,\n",
            "          0.03866014, 0.        ],\n",
            "         [0.7023861 , 0.03865359, 0.02915378, ..., 0.03775857,\n",
            "          0.06217234, 0.06504699]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9691886 , 0.03081148, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.94214016, 0.03725015, 0.02060962, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.2855278 , 0.04111348, 0.17949145, ..., 0.0733823 ,\n",
            "          0.        , 0.        ],\n",
            "         [0.46879414, 0.05115535, 0.05672891, ..., 0.08065999,\n",
            "          0.10825051, 0.        ],\n",
            "         [0.5444077 , 0.12262885, 0.01577382, ..., 0.00953542,\n",
            "          0.05531765, 0.0541077 ]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.81440324, 0.18559676, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8825736 , 0.07739146, 0.04003491, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.32835057, 0.16044886, 0.04249282, ..., 0.06748885,\n",
            "          0.        , 0.        ],\n",
            "         [0.5619571 , 0.09739602, 0.02708358, ..., 0.05250442,\n",
            "          0.08996911, 0.        ],\n",
            "         [0.5698255 , 0.07395253, 0.01982201, ..., 0.06942417,\n",
            "          0.14393085, 0.04203735]]]], dtype=float32)>, 'decoder_layer4_block2': <tf.Tensor: shape=(1, 16, 8, 100), dtype=float32, numpy=\n",
            "array([[[[0.01000333, 0.00998453, 0.00998235, ..., 0.01000248,\n",
            "          0.01000271, 0.0100031 ],\n",
            "         [0.01000083, 0.01000715, 0.0100025 , ..., 0.01000022,\n",
            "          0.01000047, 0.01000079],\n",
            "         [0.01000317, 0.01002068, 0.00999675, ..., 0.01000134,\n",
            "          0.01000163, 0.01000188],\n",
            "         ...,\n",
            "         [0.0100025 , 0.0099928 , 0.00997598, ..., 0.01000164,\n",
            "          0.01000148, 0.01000126],\n",
            "         [0.0099981 , 0.01000219, 0.01000263, ..., 0.00999905,\n",
            "          0.00999857, 0.00999793],\n",
            "         [0.00999874, 0.00998282, 0.00999251, ..., 0.01000012,\n",
            "          0.00999962, 0.00999907]],\n",
            "\n",
            "        [[0.01000021, 0.00998794, 0.00998619, ..., 0.01000051,\n",
            "          0.01000045, 0.01000064],\n",
            "         [0.00999813, 0.00998969, 0.01001174, ..., 0.00999905,\n",
            "          0.0099986 , 0.00999795],\n",
            "         [0.00999653, 0.00996913, 0.00998764, ..., 0.0099998 ,\n",
            "          0.00999951, 0.00999931],\n",
            "         ...,\n",
            "         [0.01000706, 0.0099998 , 0.00997995, ..., 0.01000422,\n",
            "          0.01000487, 0.01000537],\n",
            "         [0.0100033 , 0.00999921, 0.00999509, ..., 0.01000235,\n",
            "          0.01000274, 0.01000289],\n",
            "         [0.01000304, 0.00996505, 0.00995214, ..., 0.01000384,\n",
            "          0.01000333, 0.010003  ]],\n",
            "\n",
            "        [[0.01000076, 0.00995413, 0.00997048, ..., 0.01000209,\n",
            "          0.01000183, 0.010002  ],\n",
            "         [0.0099978 , 0.00998385, 0.00998601, ..., 0.00999823,\n",
            "          0.00999803, 0.00999775],\n",
            "         [0.00999658, 0.01000017, 0.01001289, ..., 0.00999674,\n",
            "          0.00999638, 0.00999577],\n",
            "         ...,\n",
            "         [0.00999855, 0.00996412, 0.00998599, ..., 0.01000134,\n",
            "          0.01000099, 0.01000079],\n",
            "         [0.00999741, 0.00995208, 0.0099896 , ..., 0.01000102,\n",
            "          0.0100005 , 0.00999991],\n",
            "         [0.00999975, 0.00996723, 0.00999232, ..., 0.01000286,\n",
            "          0.01000259, 0.01000238]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00999515, 0.01000527, 0.01001456, ..., 0.00999708,\n",
            "          0.00999653, 0.00999592],\n",
            "         [0.01000887, 0.01005667, 0.01000388, ..., 0.01000251,\n",
            "          0.01000403, 0.01000519],\n",
            "         [0.00999982, 0.01003531, 0.01001976, ..., 0.00999843,\n",
            "          0.00999874, 0.00999885],\n",
            "         ...,\n",
            "         [0.00999627, 0.00995054, 0.00998438, ..., 0.01000162,\n",
            "          0.01000075, 0.00999993],\n",
            "         [0.01000078, 0.01001333, 0.01000101, ..., 0.00999924,\n",
            "          0.00999993, 0.01000007],\n",
            "         [0.0100003 , 0.0099803 , 0.00999399, ..., 0.01000086,\n",
            "          0.010001  , 0.01000075]],\n",
            "\n",
            "        [[0.01000048, 0.01001342, 0.01000176, ..., 0.01000036,\n",
            "          0.01000052, 0.01000065],\n",
            "         [0.01000424, 0.01001521, 0.00999983, ..., 0.01000103,\n",
            "          0.01000099, 0.01000096],\n",
            "         [0.00999884, 0.0099644 , 0.00999013, ..., 0.00999993,\n",
            "          0.00999933, 0.00999863],\n",
            "         ...,\n",
            "         [0.00999763, 0.00994521, 0.00999399, ..., 0.01000088,\n",
            "          0.0100001 , 0.00999954],\n",
            "         [0.00999878, 0.00992081, 0.0099788 , ..., 0.01000317,\n",
            "          0.01000189, 0.01000094],\n",
            "         [0.00999609, 0.00993131, 0.00998422, ..., 0.01000001,\n",
            "          0.00999947, 0.00999928]],\n",
            "\n",
            "        [[0.00999795, 0.00997679, 0.01000392, ..., 0.00999954,\n",
            "          0.0099996 , 0.00999981],\n",
            "         [0.01000197, 0.01000967, 0.0100038 , ..., 0.01000063,\n",
            "          0.01000097, 0.01000151],\n",
            "         [0.0100017 , 0.01002068, 0.01000052, ..., 0.00999982,\n",
            "          0.01000024, 0.01000075],\n",
            "         ...,\n",
            "         [0.01000326, 0.00999877, 0.00999577, ..., 0.01000147,\n",
            "          0.0100019 , 0.01000251],\n",
            "         [0.01000318, 0.01000145, 0.00999886, ..., 0.01000111,\n",
            "          0.01000171, 0.01000237],\n",
            "         [0.0100012 , 0.00998958, 0.01000111, ..., 0.01000104,\n",
            "          0.01000123, 0.01000127]]]], dtype=float32)>, 'decoder_layer5_block1': <tf.Tensor: shape=(1, 16, 8, 8), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8343494 , 0.16565065, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.7695119 , 0.20506239, 0.02542574, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.12259237, 0.14287256, 0.02420263, ..., 0.31750423,\n",
            "          0.        , 0.        ],\n",
            "         [0.27443567, 0.22530712, 0.0320345 , ..., 0.10848492,\n",
            "          0.06092764, 0.        ],\n",
            "         [0.36469734, 0.18197148, 0.05339749, ..., 0.16671395,\n",
            "          0.02782943, 0.07955022]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9820425 , 0.01795747, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.6183628 , 0.11511923, 0.26651797, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.39752442, 0.01926456, 0.05318219, ..., 0.08831918,\n",
            "          0.        , 0.        ],\n",
            "         [0.34300774, 0.07507959, 0.1911743 , ..., 0.11761647,\n",
            "          0.04868863, 0.        ],\n",
            "         [0.29595438, 0.13050331, 0.12406337, ..., 0.17464776,\n",
            "          0.06141299, 0.04915348]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.83632874, 0.1636713 , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8150823 , 0.03787132, 0.14704637, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.12122054, 0.29743585, 0.28180462, ..., 0.05977721,\n",
            "          0.        , 0.        ],\n",
            "         [0.05070018, 0.25691605, 0.58085084, ..., 0.01473861,\n",
            "          0.01274815, 0.        ],\n",
            "         [0.4586177 , 0.12359502, 0.26751375, ..., 0.02468576,\n",
            "          0.01607217, 0.00643571]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9079303 , 0.09206969, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.597933  , 0.24152642, 0.16054055, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.08741323, 0.19159597, 0.06669926, ..., 0.18174343,\n",
            "          0.        , 0.        ],\n",
            "         [0.15376121, 0.4032292 , 0.10567056, ..., 0.05267793,\n",
            "          0.02026633, 0.        ],\n",
            "         [0.25390768, 0.11212751, 0.07619131, ..., 0.09856224,\n",
            "          0.0694887 , 0.2879021 ]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.7354214 , 0.26457858, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.6354832 , 0.27658746, 0.08792938, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.5546813 , 0.14313237, 0.03481366, ..., 0.12699814,\n",
            "          0.        , 0.        ],\n",
            "         [0.772542  , 0.0712835 , 0.00958799, ..., 0.06684452,\n",
            "          0.01909159, 0.        ],\n",
            "         [0.6450761 , 0.15571983, 0.04407106, ..., 0.02121445,\n",
            "          0.03170203, 0.06543578]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9070503 , 0.09294964, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.89278805, 0.07962935, 0.0275826 , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.26797214, 0.05906156, 0.04543394, ..., 0.18117751,\n",
            "          0.        , 0.        ],\n",
            "         [0.28823975, 0.16054349, 0.05058713, ..., 0.10947973,\n",
            "          0.09797268, 0.        ],\n",
            "         [0.1576241 , 0.09264069, 0.07855316, ..., 0.12790595,\n",
            "          0.09528748, 0.14410934]]]], dtype=float32)>, 'decoder_layer5_block2': <tf.Tensor: shape=(1, 16, 8, 100), dtype=float32, numpy=\n",
            "array([[[[0.01000548, 0.00994896, 0.00995184, ..., 0.01000537,\n",
            "          0.01000554, 0.01000608],\n",
            "         [0.00999381, 0.01000896, 0.01002557, ..., 0.00999566,\n",
            "          0.00999554, 0.00999494],\n",
            "         [0.0099998 , 0.010012  , 0.01001275, ..., 0.00999869,\n",
            "          0.00999893, 0.00999917],\n",
            "         ...,\n",
            "         [0.00999729, 0.00999673, 0.01000989, ..., 0.0099982 ,\n",
            "          0.0099978 , 0.00999733],\n",
            "         [0.01000116, 0.00999853, 0.00999618, ..., 0.0099991 ,\n",
            "          0.0099993 , 0.00999954],\n",
            "         [0.01000007, 0.00999764, 0.01000271, ..., 0.00999953,\n",
            "          0.0099993 , 0.0099991 ]],\n",
            "\n",
            "        [[0.00999926, 0.00999385, 0.00999962, ..., 0.00999945,\n",
            "          0.00999965, 0.00999978],\n",
            "         [0.01000302, 0.01001083, 0.01000628, ..., 0.01000064,\n",
            "          0.01000152, 0.01000225],\n",
            "         [0.01000118, 0.01003276, 0.01001639, ..., 0.00999845,\n",
            "          0.00999931, 0.00999993],\n",
            "         ...,\n",
            "         [0.01000339, 0.01001507, 0.01001522, ..., 0.01000092,\n",
            "          0.01000152, 0.01000186],\n",
            "         [0.01000282, 0.00998604, 0.00999099, ..., 0.01000269,\n",
            "          0.01000276, 0.01000284],\n",
            "         [0.00999827, 0.01000084, 0.01001214, ..., 0.00999839,\n",
            "          0.00999842, 0.00999821]],\n",
            "\n",
            "        [[0.01000278, 0.00998394, 0.00997101, ..., 0.01000158,\n",
            "          0.01000197, 0.01000254],\n",
            "         [0.01000104, 0.01001753, 0.00999681, ..., 0.0099981 ,\n",
            "          0.00999905, 0.00999994],\n",
            "         [0.00999892, 0.01000499, 0.01000287, ..., 0.00999876,\n",
            "          0.009999  , 0.0099993 ],\n",
            "         ...,\n",
            "         [0.01000079, 0.00999973, 0.00998616, ..., 0.00999939,\n",
            "          0.00999969, 0.00999995],\n",
            "         [0.00999719, 0.00997716, 0.00999527, ..., 0.0099992 ,\n",
            "          0.00999869, 0.00999791],\n",
            "         [0.00999479, 0.00999809, 0.01002684, ..., 0.00999796,\n",
            "          0.009997  , 0.00999576]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00999906, 0.00996585, 0.00998881, ..., 0.0100012 ,\n",
            "          0.01000087, 0.01000064],\n",
            "         [0.01000133, 0.00997459, 0.00997896, ..., 0.01000238,\n",
            "          0.01000225, 0.01000226],\n",
            "         [0.01000311, 0.00994366, 0.0099679 , ..., 0.01000455,\n",
            "          0.01000442, 0.01000457],\n",
            "         ...,\n",
            "         [0.01000142, 0.00995545, 0.00997381, ..., 0.01000308,\n",
            "          0.01000278, 0.0100027 ],\n",
            "         [0.01000306, 0.00994511, 0.00995771, ..., 0.01000463,\n",
            "          0.01000421, 0.01000417],\n",
            "         [0.00999891, 0.00991568, 0.0099502 , ..., 0.01000351,\n",
            "          0.01000285, 0.01000265]],\n",
            "\n",
            "        [[0.01000003, 0.01001117, 0.01000699, ..., 0.00999948,\n",
            "          0.00999973, 0.00999998],\n",
            "         [0.01000026, 0.01000383, 0.01000637, ..., 0.00999948,\n",
            "          0.00999994, 0.01000047],\n",
            "         [0.0100034 , 0.00999787, 0.00999155, ..., 0.0100011 ,\n",
            "          0.01000182, 0.01000277],\n",
            "         ...,\n",
            "         [0.00999915, 0.0099975 , 0.01000358, ..., 0.00999915,\n",
            "          0.00999935, 0.00999955],\n",
            "         [0.00999816, 0.00999051, 0.01000325, ..., 0.00999912,\n",
            "          0.00999927, 0.00999939],\n",
            "         [0.00999397, 0.00999491, 0.01001895, ..., 0.00999636,\n",
            "          0.00999595, 0.00999521]],\n",
            "\n",
            "        [[0.009995  , 0.00998863, 0.01001147, ..., 0.0099981 ,\n",
            "          0.00999723, 0.00999658],\n",
            "         [0.00999719, 0.00996726, 0.00999414, ..., 0.01000014,\n",
            "          0.00999954, 0.00999895],\n",
            "         [0.01000075, 0.00996045, 0.00998375, ..., 0.01000279,\n",
            "          0.01000256, 0.01000242],\n",
            "         ...,\n",
            "         [0.00999884, 0.00996257, 0.00999065, ..., 0.01000197,\n",
            "          0.01000101, 0.01000018],\n",
            "         [0.00999562, 0.00996344, 0.00999997, ..., 0.00999992,\n",
            "          0.00999849, 0.00999719],\n",
            "         [0.01000092, 0.01002427, 0.01001978, ..., 0.00999987,\n",
            "          0.01      , 0.00999999]]]], dtype=float32)>, 'decoder_layer6_block1': <tf.Tensor: shape=(1, 16, 8, 8), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.66868746, 0.33131257, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.44927448, 0.39361358, 0.15711194, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.18061836, 0.14978541, 0.04131765, ..., 0.17418148,\n",
            "          0.        , 0.        ],\n",
            "         [0.26924667, 0.39555514, 0.03861038, ..., 0.20797963,\n",
            "          0.01510331, 0.        ],\n",
            "         [0.06038959, 0.06470899, 0.10970993, ..., 0.06188412,\n",
            "          0.2374845 , 0.0520218 ]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.94735265, 0.05264733, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.65069336, 0.18268952, 0.16661718, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.09385956, 0.01787031, 0.05700373, ..., 0.13140385,\n",
            "          0.        , 0.        ],\n",
            "         [0.3836749 , 0.10025477, 0.30650944, ..., 0.08398052,\n",
            "          0.06273165, 0.        ],\n",
            "         [0.02975202, 0.00982711, 0.05549051, ..., 0.06918895,\n",
            "          0.19886938, 0.29572105]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.6884176 , 0.31158245, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.31419212, 0.65772736, 0.02808054, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.01991971, 0.0077626 , 0.03886725, ..., 0.00959744,\n",
            "          0.        , 0.        ],\n",
            "         [0.16593704, 0.11488575, 0.2815609 , ..., 0.06801745,\n",
            "          0.09682451, 0.        ],\n",
            "         [0.0800164 , 0.5577851 , 0.03680072, ..., 0.00422513,\n",
            "          0.01501503, 0.06858972]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9419826 , 0.05801731, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8416484 , 0.05385235, 0.10449924, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.00558983, 0.01112237, 0.0048491 , ..., 0.0043286 ,\n",
            "          0.        , 0.        ],\n",
            "         [0.35761735, 0.03055929, 0.05587376, ..., 0.0803773 ,\n",
            "          0.1635793 , 0.        ],\n",
            "         [0.40131706, 0.05472929, 0.00950338, ..., 0.08640298,\n",
            "          0.09376898, 0.04129471]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.8141856 , 0.18581435, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.25783187, 0.53791434, 0.2042539 , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.17442782, 0.19769157, 0.1591064 , ..., 0.05897544,\n",
            "          0.        , 0.        ],\n",
            "         [0.06324692, 0.1194492 , 0.11590255, ..., 0.02691025,\n",
            "          0.0854446 , 0.        ],\n",
            "         [0.1065111 , 0.0689866 , 0.12314809, ..., 0.14693214,\n",
            "          0.19343938, 0.09150697]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.98416543, 0.01583458, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.5741596 , 0.3591123 , 0.06672804, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.19961347, 0.04098044, 0.19793864, ..., 0.03146273,\n",
            "          0.        , 0.        ],\n",
            "         [0.3377575 , 0.12112881, 0.01098237, ..., 0.27975926,\n",
            "          0.06859144, 0.        ],\n",
            "         [0.04580044, 0.01987895, 0.03578907, ..., 0.10375335,\n",
            "          0.19985789, 0.40194273]]]], dtype=float32)>, 'decoder_layer6_block2': <tf.Tensor: shape=(1, 16, 8, 100), dtype=float32, numpy=\n",
            "array([[[[0.01000352, 0.01001435, 0.00999352, ..., 0.01000136,\n",
            "          0.01000183, 0.01000214],\n",
            "         [0.01000002, 0.01001544, 0.01000981, ..., 0.0099994 ,\n",
            "          0.00999954, 0.0099996 ],\n",
            "         [0.01000281, 0.01002893, 0.01001076, ..., 0.00999957,\n",
            "          0.0100003 , 0.01000073],\n",
            "         ...,\n",
            "         [0.01000011, 0.01001549, 0.01000954, ..., 0.00999959,\n",
            "          0.0099996 , 0.00999938],\n",
            "         [0.00999994, 0.00999898, 0.01000866, ..., 0.0100005 ,\n",
            "          0.01000037, 0.01000007],\n",
            "         [0.00999827, 0.00999976, 0.01001382, ..., 0.00999976,\n",
            "          0.00999925, 0.0099987 ]],\n",
            "\n",
            "        [[0.01000493, 0.00999003, 0.00997661, ..., 0.01000323,\n",
            "          0.01000338, 0.01000372],\n",
            "         [0.01000113, 0.00999799, 0.00998479, ..., 0.01000133,\n",
            "          0.01000119, 0.01000117],\n",
            "         [0.01000203, 0.01001129, 0.00998686, ..., 0.01000144,\n",
            "          0.01000154, 0.01000172],\n",
            "         ...,\n",
            "         [0.01000106, 0.01001315, 0.00999622, ..., 0.01000066,\n",
            "          0.01000066, 0.01000071],\n",
            "         [0.00999841, 0.01000092, 0.01000171, ..., 0.00999945,\n",
            "          0.00999912, 0.00999883],\n",
            "         [0.01000008, 0.01000375, 0.01000215, ..., 0.01000008,\n",
            "          0.01000005, 0.01000003]],\n",
            "\n",
            "        [[0.00999873, 0.01002059, 0.01001723, ..., 0.00999806,\n",
            "          0.00999816, 0.00999807],\n",
            "         [0.00999788, 0.01002328, 0.01002403, ..., 0.00999748,\n",
            "          0.00999753, 0.00999733],\n",
            "         [0.00999507, 0.01001639, 0.01003069, ..., 0.00999635,\n",
            "          0.00999588, 0.00999504],\n",
            "         ...,\n",
            "         [0.00999573, 0.00998922, 0.01001072, ..., 0.00999899,\n",
            "          0.00999817, 0.00999715],\n",
            "         [0.00999522, 0.00998762, 0.01000858, ..., 0.00999933,\n",
            "          0.00999832, 0.00999718],\n",
            "         [0.00999884, 0.00999384, 0.01000011, ..., 0.0100001 ,\n",
            "          0.00999984, 0.00999952]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01000358, 0.0100164 , 0.01000115, ..., 0.01000137,\n",
            "          0.01000169, 0.01000187],\n",
            "         [0.01000209, 0.01002752, 0.01001155, ..., 0.01000029,\n",
            "          0.01000067, 0.01000097],\n",
            "         [0.00999774, 0.01000882, 0.01001502, ..., 0.00999784,\n",
            "          0.0099978 , 0.00999751],\n",
            "         ...,\n",
            "         [0.00999596, 0.00998904, 0.01000536, ..., 0.00999827,\n",
            "          0.00999775, 0.00999723],\n",
            "         [0.00999704, 0.00999015, 0.01001268, ..., 0.00999807,\n",
            "          0.00999779, 0.00999722],\n",
            "         [0.00999746, 0.00998332, 0.0099987 , ..., 0.00999915,\n",
            "          0.00999881, 0.00999847]],\n",
            "\n",
            "        [[0.0100029 , 0.0099757 , 0.00997569, ..., 0.01000194,\n",
            "          0.0100024 , 0.01000292],\n",
            "         [0.01000151, 0.01000631, 0.00999676, ..., 0.01000075,\n",
            "          0.01000056, 0.01000038],\n",
            "         [0.01000222, 0.00999803, 0.00998855, ..., 0.01000108,\n",
            "          0.01000095, 0.01000083],\n",
            "         ...,\n",
            "         [0.00999995, 0.00998635, 0.00998828, ..., 0.01000083,\n",
            "          0.01000026, 0.00999971],\n",
            "         [0.00999949, 0.00998878, 0.0099911 , ..., 0.00999894,\n",
            "          0.00999867, 0.00999847],\n",
            "         [0.00999749, 0.00997704, 0.00998974, ..., 0.00999964,\n",
            "          0.00999897, 0.00999848]],\n",
            "\n",
            "        [[0.01000164, 0.00999696, 0.0099946 , ..., 0.01000138,\n",
            "          0.0100014 , 0.01000141],\n",
            "         [0.0100027 , 0.00999747, 0.00998936, ..., 0.01000146,\n",
            "          0.010002  , 0.01000252],\n",
            "         [0.01000359, 0.01000567, 0.00999044, ..., 0.0100013 ,\n",
            "          0.01000229, 0.01000318],\n",
            "         ...,\n",
            "         [0.009998  , 0.00999714, 0.01000317, ..., 0.00999865,\n",
            "          0.00999845, 0.00999821],\n",
            "         [0.00999702, 0.0099803 , 0.01000096, ..., 0.009999  ,\n",
            "          0.00999859, 0.00999816],\n",
            "         [0.00999737, 0.00998614, 0.01000271, ..., 0.0099989 ,\n",
            "          0.00999846, 0.00999804]]]], dtype=float32)>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvUivnGhMH9g",
        "outputId": "a5310b0e-e1ff-441b-a764-b0b12070b8e8"
      },
      "source": [
        "sentence = \"<start> juliet <end>\"\n",
        "print(evaluate(sentence)[0])"
      ],
      "id": "TvUivnGhMH9g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['<start> juliet <end>'], {'decoder_layer1_block1': <tf.Tensor: shape=(1, 16, 2, 2), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        ],\n",
            "         [0.6700952 , 0.32990485]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.17739664, 0.82260334]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9728928 , 0.02710711]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9861921 , 0.01380792]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8243811 , 0.17561887]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8723922 , 0.12760781]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.02206898, 0.977931  ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.86183745, 0.1381625 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9481114 , 0.05188864]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.97909456, 0.02090548]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.81110317, 0.18889686]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.96843475, 0.03156519]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9972746 , 0.0027254 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.5266218 , 0.47337827]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.94465613, 0.05534388]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9756436 , 0.0243564 ]]]], dtype=float32)>, 'decoder_layer1_block2': <tf.Tensor: shape=(1, 16, 2, 100), dtype=float32, numpy=\n",
            "array([[[[0.00999939, 0.00998253, 0.00999474, ..., 0.01000135,\n",
            "          0.01000087, 0.01000035],\n",
            "         [0.01000019, 0.00996406, 0.01000507, ..., 0.00999965,\n",
            "          0.00999784, 0.0099968 ]],\n",
            "\n",
            "        [[0.00999245, 0.00998858, 0.00999113, ..., 0.00999636,\n",
            "          0.00999476, 0.00999303],\n",
            "         [0.00998746, 0.0100119 , 0.00998497, ..., 0.00999148,\n",
            "          0.00999127, 0.00998961]],\n",
            "\n",
            "        [[0.01000965, 0.00998324, 0.01001064, ..., 0.01000625,\n",
            "          0.01000636, 0.01000721],\n",
            "         [0.01001562, 0.01001454, 0.01002356, ..., 0.01000891,\n",
            "          0.01001137, 0.01001553]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01000315, 0.01000634, 0.01000803, ..., 0.00999981,\n",
            "          0.01000011, 0.01000157],\n",
            "         [0.01000031, 0.01000819, 0.01000363, ..., 0.00999942,\n",
            "          0.00999988, 0.01000038]],\n",
            "\n",
            "        [[0.00999994, 0.00999449, 0.00999568, ..., 0.01000233,\n",
            "          0.01000153, 0.01000046],\n",
            "         [0.00999441, 0.00999079, 0.00999066, ..., 0.00999964,\n",
            "          0.00999805, 0.0099956 ]],\n",
            "\n",
            "        [[0.01000624, 0.00998651, 0.01000675, ..., 0.01000444,\n",
            "          0.01000512, 0.01000605],\n",
            "         [0.00999703, 0.0100067 , 0.0099989 , ..., 0.0099984 ,\n",
            "          0.00999855, 0.00999845]]]], dtype=float32)>, 'decoder_layer2_block1': <tf.Tensor: shape=(1, 16, 2, 2), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        ],\n",
            "         [0.94018704, 0.05981299]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9837863 , 0.01621369]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9867748 , 0.01322525]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.98114014, 0.01885988]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.6713348 , 0.32866523]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7725097 , 0.2274903 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9040332 , 0.09596685]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.92339706, 0.07660299]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.94755095, 0.05244899]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.994936  , 0.00506396]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8200349 , 0.17996503]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9752049 , 0.02479511]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.72054225, 0.27945772]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.99388605, 0.00611389]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.91797006, 0.08202995]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.95838434, 0.04161567]]]], dtype=float32)>, 'decoder_layer2_block2': <tf.Tensor: shape=(1, 16, 2, 100), dtype=float32, numpy=\n",
            "array([[[[0.01      , 0.00998886, 0.00999431, ..., 0.01000276,\n",
            "          0.01000171, 0.00999961],\n",
            "         [0.01000089, 0.0099647 , 0.00999887, ..., 0.01000287,\n",
            "          0.01000209, 0.01000053]],\n",
            "\n",
            "        [[0.00999419, 0.01000614, 0.00999135, ..., 0.00999887,\n",
            "          0.00999845, 0.00999721],\n",
            "         [0.01000476, 0.00998793, 0.01000803, ..., 0.01000094,\n",
            "          0.0100013 , 0.01000235]],\n",
            "\n",
            "        [[0.00999599, 0.01000069, 0.00999627, ..., 0.00999849,\n",
            "          0.00999806, 0.00999693],\n",
            "         [0.01000348, 0.00997825, 0.01000468, ..., 0.01000228,\n",
            "          0.01000271, 0.01000338]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00998666, 0.00998964, 0.00997286, ..., 0.00999537,\n",
            "          0.00999278, 0.00998849],\n",
            "         [0.00999316, 0.00998707, 0.00999068, ..., 0.00999687,\n",
            "          0.00999528, 0.00999251]],\n",
            "\n",
            "        [[0.0099959 , 0.0100208 , 0.00998561, ..., 0.01000049,\n",
            "          0.00999938, 0.00999821],\n",
            "         [0.01000381, 0.01000003, 0.01001195, ..., 0.01000101,\n",
            "          0.01000229, 0.01000347]],\n",
            "\n",
            "        [[0.01000038, 0.00999544, 0.00998893, ..., 0.01000484,\n",
            "          0.01000336, 0.01000071],\n",
            "         [0.01000576, 0.00997224, 0.01000907, ..., 0.01000546,\n",
            "          0.01000571, 0.01000603]]]], dtype=float32)>, 'decoder_layer3_block1': <tf.Tensor: shape=(1, 16, 2, 2), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        ],\n",
            "         [0.92780894, 0.07219112]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9691658 , 0.03083423]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9834473 , 0.01655271]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.5726443 , 0.42735574]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.81986076, 0.18013918]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.93522966, 0.06477028]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.93374604, 0.066254  ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9611257 , 0.03887434]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9855268 , 0.0144732 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.38242206, 0.6175779 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.87344956, 0.12655038]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.86389863, 0.13610129]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.866204  , 0.13379602]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.95180404, 0.048196  ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8972999 , 0.10270015]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8711263 , 0.12887368]]]], dtype=float32)>, 'decoder_layer3_block2': <tf.Tensor: shape=(1, 16, 2, 100), dtype=float32, numpy=\n",
            "array([[[[0.01000524, 0.00998282, 0.01000632, ..., 0.0100039 ,\n",
            "          0.01000412, 0.01000482],\n",
            "         [0.010012  , 0.01001055, 0.01002757, ..., 0.00999918,\n",
            "          0.01000047, 0.01000517]],\n",
            "\n",
            "        [[0.01000717, 0.0099871 , 0.01000883, ..., 0.0100061 ,\n",
            "          0.01000707, 0.01000863],\n",
            "         [0.01001388, 0.010008  , 0.01002031, ..., 0.01000454,\n",
            "          0.01000644, 0.01001084]],\n",
            "\n",
            "        [[0.01000185, 0.00996842, 0.00999702, ..., 0.01000479,\n",
            "          0.01000378, 0.01000179],\n",
            "         [0.00998762, 0.01001033, 0.00999437, ..., 0.00998837,\n",
            "          0.00998803, 0.00998779]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01000317, 0.01000918, 0.01000198, ..., 0.01000295,\n",
            "          0.01000315, 0.01000379],\n",
            "         [0.0099959 , 0.00998554, 0.00999066, ..., 0.00999989,\n",
            "          0.00999872, 0.00999653]],\n",
            "\n",
            "        [[0.01000323, 0.01002636, 0.01001254, ..., 0.00999959,\n",
            "          0.00999962, 0.01000107],\n",
            "         [0.01000163, 0.00998498, 0.00999977, ..., 0.01000166,\n",
            "          0.01000197, 0.01000251]],\n",
            "\n",
            "        [[0.01000139, 0.01000357, 0.00999728, ..., 0.01000312,\n",
            "          0.01000239, 0.01000124],\n",
            "         [0.00999922, 0.00999498, 0.01001021, ..., 0.00999548,\n",
            "          0.00999563, 0.00999598]]]], dtype=float32)>, 'decoder_layer4_block1': <tf.Tensor: shape=(1, 16, 2, 2), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        ],\n",
            "         [0.95488346, 0.04511655]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.97724104, 0.02275888]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.99758327, 0.00241672]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.990173  , 0.00982704]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7970247 , 0.20297529]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9875938 , 0.01240622]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.78113145, 0.21886857]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9857825 , 0.01421753]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.61993474, 0.38006523]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9955787 , 0.00442124]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8743518 , 0.12564822]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.73114413, 0.26885584]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.95330745, 0.04669254]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9305159 , 0.06948413]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.94104046, 0.05895953]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.96567255, 0.03432749]]]], dtype=float32)>, 'decoder_layer4_block2': <tf.Tensor: shape=(1, 16, 2, 100), dtype=float32, numpy=\n",
            "array([[[[0.00999644, 0.01000758, 0.00999678, ..., 0.0099964 ,\n",
            "          0.00999645, 0.00999622],\n",
            "         [0.01000009, 0.01001534, 0.01000375, ..., 0.00999917,\n",
            "          0.00999931, 0.00999982]],\n",
            "\n",
            "        [[0.01000079, 0.00999926, 0.01000215, ..., 0.0100002 ,\n",
            "          0.00999987, 0.00999995],\n",
            "         [0.01000172, 0.00995928, 0.01000608, ..., 0.01000085,\n",
            "          0.01000145, 0.01000172]],\n",
            "\n",
            "        [[0.0100027 , 0.01001703, 0.01000672, ..., 0.01000171,\n",
            "          0.01000146, 0.01000197],\n",
            "         [0.01000156, 0.00996912, 0.01001005, ..., 0.00999796,\n",
            "          0.00999817, 0.00999952]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01000308, 0.010002  , 0.01000439, ..., 0.01000022,\n",
            "          0.0100005 , 0.01000173],\n",
            "         [0.00999856, 0.00998427, 0.01000058, ..., 0.00999978,\n",
            "          0.01000009, 0.00999969]],\n",
            "\n",
            "        [[0.01000013, 0.01000509, 0.009999  , ..., 0.01000119,\n",
            "          0.01000124, 0.0100011 ],\n",
            "         [0.01001368, 0.0100136 , 0.01002569, ..., 0.01000399,\n",
            "          0.01000508, 0.0100097 ]],\n",
            "\n",
            "        [[0.01000852, 0.01000235, 0.0100106 , ..., 0.01000416,\n",
            "          0.01000607, 0.01000896],\n",
            "         [0.01000063, 0.00997814, 0.00999953, ..., 0.01000087,\n",
            "          0.01000157, 0.01000166]]]], dtype=float32)>, 'decoder_layer5_block1': <tf.Tensor: shape=(1, 16, 2, 2), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        ],\n",
            "         [0.7602482 , 0.23975183]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.77563906, 0.22436087]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9380629 , 0.06193705]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7744541 , 0.22554596]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.92255145, 0.07744861]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.6581033 , 0.3418968 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.84626126, 0.15373872]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.95398015, 0.04601984]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7948902 , 0.20510975]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9403878 , 0.05961219]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8212648 , 0.17873518]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8704859 , 0.12951405]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7097299 , 0.29027006]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9406415 , 0.05935854]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9263131 , 0.07368686]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7819627 , 0.21803728]]]], dtype=float32)>, 'decoder_layer5_block2': <tf.Tensor: shape=(1, 16, 2, 100), dtype=float32, numpy=\n",
            "array([[[[0.00999896, 0.00997125, 0.009992  , ..., 0.0100026 ,\n",
            "          0.01000165, 0.00999942],\n",
            "         [0.01000808, 0.00998972, 0.01000934, ..., 0.01000614,\n",
            "          0.01000634, 0.0100073 ]],\n",
            "\n",
            "        [[0.01000816, 0.00998957, 0.01000508, ..., 0.0100076 ,\n",
            "          0.01000776, 0.0100082 ],\n",
            "         [0.01000702, 0.00998401, 0.01001028, ..., 0.01000363,\n",
            "          0.01000483, 0.01000627]],\n",
            "\n",
            "        [[0.01000788, 0.00997469, 0.01000896, ..., 0.01000536,\n",
            "          0.01000457, 0.01000506],\n",
            "         [0.01001476, 0.00996747, 0.01002674, ..., 0.01000526,\n",
            "          0.01000492, 0.0100082 ]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.01000433, 0.01000137, 0.01000516, ..., 0.01000205,\n",
            "          0.01000272, 0.01000343],\n",
            "         [0.01002126, 0.00999087, 0.01003011, ..., 0.01001245,\n",
            "          0.01001409, 0.01001711]],\n",
            "\n",
            "        [[0.0100138 , 0.00999435, 0.0100142 , ..., 0.01000969,\n",
            "          0.01001074, 0.01001294],\n",
            "         [0.01001849, 0.0099675 , 0.01002337, ..., 0.01001237,\n",
            "          0.010014  , 0.0100167 ]],\n",
            "\n",
            "        [[0.00999224, 0.01001856, 0.00999404, ..., 0.00999301,\n",
            "          0.00999292, 0.00999275],\n",
            "         [0.00999864, 0.00998955, 0.01000238, ..., 0.0099966 ,\n",
            "          0.0099972 , 0.00999812]]]], dtype=float32)>, 'decoder_layer6_block1': <tf.Tensor: shape=(1, 16, 2, 2), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        ],\n",
            "         [0.8061483 , 0.19385175]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8735056 , 0.12649435]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7590768 , 0.24092329]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.5685928 , 0.4314072 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.85952395, 0.14047611]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7863569 , 0.21364309]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.4947481 , 0.50525194]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.31241003, 0.68759   ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.5992188 , 0.40078124]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7218261 , 0.27817392]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7379453 , 0.2620547 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.7625429 , 0.23745713]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.8449122 , 0.15508784]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.9038315 , 0.0961686 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.40116623, 0.5988338 ]],\n",
            "\n",
            "        [[1.        , 0.        ],\n",
            "         [0.62768734, 0.3723127 ]]]], dtype=float32)>, 'decoder_layer6_block2': <tf.Tensor: shape=(1, 16, 2, 100), dtype=float32, numpy=\n",
            "array([[[[0.0100059 , 0.0099925 , 0.01000807, ..., 0.01000242,\n",
            "          0.01000301, 0.01000433],\n",
            "         [0.01001226, 0.0099944 , 0.01001918, ..., 0.0100054 ,\n",
            "          0.01000775, 0.01001142]],\n",
            "\n",
            "        [[0.0099975 , 0.0099937 , 0.00999591, ..., 0.00999878,\n",
            "          0.00999851, 0.00999772],\n",
            "         [0.01000239, 0.01000199, 0.01000528, ..., 0.01000022,\n",
            "          0.01000051, 0.01000128]],\n",
            "\n",
            "        [[0.00999724, 0.01001592, 0.00999644, ..., 0.00999907,\n",
            "          0.0099979 , 0.00999655],\n",
            "         [0.00999813, 0.01002301, 0.01000585, ..., 0.00999568,\n",
            "          0.0099958 , 0.00999633]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00999579, 0.00999288, 0.00999001, ..., 0.00999843,\n",
            "          0.00999822, 0.00999756],\n",
            "         [0.00999958, 0.00998674, 0.00999475, ..., 0.0100017 ,\n",
            "          0.01000173, 0.01000141]],\n",
            "\n",
            "        [[0.01000225, 0.00997873, 0.01000198, ..., 0.01000118,\n",
            "          0.01000141, 0.01000153],\n",
            "         [0.01000045, 0.00997823, 0.01000121, ..., 0.00999955,\n",
            "          0.00999933, 0.00999909]],\n",
            "\n",
            "        [[0.00999966, 0.00999676, 0.00999499, ..., 0.01000232,\n",
            "          0.0100014 , 0.01000036],\n",
            "         [0.01000672, 0.01002242, 0.0100063 , ..., 0.0100044 ,\n",
            "          0.01000502, 0.01000693]]]], dtype=float32)>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JhTaCdZgMH9g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "571c9ad0-7107-414c-821b-f36b9e44af2f"
      },
      "source": [
        "sentence = \"<start> i swear to god , i am exceedingly tired . <end>\"\n",
        "print(evaluate(sentence)[0])"
      ],
      "id": "JhTaCdZgMH9g",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(['<start> now , by two headed monster , nature makes me see a dream . <end>'], {'decoder_layer1_block1': <tf.Tensor: shape=(1, 16, 15, 15), dtype=float32, numpy=\n",
            "array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.94965982e-01, 2.05034092e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.39460146e-01, 1.65174752e-01, 1.95365071e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.34312332e-01, 3.97762284e-02, 1.00210041e-01, ...,\n",
            "          1.06576964e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.62551343e-01, 2.29487475e-02, 4.64060530e-02, ...,\n",
            "          2.90424973e-01, 9.37284902e-03, 0.00000000e+00],\n",
            "         [9.18574691e-01, 4.28809552e-04, 1.82312615e-02, ...,\n",
            "          7.86131714e-03, 4.35665879e-06, 1.18891693e-11]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.93759859e-01, 3.06240112e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.92722362e-01, 2.26132214e-01, 2.81145364e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.49796844e-01, 1.78174973e-02, 3.57094146e-02, ...,\n",
            "          2.28567094e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.91385004e-02, 9.45729204e-03, 1.47550087e-02, ...,\n",
            "          4.73223954e-01, 1.70873851e-02, 0.00000000e+00],\n",
            "         [6.16265357e-01, 6.61633021e-05, 4.79802163e-03, ...,\n",
            "          1.19417794e-02, 2.32950882e-07, 1.87711943e-10]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.74267757e-01, 1.25732198e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.98774433e-01, 1.81640774e-01, 2.19584808e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [7.95518830e-02, 3.51415388e-02, 4.24577035e-02, ...,\n",
            "          1.53384387e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.13953052e-02, 1.82688900e-03, 3.68316914e-03, ...,\n",
            "          2.36881580e-02, 1.25329541e-02, 0.00000000e+00],\n",
            "         [8.06545734e-01, 1.41698096e-04, 3.45389964e-03, ...,\n",
            "          6.02559187e-02, 3.96469586e-05, 2.77275976e-05]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.85077190e-01, 3.14922810e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.16648179e-01, 1.91714972e-01, 3.91636848e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [8.21076110e-02, 5.66920266e-02, 5.27548715e-02, ...,\n",
            "          4.19174321e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.06283177e-02, 1.32193780e-02, 2.59369258e-02, ...,\n",
            "          3.55651155e-02, 1.51331602e-02, 0.00000000e+00],\n",
            "         [4.82887417e-01, 2.29415346e-05, 3.46577205e-02, ...,\n",
            "          3.69264418e-03, 6.49585912e-04, 1.58668859e-15]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.12133992e-01, 8.78660604e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [3.55290174e-01, 3.45610619e-01, 2.99099177e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [4.98963222e-02, 3.01050171e-02, 2.32468359e-02, ...,\n",
            "          8.68292972e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.53921473e-02, 1.43679120e-02, 1.29703293e-02, ...,\n",
            "          5.94515949e-02, 6.65076897e-02, 0.00000000e+00],\n",
            "         [9.37755883e-01, 1.31566658e-05, 1.92825188e-04, ...,\n",
            "          1.58154187e-04, 3.53140628e-08, 7.52302138e-15]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.89765060e-01, 2.10234925e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.94259566e-01, 2.96210885e-01, 2.09529564e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [7.20794126e-02, 5.63427843e-02, 6.35310411e-02, ...,\n",
            "          8.60653073e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.17261881e-02, 2.51683947e-02, 3.95020433e-02, ...,\n",
            "          1.74653992e-01, 1.93947759e-02, 0.00000000e+00],\n",
            "         [2.98817039e-01, 2.21694412e-04, 2.04112963e-03, ...,\n",
            "          1.26671925e-01, 2.68864911e-04, 5.18327214e-10]]]],\n",
            "      dtype=float32)>, 'decoder_layer1_block2': <tf.Tensor: shape=(1, 16, 15, 100), dtype=float32, numpy=\n",
            "array([[[[0.00999475, 0.01000166, 0.0100303 , ..., 0.00999537,\n",
            "          0.00999553, 0.00999605],\n",
            "         [0.00998479, 0.01005527, 0.01003006, ..., 0.00999104,\n",
            "          0.00999026, 0.00998936],\n",
            "         [0.00998111, 0.01005854, 0.0100786 , ..., 0.00998488,\n",
            "          0.00998517, 0.00998492],\n",
            "         ...,\n",
            "         [0.00999132, 0.01002614, 0.01004089, ..., 0.00999256,\n",
            "          0.00999265, 0.00999246],\n",
            "         [0.00999567, 0.01000177, 0.01001426, ..., 0.00999521,\n",
            "          0.0099946 , 0.00999404],\n",
            "         [0.0099967 , 0.00999842, 0.00998468, ..., 0.00999922,\n",
            "          0.00999873, 0.0099984 ]],\n",
            "\n",
            "        [[0.00998996, 0.0100534 , 0.01001166, ..., 0.00999475,\n",
            "          0.00999439, 0.00999317],\n",
            "         [0.0100169 , 0.00996538, 0.00990201, ..., 0.01001885,\n",
            "          0.01001862, 0.0100188 ],\n",
            "         [0.0100034 , 0.01003017, 0.00996819, ..., 0.01000609,\n",
            "          0.01000521, 0.01000427],\n",
            "         ...,\n",
            "         [0.00999537, 0.01004472, 0.01000995, ..., 0.00999764,\n",
            "          0.0099972 , 0.00999726],\n",
            "         [0.00999765, 0.01002369, 0.00999752, ..., 0.01000004,\n",
            "          0.00999964, 0.00999921],\n",
            "         [0.01000907, 0.00995291, 0.00995036, ..., 0.01001128,\n",
            "          0.01001213, 0.01001327]],\n",
            "\n",
            "        [[0.0100073 , 0.00998316, 0.00996342, ..., 0.01000624,\n",
            "          0.01000614, 0.0100063 ],\n",
            "         [0.00997218, 0.01011398, 0.01016214, ..., 0.00997663,\n",
            "          0.009977  , 0.00997698],\n",
            "         [0.01000644, 0.00997102, 0.00997262, ..., 0.01000437,\n",
            "          0.01000436, 0.01000454],\n",
            "         ...,\n",
            "         [0.01000624, 0.01000657, 0.00995277, ..., 0.01000553,\n",
            "          0.01000484, 0.01000417],\n",
            "         [0.00999172, 0.00995251, 0.01002781, ..., 0.00999075,\n",
            "          0.00999117, 0.00999226],\n",
            "         [0.00997383, 0.01006296, 0.01007595, ..., 0.00997962,\n",
            "          0.00997911, 0.00997843]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00999239, 0.01005089, 0.01004062, ..., 0.00999396,\n",
            "          0.00999395, 0.00999368],\n",
            "         [0.00999046, 0.01007279, 0.01004374, ..., 0.00999097,\n",
            "          0.00999056, 0.00999039],\n",
            "         [0.009994  , 0.01005061, 0.01000842, ..., 0.00999469,\n",
            "          0.00999425, 0.00999359],\n",
            "         ...,\n",
            "         [0.00998897, 0.01004179, 0.0100673 , ..., 0.00998976,\n",
            "          0.00998992, 0.00998997],\n",
            "         [0.00998023, 0.0100593 , 0.01009692, ..., 0.00998224,\n",
            "          0.0099826 , 0.00998216],\n",
            "         [0.00996471, 0.01012929, 0.0101374 , ..., 0.00996794,\n",
            "          0.00996762, 0.00996627]],\n",
            "\n",
            "        [[0.01000049, 0.009985  , 0.01000532, ..., 0.01000037,\n",
            "          0.0100006 , 0.01000104],\n",
            "         [0.01000631, 0.0100345 , 0.00995407, ..., 0.01000808,\n",
            "          0.01000693, 0.01000587],\n",
            "         [0.01001115, 0.00999526, 0.00994562, ..., 0.01001204,\n",
            "          0.01001125, 0.0100111 ],\n",
            "         ...,\n",
            "         [0.00998808, 0.0099924 , 0.0100618 , ..., 0.00998795,\n",
            "          0.00998872, 0.00998934],\n",
            "         [0.00998226, 0.00998964, 0.01007576, ..., 0.0099844 ,\n",
            "          0.00998587, 0.00998659],\n",
            "         [0.00999147, 0.00994533, 0.01002555, ..., 0.00998954,\n",
            "          0.00998971, 0.00999004]],\n",
            "\n",
            "        [[0.01001416, 0.00994228, 0.00993235, ..., 0.01001319,\n",
            "          0.01001303, 0.01001327],\n",
            "         [0.01001844, 0.00986938, 0.00990138, ..., 0.01001701,\n",
            "          0.01001774, 0.01001779],\n",
            "         [0.0100055 , 0.00996738, 0.00996645, ..., 0.01000705,\n",
            "          0.01000727, 0.0100076 ],\n",
            "         ...,\n",
            "         [0.01001466, 0.00992776, 0.0099229 , ..., 0.01000967,\n",
            "          0.01000982, 0.01001016],\n",
            "         [0.01001638, 0.0099178 , 0.00991548, ..., 0.01001507,\n",
            "          0.01001503, 0.0100159 ],\n",
            "         [0.00997376, 0.01006383, 0.01008949, ..., 0.00997964,\n",
            "          0.00998015, 0.00997924]]]], dtype=float32)>, 'decoder_layer2_block1': <tf.Tensor: shape=(1, 16, 15, 15), dtype=float32, numpy=\n",
            "array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.76547456e-01, 1.23452596e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.13957369e-01, 7.26177245e-02, 2.13424891e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.94146127e-01, 1.76983420e-02, 3.54179554e-02, ...,\n",
            "          1.36891631e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.78188145e-01, 3.80309694e-03, 5.42703122e-02, ...,\n",
            "          1.19390646e-02, 1.55370040e-02, 0.00000000e+00],\n",
            "         [5.97325981e-01, 2.99545820e-03, 8.06601048e-02, ...,\n",
            "          1.48445303e-02, 2.40366738e-02, 4.85379733e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.96963322e-01, 3.03660915e-03, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.27295077e-01, 1.37203678e-01, 2.35501260e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.17357813e-01, 8.81708134e-03, 7.30447173e-02, ...,\n",
            "          7.53414631e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.60248250e-01, 4.22279239e-02, 8.04238245e-02, ...,\n",
            "          4.89747152e-02, 2.30792426e-02, 0.00000000e+00],\n",
            "         [8.92418981e-01, 3.07336799e-04, 2.96648331e-02, ...,\n",
            "          4.92528547e-03, 1.85761461e-03, 2.49951482e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.66280639e-01, 3.37193795e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.98403668e-01, 2.62308478e-01, 2.39287809e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.22724697e-01, 2.26294994e-01, 9.78219360e-02, ...,\n",
            "          4.10677120e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.11700617e-01, 2.82931589e-02, 1.10718623e-01, ...,\n",
            "          4.21815626e-02, 4.33566831e-02, 0.00000000e+00],\n",
            "         [6.68701768e-01, 5.20065520e-03, 3.85517776e-02, ...,\n",
            "          1.76346414e-02, 6.78644031e-02, 8.01235363e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.78158963e-01, 2.21841067e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.91676199e-01, 9.97526571e-02, 2.08571211e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [8.58437121e-02, 1.20513998e-02, 4.76844832e-02, ...,\n",
            "          5.25063127e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.19616309e-01, 2.76006684e-02, 8.77024308e-02, ...,\n",
            "          5.91266528e-02, 9.30963010e-02, 0.00000000e+00],\n",
            "         [5.27879357e-01, 1.32631101e-02, 9.32872966e-02, ...,\n",
            "          2.74736453e-02, 4.84555922e-02, 2.82909088e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.47854257e-01, 1.52145773e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.20718819e-01, 2.84477949e-01, 2.94803262e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.18212447e-01, 6.39054272e-03, 3.24761383e-02, ...,\n",
            "          7.41207451e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.09061730e-02, 6.74416358e-03, 3.17592211e-02, ...,\n",
            "          9.48402882e-02, 2.06051931e-01, 0.00000000e+00],\n",
            "         [6.46317542e-01, 7.78080663e-03, 1.04495466e-01, ...,\n",
            "          3.34895588e-02, 1.38734784e-02, 3.23801078e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.19513655e-01, 2.80486315e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.11038291e-01, 7.42711052e-02, 2.14690581e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.54817790e-01, 3.68117578e-02, 7.78770447e-02, ...,\n",
            "          4.10136096e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.58589387e-02, 1.38533756e-03, 4.64201486e-03, ...,\n",
            "          1.73819754e-02, 6.63729431e-03, 0.00000000e+00],\n",
            "         [8.65710437e-01, 7.58357020e-03, 5.49281090e-02, ...,\n",
            "          2.02557426e-02, 2.52133887e-03, 7.16284430e-03]]]],\n",
            "      dtype=float32)>, 'decoder_layer2_block2': <tf.Tensor: shape=(1, 16, 15, 100), dtype=float32, numpy=\n",
            "array([[[[0.00999117, 0.01001646, 0.01003789, ..., 0.00999096,\n",
            "          0.00999104, 0.00999025],\n",
            "         [0.0100001 , 0.0100052 , 0.00997283, ..., 0.01000043,\n",
            "          0.00999987, 0.00999904],\n",
            "         [0.00997088, 0.01005281, 0.01014237, ..., 0.00997235,\n",
            "          0.0099738 , 0.00997325],\n",
            "         ...,\n",
            "         [0.00999572, 0.01004362, 0.01001474, ..., 0.00999465,\n",
            "          0.00999263, 0.00999129],\n",
            "         [0.00997242, 0.01006467, 0.01010541, ..., 0.00997963,\n",
            "          0.00998085, 0.00998091],\n",
            "         [0.00995927, 0.01012699, 0.01017199, ..., 0.00996545,\n",
            "          0.00996519, 0.00996378]],\n",
            "\n",
            "        [[0.01000895, 0.00995017, 0.0100076 , ..., 0.01000755,\n",
            "          0.01000851, 0.01000963],\n",
            "         [0.00999155, 0.00998218, 0.01002997, ..., 0.00999257,\n",
            "          0.00999294, 0.00999289],\n",
            "         [0.01000946, 0.00993242, 0.00997234, ..., 0.01001055,\n",
            "          0.01001179, 0.01001311],\n",
            "         ...,\n",
            "         [0.01001132, 0.00998864, 0.00997555, ..., 0.01001145,\n",
            "          0.01001109, 0.01001135],\n",
            "         [0.00998068, 0.0100761 , 0.01010666, ..., 0.00998442,\n",
            "          0.00998541, 0.00998568],\n",
            "         [0.00997873, 0.01011763, 0.01007875, ..., 0.00998368,\n",
            "          0.00998303, 0.00998182]],\n",
            "\n",
            "        [[0.01001697, 0.00995113, 0.00991633, ..., 0.01001699,\n",
            "          0.01001695, 0.01001788],\n",
            "         [0.01002225, 0.00988017, 0.00987181, ..., 0.01002195,\n",
            "          0.01002269, 0.01002487],\n",
            "         [0.00999465, 0.01000866, 0.01002759, ..., 0.00999321,\n",
            "          0.00999331, 0.00999417],\n",
            "         ...,\n",
            "         [0.01000959, 0.01001181, 0.00995762, ..., 0.01000967,\n",
            "          0.01000955, 0.0100096 ],\n",
            "         [0.00999095, 0.01002033, 0.01003697, ..., 0.00999036,\n",
            "          0.00999004, 0.00998944],\n",
            "         [0.0100138 , 0.00991769, 0.00990602, ..., 0.01001255,\n",
            "          0.01001268, 0.01001188]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00994443, 0.01014218, 0.01021329, ..., 0.00995484,\n",
            "          0.0099553 , 0.00995461],\n",
            "         [0.00994118, 0.01018029, 0.01023363, ..., 0.00995173,\n",
            "          0.00995173, 0.00995066],\n",
            "         [0.00995625, 0.01009912, 0.01015611, ..., 0.0099657 ,\n",
            "          0.00996628, 0.0099661 ],\n",
            "         ...,\n",
            "         [0.00998388, 0.01006596, 0.0100803 , ..., 0.00998682,\n",
            "          0.00998678, 0.00998617],\n",
            "         [0.00998851, 0.00999137, 0.01003723, ..., 0.00998955,\n",
            "          0.00998954, 0.00998945],\n",
            "         [0.00997803, 0.01010749, 0.01010146, ..., 0.00998127,\n",
            "          0.00998064, 0.00997937]],\n",
            "\n",
            "        [[0.00997412, 0.01004622, 0.01015563, ..., 0.00997636,\n",
            "          0.00997729, 0.00997627],\n",
            "         [0.00999792, 0.00996903, 0.01001911, ..., 0.00999857,\n",
            "          0.00999952, 0.01000079],\n",
            "         [0.00997478, 0.01004201, 0.01011   , ..., 0.00997817,\n",
            "          0.00997839, 0.00997739],\n",
            "         ...,\n",
            "         [0.00998215, 0.01005641, 0.01010559, ..., 0.00998392,\n",
            "          0.00998423, 0.0099836 ],\n",
            "         [0.00998543, 0.0100488 , 0.01006248, ..., 0.00998915,\n",
            "          0.00998938, 0.00998878],\n",
            "         [0.00999825, 0.01000528, 0.01000626, ..., 0.0099992 ,\n",
            "          0.00999954, 0.00999993]],\n",
            "\n",
            "        [[0.00995786, 0.01015636, 0.01018157, ..., 0.00996352,\n",
            "          0.00996312, 0.00996187],\n",
            "         [0.0100008 , 0.00997397, 0.00996129, ..., 0.01000076,\n",
            "          0.01000069, 0.00999998],\n",
            "         [0.00999778, 0.00998132, 0.00998652, ..., 0.00999745,\n",
            "          0.00999699, 0.00999654],\n",
            "         ...,\n",
            "         [0.00995819, 0.0100968 , 0.01017253, ..., 0.00996263,\n",
            "          0.00996245, 0.00996201],\n",
            "         [0.01002365, 0.00990902, 0.0099024 , ..., 0.01001974,\n",
            "          0.0100197 , 0.01002011],\n",
            "         [0.01000967, 0.00990635, 0.00993664, ..., 0.01000721,\n",
            "          0.0100079 , 0.01000889]]]], dtype=float32)>, 'decoder_layer3_block1': <tf.Tensor: shape=(1, 16, 15, 15), dtype=float32, numpy=\n",
            "array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.07503521e-01, 1.92496493e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.10413194e-01, 1.61020115e-01, 2.28566706e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.44568160e-01, 3.96849699e-02, 3.00375037e-02, ...,\n",
            "          4.91925329e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.85201597e-02, 4.14559543e-02, 3.50676700e-02, ...,\n",
            "          9.69938561e-02, 8.27239603e-02, 0.00000000e+00],\n",
            "         [6.13904595e-01, 1.03045991e-02, 5.03689349e-02, ...,\n",
            "          4.30134460e-02, 1.77865066e-02, 8.00413042e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.81429398e-01, 1.85705833e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.85125995e-01, 1.75975785e-02, 9.72764343e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [8.62068404e-03, 1.30059907e-05, 1.62144221e-04, ...,\n",
            "          2.00031302e-03, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.73514867e-01, 8.82840529e-03, 1.07096881e-02, ...,\n",
            "          2.40483545e-02, 4.35607396e-02, 0.00000000e+00],\n",
            "         [8.85817945e-01, 9.32132825e-03, 7.55472295e-03, ...,\n",
            "          2.03367136e-03, 9.75525659e-03, 1.89557541e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.80384290e-01, 1.96156949e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.83544028e-01, 5.01953885e-02, 6.62606061e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [8.03117305e-02, 2.19029328e-03, 2.57873209e-03, ...,\n",
            "          1.34994937e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.03488201e-01, 1.72879882e-02, 1.40527561e-02, ...,\n",
            "          2.68538352e-02, 5.28872907e-02, 0.00000000e+00],\n",
            "         [8.47324312e-01, 1.02518518e-02, 7.08619785e-03, ...,\n",
            "          1.21714901e-02, 1.33136706e-02, 4.86103743e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.28419888e-01, 7.15801418e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.35224354e-01, 3.62852588e-02, 2.84904148e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [9.89439115e-02, 3.26720141e-02, 1.95268579e-02, ...,\n",
            "          1.12461992e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.90854287e-01, 1.16732875e-02, 1.22305555e-02, ...,\n",
            "          6.92942878e-03, 4.49252017e-02, 0.00000000e+00],\n",
            "         [7.82997727e-01, 1.69158913e-02, 2.30501145e-02, ...,\n",
            "          3.21808159e-02, 1.07483016e-02, 1.79762468e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.19095755e-01, 8.09042975e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.11354399e-01, 9.43820551e-02, 9.42634940e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [4.83062454e-02, 4.98049753e-03, 7.02186255e-03, ...,\n",
            "          3.27289314e-03, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.41328776e-01, 1.50603885e-02, 1.08940946e-02, ...,\n",
            "          1.51826199e-02, 2.87246238e-02, 0.00000000e+00],\n",
            "         [8.52092266e-01, 5.40486537e-03, 4.22856165e-03, ...,\n",
            "          8.00667331e-03, 5.97141730e-03, 7.60316849e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.05089259e-01, 9.49107632e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.46856201e-01, 1.23169404e-02, 4.08268571e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [4.71243262e-01, 8.56468827e-03, 2.87831314e-02, ...,\n",
            "          4.48943190e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.41129613e-01, 2.20921971e-02, 1.77379902e-02, ...,\n",
            "          6.21993765e-02, 3.37663591e-02, 0.00000000e+00],\n",
            "         [7.05303133e-01, 2.77807210e-02, 3.98466028e-02, ...,\n",
            "          7.32005313e-02, 1.32785374e-02, 1.58065986e-02]]]],\n",
            "      dtype=float32)>, 'decoder_layer3_block2': <tf.Tensor: shape=(1, 16, 15, 100), dtype=float32, numpy=\n",
            "array([[[[0.01002453, 0.00987714, 0.00989765, ..., 0.01002004,\n",
            "          0.01002086, 0.01002206],\n",
            "         [0.01004293, 0.00983155, 0.00979741, ..., 0.01003655,\n",
            "          0.01003649, 0.01003737],\n",
            "         [0.01001634, 0.0099023 , 0.00993897, ..., 0.01001291,\n",
            "          0.01001407, 0.01001556],\n",
            "         ...,\n",
            "         [0.01000503, 0.00998687, 0.00996641, ..., 0.01000523,\n",
            "          0.01000507, 0.0100048 ],\n",
            "         [0.01000279, 0.01001217, 0.00997887, ..., 0.01000161,\n",
            "          0.01000119, 0.01000092],\n",
            "         [0.00998525, 0.01007035, 0.01002946, ..., 0.00999077,\n",
            "          0.00998895, 0.00998726]],\n",
            "\n",
            "        [[0.009989  , 0.01001643, 0.01007925, ..., 0.00998895,\n",
            "          0.00998981, 0.00999021],\n",
            "         [0.00998202, 0.01003083, 0.01007413, ..., 0.00997899,\n",
            "          0.00997862, 0.00997807],\n",
            "         [0.00998569, 0.01000826, 0.01005398, ..., 0.00998336,\n",
            "          0.00998324, 0.00998331],\n",
            "         ...,\n",
            "         [0.00997956, 0.01003731, 0.01008291, ..., 0.00997854,\n",
            "          0.00997819, 0.00997709],\n",
            "         [0.00997824, 0.00999984, 0.01006834, ..., 0.00997841,\n",
            "          0.00997874, 0.00997828],\n",
            "         [0.00998719, 0.01005797, 0.01005687, ..., 0.00999004,\n",
            "          0.00999033, 0.00998973]],\n",
            "\n",
            "        [[0.01001745, 0.00988317, 0.00987716, ..., 0.01001507,\n",
            "          0.01001578, 0.01001695],\n",
            "         [0.00999819, 0.01001762, 0.01001318, ..., 0.00999925,\n",
            "          0.00999988, 0.01000054],\n",
            "         [0.01001642, 0.00994968, 0.0099377 , ..., 0.01001499,\n",
            "          0.010016  , 0.01001712],\n",
            "         ...,\n",
            "         [0.01002166, 0.00992575, 0.0098979 , ..., 0.01002009,\n",
            "          0.01002035, 0.01002124],\n",
            "         [0.01001025, 0.0099608 , 0.00992697, ..., 0.01001004,\n",
            "          0.01000993, 0.01001011],\n",
            "         [0.01002465, 0.00985225, 0.0098356 , ..., 0.01002211,\n",
            "          0.01002179, 0.01002142]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00999812, 0.01000264, 0.01001568, ..., 0.00999799,\n",
            "          0.00999838, 0.00999843],\n",
            "         [0.00999658, 0.01000212, 0.0100199 , ..., 0.00999852,\n",
            "          0.00999856, 0.00999822],\n",
            "         [0.0100061 , 0.00998718, 0.00996811, ..., 0.01000643,\n",
            "          0.01000641, 0.01000637],\n",
            "         ...,\n",
            "         [0.01001418, 0.00991504, 0.00989234, ..., 0.01001432,\n",
            "          0.01001403, 0.0100142 ],\n",
            "         [0.01001354, 0.0099655 , 0.00993847, ..., 0.01001189,\n",
            "          0.01001117, 0.01001089],\n",
            "         [0.01001114, 0.00993293, 0.00993233, ..., 0.01000918,\n",
            "          0.01000836, 0.01000842]],\n",
            "\n",
            "        [[0.01000806, 0.00997992, 0.00995245, ..., 0.01000729,\n",
            "          0.01000678, 0.0100066 ],\n",
            "         [0.00999492, 0.00999285, 0.01001117, ..., 0.00999717,\n",
            "          0.00999767, 0.00999803],\n",
            "         [0.00997642, 0.01006261, 0.01009364, ..., 0.00998084,\n",
            "          0.00998071, 0.00998033],\n",
            "         ...,\n",
            "         [0.00999278, 0.01004506, 0.01002665, ..., 0.00999487,\n",
            "          0.00999411, 0.00999342],\n",
            "         [0.01000011, 0.01000223, 0.0100033 , ..., 0.01000061,\n",
            "          0.01000082, 0.01000113],\n",
            "         [0.01002566, 0.00996214, 0.00988653, ..., 0.01002485,\n",
            "          0.0100245 , 0.01002484]],\n",
            "\n",
            "        [[0.01000144, 0.00999943, 0.01000583, ..., 0.01000078,\n",
            "          0.01000094, 0.01000088],\n",
            "         [0.01000953, 0.00992619, 0.00990799, ..., 0.01000665,\n",
            "          0.01000547, 0.01000517],\n",
            "         [0.0099587 , 0.01009415, 0.01019615, ..., 0.00996264,\n",
            "          0.00996368, 0.00996372],\n",
            "         ...,\n",
            "         [0.00999794, 0.00999459, 0.01000507, ..., 0.00999694,\n",
            "          0.00999668, 0.00999668],\n",
            "         [0.0099944 , 0.009987  , 0.00998413, ..., 0.00999427,\n",
            "          0.00999345, 0.00999309],\n",
            "         [0.01002804, 0.0098861 , 0.00982223, ..., 0.01002515,\n",
            "          0.0100237 , 0.01002389]]]], dtype=float32)>, 'decoder_layer4_block1': <tf.Tensor: shape=(1, 16, 15, 15), dtype=float32, numpy=\n",
            "array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.96725810e-01, 2.03274220e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.65767813e-01, 1.08814776e-01, 1.25417396e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.42555330e-02, 3.66747146e-04, 2.89909705e-03, ...,\n",
            "          6.47580624e-03, 0.00000000e+00, 0.00000000e+00],\n",
            "         [3.05255532e-01, 4.41993549e-02, 2.78858934e-02, ...,\n",
            "          5.20016141e-02, 2.32639611e-02, 0.00000000e+00],\n",
            "         [5.26948869e-01, 9.97059699e-03, 1.10944677e-02, ...,\n",
            "          2.49941982e-02, 1.41767383e-01, 5.26625775e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.44926858e-01, 5.50731942e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.66434431e-01, 6.52464151e-01, 8.11014473e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [7.75279570e-03, 2.16716826e-02, 2.00749119e-03, ...,\n",
            "          1.22117344e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.70965844e-01, 7.52327368e-02, 5.03133163e-02, ...,\n",
            "          4.38833944e-02, 8.60978756e-03, 0.00000000e+00],\n",
            "         [5.95820725e-01, 1.83244294e-03, 2.26146206e-02, ...,\n",
            "          1.01928189e-02, 1.92767736e-02, 1.26581326e-01]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.67989981e-01, 3.20100412e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.94556153e-01, 1.84090987e-01, 2.21352950e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [2.40084324e-02, 4.81684841e-02, 1.48368049e-02, ...,\n",
            "          2.11701334e-01, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.11873397e-01, 3.43178242e-01, 1.89366825e-02, ...,\n",
            "          1.03493974e-01, 3.07889264e-02, 0.00000000e+00],\n",
            "         [7.44126856e-01, 5.23130894e-02, 1.35812219e-02, ...,\n",
            "          4.30029258e-02, 1.20825926e-02, 3.84968109e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.66100931e-01, 3.38990241e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.20934129e-01, 1.11107238e-01, 2.67958641e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.35278150e-01, 2.52989065e-02, 3.68266739e-02, ...,\n",
            "          3.13521139e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.07573017e-01, 9.46532935e-02, 1.15355002e-02, ...,\n",
            "          3.95632535e-02, 1.03846803e-01, 0.00000000e+00],\n",
            "         [4.59208518e-01, 5.94761185e-02, 3.03894021e-02, ...,\n",
            "          4.90591154e-02, 1.04654282e-01, 4.15051207e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.09014523e-01, 9.09854919e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.61060691e-01, 8.08456764e-02, 2.58093625e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [8.26017261e-02, 3.64216045e-02, 4.37639877e-02, ...,\n",
            "          6.65787831e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.17556468e-01, 6.99903816e-02, 4.44717854e-02, ...,\n",
            "          8.73019993e-02, 7.57277086e-02, 0.00000000e+00],\n",
            "         [6.17826581e-01, 1.28441006e-02, 2.21447255e-02, ...,\n",
            "          8.20380356e-03, 1.01917954e-02, 3.12084649e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.79680133e-01, 2.20319927e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.05099475e-01, 2.61570483e-01, 2.33330041e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.52676240e-01, 8.97841007e-02, 1.41835228e-01, ...,\n",
            "          3.59730534e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.27322090e-01, 2.98474990e-02, 5.99590503e-02, ...,\n",
            "          4.90481146e-02, 1.18656950e-02, 0.00000000e+00],\n",
            "         [5.42507291e-01, 2.29196828e-02, 2.81529408e-02, ...,\n",
            "          9.35019478e-02, 7.29251653e-03, 5.64404279e-02]]]],\n",
            "      dtype=float32)>, 'decoder_layer4_block2': <tf.Tensor: shape=(1, 16, 15, 100), dtype=float32, numpy=\n",
            "array([[[[0.01001333, 0.00994628, 0.00994485, ..., 0.01001085,\n",
            "          0.01001098, 0.01001145],\n",
            "         [0.01000297, 0.01000046, 0.01000743, ..., 0.00999937,\n",
            "          0.00999966, 0.00999965],\n",
            "         [0.00997238, 0.01010972, 0.01017385, ..., 0.00997284,\n",
            "          0.0099731 , 0.00997269],\n",
            "         ...,\n",
            "         [0.01001133, 0.00993097, 0.00992002, ..., 0.01000918,\n",
            "          0.01000895, 0.01000904],\n",
            "         [0.0099761 , 0.01006369, 0.01008099, ..., 0.00998033,\n",
            "          0.00997964, 0.00997826],\n",
            "         [0.01000031, 0.00999668, 0.00995375, ..., 0.01000253,\n",
            "          0.01000164, 0.01000073]],\n",
            "\n",
            "        [[0.01001555, 0.0099531 , 0.00993857, ..., 0.01001195,\n",
            "          0.01001222, 0.01001304],\n",
            "         [0.01002048, 0.00990003, 0.00994012, ..., 0.01001803,\n",
            "          0.01001961, 0.01002174],\n",
            "         [0.01000408, 0.00999345, 0.00998867, ..., 0.01000275,\n",
            "          0.01000296, 0.01000409],\n",
            "         ...,\n",
            "         [0.00999071, 0.01000206, 0.01002785, ..., 0.00999226,\n",
            "          0.00999261, 0.00999232],\n",
            "         [0.00996379, 0.01012488, 0.01010972, ..., 0.00997036,\n",
            "          0.00996865, 0.00996594],\n",
            "         [0.0100034 , 0.00999096, 0.00990401, ..., 0.01000779,\n",
            "          0.01000654, 0.01000496]],\n",
            "\n",
            "        [[0.01001059, 0.00996304, 0.00995253, ..., 0.01001107,\n",
            "          0.01001119, 0.01001214],\n",
            "         [0.00999393, 0.00999648, 0.0100253 , ..., 0.00999454,\n",
            "          0.00999481, 0.00999531],\n",
            "         [0.01000553, 0.00998997, 0.00998624, ..., 0.01000558,\n",
            "          0.01000568, 0.01000638],\n",
            "         ...,\n",
            "         [0.01001631, 0.00992517, 0.00991994, ..., 0.0100147 ,\n",
            "          0.01001479, 0.0100153 ],\n",
            "         [0.01003098, 0.009893  , 0.00983357, ..., 0.01002979,\n",
            "          0.01002961, 0.01002972],\n",
            "         [0.01002424, 0.00991048, 0.00987677, ..., 0.01002143,\n",
            "          0.01002132, 0.01002117]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00999375, 0.01004465, 0.01005714, ..., 0.00999302,\n",
            "          0.00999264, 0.00999232],\n",
            "         [0.00994673, 0.01020674, 0.01026797, ..., 0.00995306,\n",
            "          0.00995327, 0.00995135],\n",
            "         [0.00999654, 0.0100451 , 0.010059  , ..., 0.00999466,\n",
            "          0.00999461, 0.00999425],\n",
            "         ...,\n",
            "         [0.01003583, 0.0099177 , 0.00984951, ..., 0.01003151,\n",
            "          0.0100304 , 0.01003011],\n",
            "         [0.00998326, 0.01008041, 0.01012093, ..., 0.00998261,\n",
            "          0.00998256, 0.00998123],\n",
            "         [0.00999921, 0.01000387, 0.01000979, ..., 0.00999774,\n",
            "          0.00999779, 0.00999731]],\n",
            "\n",
            "        [[0.00999684, 0.01002924, 0.01002728, ..., 0.00999741,\n",
            "          0.00999755, 0.00999731],\n",
            "         [0.00997875, 0.01006998, 0.01009674, ..., 0.00998277,\n",
            "          0.00998314, 0.00998374],\n",
            "         [0.00999698, 0.01002742, 0.00999318, ..., 0.00999803,\n",
            "          0.00999769, 0.0099968 ],\n",
            "         ...,\n",
            "         [0.01000675, 0.00998641, 0.00996237, ..., 0.01000755,\n",
            "          0.01000776, 0.01000793],\n",
            "         [0.01000436, 0.01000893, 0.00996219, ..., 0.01000917,\n",
            "          0.01000938, 0.01000927],\n",
            "         [0.01001621, 0.00991797, 0.00993066, ..., 0.01001346,\n",
            "          0.01001352, 0.01001455]],\n",
            "\n",
            "        [[0.01002635, 0.00990753, 0.00991866, ..., 0.01002123,\n",
            "          0.01002189, 0.01002376],\n",
            "         [0.01000575, 0.00996857, 0.01000268, ..., 0.01000663,\n",
            "          0.01000756, 0.01000924],\n",
            "         [0.01000303, 0.00998833, 0.0100116 , ..., 0.01000439,\n",
            "          0.01000507, 0.01000641],\n",
            "         ...,\n",
            "         [0.0100139 , 0.00995801, 0.00995376, ..., 0.0100117 ,\n",
            "          0.01001196, 0.01001294],\n",
            "         [0.0099986 , 0.00999509, 0.01000344, ..., 0.01000034,\n",
            "          0.01000083, 0.01000152],\n",
            "         [0.01000202, 0.0099784 , 0.00997943, ..., 0.01000157,\n",
            "          0.01000154, 0.01000158]]]], dtype=float32)>, 'decoder_layer5_block1': <tf.Tensor: shape=(1, 16, 15, 15), dtype=float32, numpy=\n",
            "array([[[[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9099929 , 0.09000715, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.45113945, 0.270977  , 0.2778835 , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.03983815, 0.0019885 , 0.02559105, ..., 0.02843247,\n",
            "          0.        , 0.        ],\n",
            "         [0.10301801, 0.03087242, 0.12777413, ..., 0.03449314,\n",
            "          0.0612081 , 0.        ],\n",
            "         [0.22088067, 0.05444228, 0.09204575, ..., 0.0341509 ,\n",
            "          0.02980967, 0.10234573]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.9839806 , 0.01601943, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.86044127, 0.07099294, 0.06856578, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.09629938, 0.07467452, 0.03477987, ..., 0.03070756,\n",
            "          0.        , 0.        ],\n",
            "         [0.23474406, 0.01734958, 0.06742431, ..., 0.16541162,\n",
            "          0.03559107, 0.        ],\n",
            "         [0.17032425, 0.02789809, 0.15989344, ..., 0.10594761,\n",
            "          0.03246716, 0.045279  ]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.5371175 , 0.46288255, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.45432827, 0.4480309 , 0.09764083, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.0315884 , 0.11300474, 0.01699374, ..., 0.02828022,\n",
            "          0.        , 0.        ],\n",
            "         [0.06819291, 0.02020227, 0.02572364, ..., 0.01281453,\n",
            "          0.10636023, 0.        ],\n",
            "         [0.20420137, 0.02381119, 0.10738964, ..., 0.02297724,\n",
            "          0.07068214, 0.00436893]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.6816601 , 0.31833988, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.24227138, 0.41344002, 0.34428856, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.02642821, 0.06293486, 0.01468843, ..., 0.08615254,\n",
            "          0.        , 0.        ],\n",
            "         [0.13296473, 0.10417049, 0.06168725, ..., 0.1720183 ,\n",
            "          0.01872333, 0.        ],\n",
            "         [0.1674921 , 0.02882813, 0.14464235, ..., 0.09376051,\n",
            "          0.02098018, 0.03993934]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.54877734, 0.45122272, 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.6842301 , 0.11029822, 0.20547166, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.13056175, 0.03164371, 0.06177317, ..., 0.04765811,\n",
            "          0.        , 0.        ],\n",
            "         [0.08948129, 0.03131156, 0.01007438, ..., 0.31236988,\n",
            "          0.04424591, 0.        ],\n",
            "         [0.26691192, 0.08609133, 0.13939373, ..., 0.04088569,\n",
            "          0.02292346, 0.04254615]],\n",
            "\n",
            "        [[1.        , 0.        , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.94943964, 0.0505603 , 0.        , ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         [0.71262634, 0.08525641, 0.20211725, ..., 0.        ,\n",
            "          0.        , 0.        ],\n",
            "         ...,\n",
            "         [0.31776264, 0.01488757, 0.04148784, ..., 0.22299486,\n",
            "          0.        , 0.        ],\n",
            "         [0.18506551, 0.11923357, 0.02111032, ..., 0.08151729,\n",
            "          0.00734869, 0.        ],\n",
            "         [0.17241292, 0.03802408, 0.04544566, ..., 0.18654366,\n",
            "          0.03267218, 0.0970546 ]]]], dtype=float32)>, 'decoder_layer5_block2': <tf.Tensor: shape=(1, 16, 15, 100), dtype=float32, numpy=\n",
            "array([[[[0.01002512, 0.00989474, 0.00987159, ..., 0.01002129,\n",
            "          0.01002153, 0.01002219],\n",
            "         [0.01000314, 0.01001527, 0.00999687, ..., 0.0100024 ,\n",
            "          0.01000198, 0.01000196],\n",
            "         [0.00999362, 0.01005264, 0.0100542 , ..., 0.0099939 ,\n",
            "          0.00999377, 0.00999359],\n",
            "         ...,\n",
            "         [0.00999296, 0.00998703, 0.0100106 , ..., 0.00999332,\n",
            "          0.00999317, 0.00999296],\n",
            "         [0.0099914 , 0.00996441, 0.01000066, ..., 0.00999165,\n",
            "          0.00999165, 0.00999146],\n",
            "         [0.00997362, 0.01004873, 0.01009279, ..., 0.00997727,\n",
            "          0.00997703, 0.00997633]],\n",
            "\n",
            "        [[0.01000553, 0.00994897, 0.00997701, ..., 0.01000668,\n",
            "          0.01000723, 0.01000868],\n",
            "         [0.00999468, 0.01002977, 0.01006555, ..., 0.00999536,\n",
            "          0.00999582, 0.00999605],\n",
            "         [0.00999022, 0.01003341, 0.01008807, ..., 0.0099911 ,\n",
            "          0.00999168, 0.00999262],\n",
            "         ...,\n",
            "         [0.00999415, 0.01000888, 0.01003456, ..., 0.00999682,\n",
            "          0.00999735, 0.00999724],\n",
            "         [0.01000994, 0.00995298, 0.00992613, ..., 0.01001071,\n",
            "          0.0100106 , 0.01001076],\n",
            "         [0.01000531, 0.00996597, 0.00996711, ..., 0.01000371,\n",
            "          0.01000369, 0.01000356]],\n",
            "\n",
            "        [[0.01000853, 0.00994499, 0.00994421, ..., 0.01000744,\n",
            "          0.01000749, 0.0100082 ],\n",
            "         [0.01000335, 0.00999245, 0.00997346, ..., 0.01000309,\n",
            "          0.01000298, 0.0100031 ],\n",
            "         [0.01001909, 0.00991508, 0.0098991 , ..., 0.01001772,\n",
            "          0.01001795, 0.0100188 ],\n",
            "         ...,\n",
            "         [0.00998782, 0.01001196, 0.01003431, ..., 0.00998942,\n",
            "          0.00999   , 0.00999055],\n",
            "         [0.00997605, 0.01007142, 0.01006941, ..., 0.00997785,\n",
            "          0.0099774 , 0.00997647],\n",
            "         [0.00999074, 0.01006805, 0.01002202, ..., 0.0099929 ,\n",
            "          0.00999222, 0.00999077]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00999925, 0.01001033, 0.01000683, ..., 0.01000142,\n",
            "          0.01000169, 0.01000174],\n",
            "         [0.00999984, 0.01001321, 0.00997645, ..., 0.01000248,\n",
            "          0.0100021 , 0.01000198],\n",
            "         [0.00998563, 0.01004812, 0.01004692, ..., 0.00998882,\n",
            "          0.00998872, 0.00998773],\n",
            "         ...,\n",
            "         [0.0100011 , 0.01000044, 0.00997028, ..., 0.01000261,\n",
            "          0.01000222, 0.01000187],\n",
            "         [0.01000412, 0.00998979, 0.00995624, ..., 0.01000368,\n",
            "          0.01000311, 0.01000299],\n",
            "         [0.01001368, 0.00992871, 0.00992449, ..., 0.0100115 ,\n",
            "          0.01001166, 0.01001213]],\n",
            "\n",
            "        [[0.00998937, 0.01006049, 0.01007953, ..., 0.00999089,\n",
            "          0.00999098, 0.0099912 ],\n",
            "         [0.0100044 , 0.00997159, 0.01001834, ..., 0.01000636,\n",
            "          0.01000831, 0.01001079],\n",
            "         [0.00999356, 0.01002912, 0.01008844, ..., 0.0099967 ,\n",
            "          0.00999849, 0.01000051],\n",
            "         ...,\n",
            "         [0.00999581, 0.0100261 , 0.01005661, ..., 0.00999756,\n",
            "          0.00999858, 0.00999967],\n",
            "         [0.00998719, 0.01005468, 0.01010189, ..., 0.0099889 ,\n",
            "          0.00998949, 0.00998976],\n",
            "         [0.01000326, 0.00998513, 0.00998942, ..., 0.01000311,\n",
            "          0.01000334, 0.01000302]],\n",
            "\n",
            "        [[0.01000494, 0.01001085, 0.01000417, ..., 0.0100019 ,\n",
            "          0.01000184, 0.01000201],\n",
            "         [0.01002886, 0.00989355, 0.00984861, ..., 0.01002555,\n",
            "          0.01002579, 0.01002638],\n",
            "         [0.01000504, 0.00995207, 0.0099868 , ..., 0.01000378,\n",
            "          0.01000459, 0.01000509],\n",
            "         ...,\n",
            "         [0.01002041, 0.00992785, 0.00988786, ..., 0.01001891,\n",
            "          0.01001905, 0.01001935],\n",
            "         [0.01000557, 0.00998981, 0.00995739, ..., 0.01000674,\n",
            "          0.01000674, 0.01000665],\n",
            "         [0.00999384, 0.01000508, 0.01001693, ..., 0.00999558,\n",
            "          0.00999578, 0.00999555]]]], dtype=float32)>, 'decoder_layer6_block1': <tf.Tensor: shape=(1, 16, 15, 15), dtype=float32, numpy=\n",
            "array([[[[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [8.21659386e-01, 1.78340584e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [3.10961902e-01, 5.58304727e-01, 1.30733430e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [6.37608208e-03, 1.89998355e-02, 2.10135872e-03, ...,\n",
            "          1.34474202e-03, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.73502311e-01, 6.67820945e-02, 1.16016559e-01, ...,\n",
            "          4.83966954e-02, 2.23948229e-02, 0.00000000e+00],\n",
            "         [2.54227035e-02, 1.06358379e-01, 6.26610592e-03, ...,\n",
            "          1.31813437e-02, 4.96616289e-02, 5.53942025e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.59007263e-01, 4.09927480e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.47292471e-01, 3.62907708e-01, 8.97998363e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [6.87214956e-02, 1.30316466e-02, 1.65178087e-02, ...,\n",
            "          7.22939670e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [4.01755273e-02, 3.31600988e-03, 6.15526456e-03, ...,\n",
            "          5.91007732e-02, 2.92522758e-01, 0.00000000e+00],\n",
            "         [3.08905803e-02, 2.58980156e-03, 1.49845565e-02, ...,\n",
            "          3.68343629e-02, 1.06569722e-01, 2.37069786e-01]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.57675934e-01, 4.23240773e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.81937718e-01, 1.30302221e-01, 8.77601057e-02, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.34096574e-03, 3.02323629e-03, 1.44512032e-03, ...,\n",
            "          4.99217364e-04, 0.00000000e+00, 0.00000000e+00],\n",
            "         [2.05330979e-02, 5.01252897e-02, 4.95741554e-02, ...,\n",
            "          3.27098668e-02, 2.47915350e-02, 0.00000000e+00],\n",
            "         [5.15042320e-02, 8.39607418e-03, 1.04314638e-02, ...,\n",
            "          1.56313386e-02, 3.14003564e-02, 6.25530630e-02]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.56533134e-01, 4.34669182e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [6.28594995e-01, 1.37614325e-01, 2.33790666e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [1.04361447e-02, 7.68806925e-03, 7.37504754e-03, ...,\n",
            "          3.80817009e-03, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.94663984e-02, 3.62161487e-01, 2.08725125e-01, ...,\n",
            "          6.10600300e-02, 1.11556880e-03, 0.00000000e+00],\n",
            "         [7.27467611e-02, 1.12144403e-01, 3.46307978e-02, ...,\n",
            "          4.98292446e-02, 5.83001934e-02, 1.89108476e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.88303971e-01, 2.11696029e-01, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [5.84225297e-01, 1.06633693e-01, 3.09140950e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [6.02929993e-03, 9.10700578e-03, 2.11823098e-02, ...,\n",
            "          3.56364157e-03, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.98965525e-03, 2.09854357e-02, 2.27347743e-02, ...,\n",
            "          1.00711416e-02, 4.19349931e-02, 0.00000000e+00],\n",
            "         [5.19997515e-02, 2.82450378e-01, 7.14425296e-02, ...,\n",
            "          5.43049052e-02, 1.39934430e-02, 2.89390571e-02]],\n",
            "\n",
            "        [[1.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [9.57004309e-01, 4.29957323e-02, 0.00000000e+00, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         [7.99835563e-01, 4.84451652e-02, 1.51719242e-01, ...,\n",
            "          0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
            "         ...,\n",
            "         [5.68926334e-02, 6.43340545e-03, 3.97870038e-03, ...,\n",
            "          1.07836854e-02, 0.00000000e+00, 0.00000000e+00],\n",
            "         [1.86920345e-01, 3.42793129e-02, 7.36425817e-02, ...,\n",
            "          8.32200274e-02, 1.98290106e-02, 0.00000000e+00],\n",
            "         [1.73997842e-02, 8.19186680e-03, 9.17159300e-03, ...,\n",
            "          2.36611944e-02, 8.80482420e-02, 4.21322644e-01]]]],\n",
            "      dtype=float32)>, 'decoder_layer6_block2': <tf.Tensor: shape=(1, 16, 15, 100), dtype=float32, numpy=\n",
            "array([[[[0.00999644, 0.00999212, 0.0100104 , ..., 0.00999593,\n",
            "          0.00999597, 0.00999597],\n",
            "         [0.00999758, 0.00999335, 0.01000764, ..., 0.00999755,\n",
            "          0.0099977 , 0.00999766],\n",
            "         [0.01000109, 0.00997475, 0.00998939, ..., 0.01      ,\n",
            "          0.01000011, 0.01000035],\n",
            "         ...,\n",
            "         [0.01000347, 0.01000179, 0.00998221, ..., 0.01000403,\n",
            "          0.01000404, 0.01000373],\n",
            "         [0.01001239, 0.00994787, 0.00993833, ..., 0.01001305,\n",
            "          0.01001338, 0.01001369],\n",
            "         [0.01000017, 0.010012  , 0.00999569, ..., 0.01000209,\n",
            "          0.01000233, 0.01000212]],\n",
            "\n",
            "        [[0.01000996, 0.00994722, 0.00994842, ..., 0.01000783,\n",
            "          0.01000788, 0.01000852],\n",
            "         [0.01001147, 0.00994972, 0.00993928, ..., 0.01001104,\n",
            "          0.01001108, 0.01001092],\n",
            "         [0.01000914, 0.00995268, 0.00994865, ..., 0.01000855,\n",
            "          0.01000866, 0.01000869],\n",
            "         ...,\n",
            "         [0.00999646, 0.01000194, 0.00999563, ..., 0.00999747,\n",
            "          0.00999724, 0.00999695],\n",
            "         [0.00998946, 0.01001638, 0.01002283, ..., 0.00999199,\n",
            "          0.00999173, 0.00999124],\n",
            "         [0.00999508, 0.01002898, 0.01001455, ..., 0.00999604,\n",
            "          0.00999567, 0.00999523]],\n",
            "\n",
            "        [[0.0099906 , 0.01001368, 0.01003702, ..., 0.00999193,\n",
            "          0.00999212, 0.00999229],\n",
            "         [0.00999675, 0.01003875, 0.01002103, ..., 0.00999728,\n",
            "          0.00999693, 0.00999605],\n",
            "         [0.00998915, 0.01006743, 0.01006395, ..., 0.00998963,\n",
            "          0.00998946, 0.00998835],\n",
            "         ...,\n",
            "         [0.0100017 , 0.01000756, 0.00996646, ..., 0.01000033,\n",
            "          0.00999952, 0.00999856],\n",
            "         [0.01000668, 0.00998326, 0.00993737, ..., 0.01000538,\n",
            "          0.0100044 , 0.01000353],\n",
            "         [0.01001309, 0.00997818, 0.00992898, ..., 0.01001113,\n",
            "          0.01001029, 0.01000979]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0.00998682, 0.01004581, 0.0100778 , ..., 0.00998737,\n",
            "          0.00998747, 0.00998781],\n",
            "         [0.00998355, 0.01005253, 0.01008991, ..., 0.00998519,\n",
            "          0.00998536, 0.00998532],\n",
            "         [0.00999226, 0.01001455, 0.01004184, ..., 0.00999301,\n",
            "          0.00999334, 0.00999361],\n",
            "         ...,\n",
            "         [0.00999544, 0.00999256, 0.01002019, ..., 0.00999624,\n",
            "          0.00999669, 0.00999707],\n",
            "         [0.00999625, 0.01001223, 0.01002872, ..., 0.00999592,\n",
            "          0.00999592, 0.00999636],\n",
            "         [0.0100075 , 0.00997138, 0.00996494, ..., 0.01000665,\n",
            "          0.01000674, 0.01000694]],\n",
            "\n",
            "        [[0.01000855, 0.0099289 , 0.00993962, ..., 0.01000751,\n",
            "          0.01000793, 0.01000867],\n",
            "         [0.0099989 , 0.0099806 , 0.00999144, ..., 0.01000058,\n",
            "          0.01000077, 0.01000104],\n",
            "         [0.00998627, 0.0100123 , 0.01005948, ..., 0.0099869 ,\n",
            "          0.00998711, 0.00998682],\n",
            "         ...,\n",
            "         [0.00999785, 0.00998469, 0.00999374, ..., 0.00999904,\n",
            "          0.00999904, 0.00999916],\n",
            "         [0.00999576, 0.00999321, 0.01003351, ..., 0.00999504,\n",
            "          0.0099954 , 0.00999577],\n",
            "         [0.00999909, 0.01001266, 0.01000944, ..., 0.00999703,\n",
            "          0.00999678, 0.00999634]],\n",
            "\n",
            "        [[0.01000751, 0.00997148, 0.00996426, ..., 0.0100077 ,\n",
            "          0.01000729, 0.01000693],\n",
            "         [0.00998517, 0.01005891, 0.01008545, ..., 0.00998815,\n",
            "          0.00998832, 0.00998793],\n",
            "         [0.00999312, 0.01001069, 0.01005447, ..., 0.00999428,\n",
            "          0.0099949 , 0.00999511],\n",
            "         ...,\n",
            "         [0.01000257, 0.00998203, 0.00999246, ..., 0.01000217,\n",
            "          0.01000219, 0.01000228],\n",
            "         [0.00998697, 0.0100403 , 0.01007452, ..., 0.00998875,\n",
            "          0.00998864, 0.00998822],\n",
            "         [0.01000168, 0.00999173, 0.00998544, ..., 0.01000158,\n",
            "          0.01000139, 0.01000126]]]], dtype=float32)>})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLPOlemLOTlX",
        "outputId": "cff4e975-3cc4-4d02-95a4-4eb34058bd3a"
      },
      "source": [
        "sentence = \"<start> i swear to god , i am exceedingly tired . <end>\"\n",
        "print(evaluate(sentence)[0])"
      ],
      "id": "HLPOlemLOTlX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start> now , by two headed monster , nature makes me see a dream . <end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydwfs590O1Dr",
        "outputId": "c75127d4-cf0d-4ee0-d063-8065f95a354f"
      },
      "source": [
        "sentence = \"<start> hello , how are you ? <end>\"\n",
        "print(evaluate(sentence)[0])"
      ],
      "id": "ydwfs590O1Dr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start> how now , what sayst thou ? <end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5deJYNsQNpl",
        "outputId": "dc8381a8-7b5b-4dce-f712-198671a0b649"
      },
      "source": [
        "sentence = \"<start> i do not speak very well . <end>\"\n",
        "print(evaluate(sentence)[0])"
      ],
      "id": "o5deJYNsQNpl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<start> i have done so , and therefore speak no more . <end>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrMjb2pMH9g"
      },
      "source": [
        "BLEU score"
      ],
      "id": "_yrMjb2pMH9g"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxGfwUd4MH9h"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "id": "gxGfwUd4MH9h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7xTLgI2MH9h"
      },
      "source": [
        "# creating a test set with the plays that have odd indices (henry4pt1 and henry4pt2)\n",
        "sparknotesScraper('henry4pt1', \"odd pt1\")\n",
        "sparknotesScraper('henry4pt2', \"odd pt2\")\n",
        "\n",
        "filenames = ['henry4pt1.txt', 'henry4pt2.txt']\n",
        "with open('testSet.txt', 'w', encoding=\"utf-8\") as outfile:\n",
        "    for fname in filenames:\n",
        "        with open(fname, encoding=\"utf-8\") as infile:\n",
        "            for line in infile:\n",
        "                outfile.write(line)"
      ],
      "id": "E7xTLgI2MH9h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gDttMcoMH9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e905732-b46c-4b00-b5c6-c1d6865785b8"
      },
      "source": [
        "# Computing the BLEU score for the test set\n",
        "\n",
        "# load dataset\n",
        "test_doc = load_document('/content/gdrive/MyDrive/text_files/testSet.txt')\n",
        "# split into pairs\n",
        "test_pairs = make_pairs(test_doc)\n",
        "#clean sentences\n",
        "cleaned_test_pairs = clean(test_pairs, 'all')\n",
        "#associate sentences to their respective language \n",
        "input_test, target_test = cleaned_test_pairs\n",
        "\n",
        "sum_bleu = 0\n",
        "\n",
        "for i in range(len(input_test)) :\n",
        "    # set our translation and target\n",
        "    sentence = evaluate(input_test[i])[0]\n",
        "    target = [target_test[i]]\n",
        "    # split by words, remove the start and end tokens \n",
        "    sentence = [ word.split() for word in sentence ][0][1:-1]\n",
        "    target = [ word.split() for word in target ][0][1:-1]\n",
        "    # compute bleu score\n",
        "    curr_bleu = sentence_bleu([target],sentence)\n",
        "    print('BLEU score -> {}'.format(curr_bleu))\n",
        "    sum_bleu += curr_bleu\n",
        "\n",
        "test_average_bleu = sum_bleu/len(input_test)"
      ],
      "id": "9gDttMcoMH9h",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score -> 0.8055344092731546\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.5811850816158368\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.7400828044922853\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score -> 0.2752910796987188\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45622720708659226\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.35930411196308426\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.4165926999433137\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.5124797359336637\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.15882481735499007\n",
            "BLEU score -> 0.429379141889796\n",
            "BLEU score -> 0.3575297164449809\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.13501633901742352\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.20142070913129906\n",
            "BLEU score -> 0.4518010018049224\n",
            "BLEU score -> 0.3330718739770144\n",
            "BLEU score -> 0.4954150028307823\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.003084785356204433\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.49294366294228187\n",
            "BLEU score -> 0.3617119183840374\n",
            "BLEU score -> 0.28387021048806443\n",
            "BLEU score -> 0.3086194627209984\n",
            "BLEU score -> 0.5009145562577421\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.46199933699457096\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.34772504705825924\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.37848698581337653\n",
            "BLEU score -> 0.11054202892156728\n",
            "BLEU score -> 0.4604061366599648\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4929016180976005\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5444358245099123\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.4056114983537769\n",
            "BLEU score -> 0.19605997334125028\n",
            "BLEU score -> 0.04966973534674377\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.40588949514595957\n",
            "BLEU score -> 0.3458601162157628\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.6598203338556885\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.36964680391866866\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.4630777161991027\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4518010018049224\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3043119239482219\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.33437015248821106\n",
            "BLEU score -> 0.3646604730035589\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.7071067811865476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score -> 0.1717218426146756\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.7187737616553884\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5235215949109693\n",
            "BLEU score -> 0.6799308458396492\n",
            "BLEU score -> 0.35930411196308426\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.5506953149031837\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.7447819789879647\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.7730551756939454\n",
            "BLEU score -> 0.4212172547055404\n",
            "BLEU score -> 0.1382932529237074\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.46199933699457096\n",
            "BLEU score -> 0.43645382979233377\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.16954225822593183\n",
            "BLEU score -> 0.6059285518620335\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.43173061492439624\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.3467847604369132\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.16842357369809585\n",
            "BLEU score -> 0.46230595512422085\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5430321466633391\n",
            "BLEU score -> 0.12228923309170908\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.49473859088183875\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.427287006396234\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.07791003482325053\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.19303951204286907\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5738271161810361\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.41368954504257255\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5585783491121418\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.17521137606757647\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5502659908318907\n",
            "BLEU score -> 0.5497775311418521\n",
            "BLEU score -> 0.7364279629037999\n",
            "BLEU score -> 0.36064528799877893\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.10043075487859517\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.18762935180380186\n",
            "BLEU score -> 0.08848212044257439\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.0016048907225441346\n",
            "BLEU score -> 0.4578141331660858\n",
            "BLEU score -> 0.6009638585283708\n",
            "BLEU score -> 0.08762406445626589\n",
            "BLEU score -> 0.5166357204442371\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.4859869096699081\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.23735579159148829\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.30096441267171586\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6985342056580097\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.37846125781090306\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.7476743906106103\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5828233954152654\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.18762935180380186\n",
            "BLEU score -> 0.31702331385234306\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.46173663094410267\n",
            "BLEU score -> 0.37239098949398236\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.38538569180303145\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.35733817274964674\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.4859869096699081\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.32605933883039784\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4608636396914616\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.2718895182490884\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.46230595512422085\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6094445482670285\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.4063798282013443\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.36751424429959983\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.10283257773094409\n",
            "BLEU score -> 0.11380295453101374\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7066452455664317\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.6080253214198359\n",
            "BLEU score -> 0.23574921873400892\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.6598203338556885\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.24303324868167356\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.4608636396914616\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.3340135926488844\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.3367765041827165\n",
            "BLEU score -> 0.6905911470987942\n",
            "BLEU score -> 0.3860973950960897\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.18306026428729755\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.38503228868787126\n",
            "BLEU score -> 0.1961113472255835\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5499041772350242\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.7765453555044466\n",
            "BLEU score -> 0.35733817274964674\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.260899397956404\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.0045059367310833825\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.408248290463863\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.32253883071974426\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3860973950960897\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3860973950960897\n",
            "BLEU score -> 0.34772504705825924\n",
            "BLEU score -> 0.48143015836328423\n",
            "BLEU score -> 0.3769486629893372\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0.35831291876413535\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4128713178810563\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.3655552228545123\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4971247413297251\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.408248290463863\n",
            "BLEU score -> 0.41368954504257255\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.38170386946328344\n",
            "BLEU score -> 0.21769656876473278\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.16490932887256968\n",
            "BLEU score -> 0.5482668263811105\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3639412530979476\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6256112264776171\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5369787816169341\n",
            "BLEU score -> 0.29446564457869806\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6537993517025207\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4791733671582712\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.4539249573898065\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.7071067811865475\n",
            "BLEU score -> 0.7071067811865475\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.46199933699457096\n",
            "BLEU score -> 0.4760116549244004\n",
            "BLEU score -> 0.7351304494643948\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.2025894847023147\n",
            "BLEU score -> 0.6474591278836639\n",
            "BLEU score -> 0.5066641486392106\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.40289729917086176\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.3575297164449809\n",
            "BLEU score -> 0.4824015383731099\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.40866465020165643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.4954150028307823\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.1353352832366127\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.3430988896407051\n",
            "BLEU score -> 0.41843795218458035\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.2601300475114445\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.02654078472268803\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.785629301801026\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.3302232277439296\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.30503839273089395\n",
            "BLEU score -> 0.3302232277439296\n",
            "BLEU score -> 0.5828233954152654\n",
            "BLEU score -> 0.22290865704882615\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.7447819789879647\n",
            "BLEU score -> 0.2980810823649563\n",
            "BLEU score -> 0.3665113625996641\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.31554317807824367\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.2998238942726299\n",
            "BLEU score -> 0.5166357204442371\n",
            "BLEU score -> 0.10215406126626542\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2925637512788283\n",
            "BLEU score -> 0.5487773527367232\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.5878560343277044\n",
            "BLEU score -> 0.4402274324685413\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.4063798282013443\n",
            "BLEU score -> 0.14733461804689552\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.37524619246121804\n",
            "BLEU score -> 0.4954150028307823\n",
            "BLEU score -> 0.5899565399238539\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.32406944672724197\n",
            "BLEU score -> 0.2526762451280852\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.5247357977607321\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5270837136273565\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.07409853791557794\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0.37239098949398236\n",
            "BLEU score -> 0.32406944672724197\n",
            "BLEU score -> 0.5302158042203026\n",
            "BLEU score -> 0.5200672613758193\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6904573083274563\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.2842202242491899\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3302232277439296\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2947659609337933\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6186763792600443\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6192596340984008\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.5878560343277044\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.34920593006397116\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.007145162398882999\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.5247357977607321\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.645777692487244\n",
            "BLEU score -> 0.6505696445772021\n",
            "BLEU score -> 0.42456725576936255\n",
            "BLEU score -> 0.6334717766551771\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6496350258549114\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.30096441267171586\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5497775311418521\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.408248290463863\n",
            "BLEU score -> 0.7162326270441588\n",
            "BLEU score -> 0.34334272076197153\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5885661912765424\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.3288580454955831\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2601300475114445\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.43173061492439624\n",
            "BLEU score -> 0.34772504705825924\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5378329261427206\n",
            "BLEU score -> 0.3302232277439296\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.010028146616223624\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.14256747717620566\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.1973862244299071\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.04469085405803316\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.5095231471606585\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.35930411196308426\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5112863131654276\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.08762406445626589\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4859869096699081\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6904573083274563\n",
            "BLEU score -> 0.18650837519387592\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.43645382979233377\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.36064528799877893\n",
            "BLEU score -> 0.6905911470987942\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.645777692487244\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.35773299205763026\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.40276720463657734\n",
            "BLEU score -> 0.5899565399238539\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.3324166001293853\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.18762935180380186\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.4954150028307823\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.4919625503668659\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2808708327044614\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.46139791522696905\n",
            "BLEU score -> 0.4539249573898065\n",
            "BLEU score -> 0.5115078115793242\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.34772504705825924\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.31239399369202553\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.18480443712078704\n",
            "BLEU score -> 0.4919625503668659\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.3073073403268802\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.7023750892707125\n",
            "BLEU score -> 0.5911842794331965\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.34823528327578546\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.6566833926868654\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.18762935180380186\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5911842794331965\n",
            "BLEU score -> 0.2658506095945427\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.3324166001293853\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.13520093481548168\n",
            "BLEU score -> 0.4165926999433137\n",
            "BLEU score -> 0.427287006396234\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.3588478753117309\n",
            "BLEU score -> 0.260899397956404\n",
            "BLEU score -> 0.6025172834222573\n",
            "BLEU score -> 0.6985342056580097\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.08762406445626589\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.1353352832366127\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.26896050220204015\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.47587330964125224\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.310263420349682\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.6703200460356393\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4763100914774508\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.20211771013895866\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.4061660547784665\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.2601300475114445\n",
            "BLEU score -> 0.4004970149398301\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.35892096348084807\n",
            "BLEU score -> 0.460407711864358\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.18225299438933673\n",
            "BLEU score -> 0.20142070913129906\n",
            "BLEU score -> 0.39664070074131375\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.4193597461328891\n",
            "BLEU score -> 0.4071220775553065\n",
            "BLEU score -> 0.2561812362933974\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.23735579159148829\n",
            "BLEU score -> 0.4630777161991027\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.14733461804689552\n",
            "BLEU score -> 0.34823528327578546\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.46149373744099287\n",
            "BLEU score -> 0.7447819789879647\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.31702331385234306\n",
            "BLEU score -> 0.5206412525067085\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.35555561857798457\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.36787944117144233\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.36064528799877893\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.7195907128571539\n",
            "BLEU score -> 0.5461499540157964\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6044142795544439\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.38297956737438044\n",
            "BLEU score -> 0.43668354428478123\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5585783491121418\n",
            "BLEU score -> 0.3280443278078623\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.10283257773094409\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.33065331125965697\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.3352535288128678\n",
            "BLEU score -> 0.5206571060403834\n",
            "BLEU score -> 0.0016048907225441346\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.408248290463863\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5470102695338788\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4604061366599648\n",
            "BLEU score -> 0.32257942276784324\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.48549177170732344\n",
            "BLEU score -> 0.6904573083274563\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.36821398145189993\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5673835037021759\n",
            "BLEU score -> 0.3381307292971254\n",
            "BLEU score -> 0.7108315914033534\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.5166357204442371\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.2236800154620159\n",
            "BLEU score -> 0.5807156200013266\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.37991784282579627\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5270837136273565\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.1353352832366127\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.38498150077635496\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.23818690214719165\n",
            "BLEU score -> 0.4919625503668659\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.09050415858572283\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.37991784282579627\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.28146399662233007\n",
            "BLEU score -> 0.4329820146406897\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5755901146501323\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.2216537215215156\n",
            "BLEU score -> 0.052796005801316395\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.23735579159148829\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3976353643835253\n",
            "BLEU score -> 0.12148233648426672\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.41447920373102765\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.052796005801316395\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.34718201116725705\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.36751424429959983\n",
            "BLEU score -> 0.35250657096759425\n",
            "BLEU score -> 0.6080253214198359\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.08189154917622246\n",
            "BLEU score -> 0.10215406126626542\n",
            "BLEU score -> 0.34107725495137897\n",
            "BLEU score -> 0.044987694731062185\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.1353352832366127\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.4919625503668659\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.145172495655484\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.3302232277439296\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.1314573673003644\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.4181117364345025\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6985342056580097\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.33125669191122536\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.36408617932284865\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.43443712531357925\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5878560343277044\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.43645382979233377\n",
            "BLEU score -> 0.5206571060403834\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.5899565399238539\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.2980810823649563\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7447819789879647\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.39721134088567395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.41722614486115056\n",
            "BLEU score -> 0.6256112264776171\n",
            "BLEU score -> 0.4001601601922499\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.42631962149316155\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6025172834222573\n",
            "BLEU score -> 0.2601300475114445\n",
            "BLEU score -> 0.5369787816169341\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.1284223483958171\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5408854763087599\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.22825471031091593\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.1939032620198005\n",
            "BLEU score -> 0.30096441267171586\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.37493818432751624\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.15777684932819508\n",
            "BLEU score -> 0.09569649651041094\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.27606941372744115\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3367765041827165\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.3085018208266527\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0.36751424429959983\n",
            "BLEU score -> 0.28433291815307693\n",
            "BLEU score -> 0.3655281142832326\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.006737946999085467\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.23818242442380938\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.261857854688235\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.36787944117144233\n",
            "BLEU score -> 0.3860973950960897\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.4001755536681283\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.36787944117144233\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.34823528327578546\n",
            "BLEU score -> 0.11413953369418132\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.43645382979233377\n",
            "BLEU score -> 0.487191740069029\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.23953094040783438\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5235215949109693\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5497775311418521\n",
            "BLEU score -> 0.8210967436686386\n",
            "BLEU score -> 0.36787944117144233\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.34916855153702\n",
            "BLEU score -> 0.6799308458396492\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5535566902494308\n",
            "BLEU score -> 0.28387021048806443\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5247357977607321\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.43645382979233377\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.3599156581257072\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4539249573898065\n",
            "BLEU score -> 0.5430321466633391\n",
            "BLEU score -> 0.0004230446014511149\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.27151829550071505\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.8801117367933934\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.02654078472268803\n",
            "BLEU score -> 0.4566337854967312\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6009638585283708\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.3922967662532704\n",
            "BLEU score -> 0.6025172834222573\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.4111336169005197\n",
            "BLEU score -> 0.5504024028869415\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.7195907128571539\n",
            "BLEU score -> 0.08990698828155157\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.7765453555044466\n",
            "BLEU score -> 0.7691605673134586\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.16842357369809585\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5023505432440377\n",
            "BLEU score -> 0.3154367391372182\n",
            "BLEU score -> 0.36064528799877893\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.2626909894424158\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.38827267775222324\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.26896050220204015\n",
            "BLEU score -> 0.7203294535577252\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.24902467466187805\n",
            "BLEU score -> 0.35733817274964674\n",
            "BLEU score -> 0.1669896596583327\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.30603689509300896\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.34107725495137897\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.4344371253135792\n",
            "BLEU score -> 0.4402274324685413\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.3701856698591431\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.3949197564412015\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6334717766551771\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.14612588611198102\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.2808708327044614\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.2792504378041133\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.15218787864872976\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.38827267775222324\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.4591497693322865\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.5106948710219815\n",
            "BLEU score -> 0.04186576731652839\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.43645382979233377\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.3575297164449809\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.3468399101570504\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.6009638585283708\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.6566833926868654\n",
            "BLEU score -> 0.6904573083274563\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.5762018056183854\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.5504024028869415\n",
            "BLEU score -> 0\n",
            "BLEU score -> 2.3155301547017863e-05\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.037829991229878616\n",
            "BLEU score -> 0.35733817274964674\n",
            "BLEU score -> 0.22165788851265286\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5300714512917181\n",
            "BLEU score -> 0.7364279629037999\n",
            "BLEU score -> 0.00033546262790251185\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.392814650900513\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.3715011599826719\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.15033932314270831\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.36787944117144233\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.20066991261040096\n",
            "BLEU score -> 0.4056114983537769\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5124797359336637\n",
            "BLEU score -> 0.6684577612015127\n",
            "BLEU score -> 0.31702331385234306\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6842636738577972\n",
            "BLEU score -> 0.6904573083274563\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.41368954504257255\n",
            "BLEU score -> 0.0870458977726377\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.31702331385234306\n",
            "BLEU score -> 0.645777692487244\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.46199933699457096\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7765453555044466\n",
            "BLEU score -> 0.4359583238409739\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.19896894256029013\n",
            "BLEU score -> 0.5499041772350242\n",
            "BLEU score -> 0.5980456132683322\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5432852079582625\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.64264183387712\n",
            "BLEU score -> 0.062077839810594664\n",
            "BLEU score -> 0.6800874738944077\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5444358245099123\n",
            "BLEU score -> 0.14528285930715126\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.2808708327044614\n",
            "BLEU score -> 0.34602490993480084\n",
            "BLEU score -> 0.7146704964214272\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.14893601435317105\n",
            "BLEU score -> 0.34718201116725705\n",
            "BLEU score -> 0.4604061366599648\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5820272706551887\n",
            "BLEU score -> 0.02539287146986957\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.48549177170732344\n",
            "BLEU score -> 0.4630777161991027\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.15777684932819508\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3367765041827165\n",
            "BLEU score -> 0.28546623680912253\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.7162326270441588\n",
            "BLEU score -> 0.4553943917994181\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.7162326270441588\n",
            "BLEU score -> 0.46199933699457096\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5369787816169341\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.7790872728179328\n",
            "BLEU score -> 0.4824015383731099\n",
            "BLEU score -> 0.15943003150791318\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.3426375671185299\n",
            "BLEU score -> 0.36840762547436395\n",
            "BLEU score -> 0.3769486629893372\n",
            "BLEU score -> 0.4192033222798747\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6546916505371511\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.37848698581337653\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.20142070913129906\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.645777692487244\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.49760938992507125\n",
            "BLEU score -> 0.23818690214719165\n",
            "BLEU score -> 0.25091539267089075\n",
            "BLEU score -> 0.29059130342609063\n",
            "BLEU score -> 0.14612588611198102\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.4954150028307823\n",
            "BLEU score -> 0.22616792214653433\n",
            "BLEU score -> 0.4630777161991027\n",
            "BLEU score -> 0.5093330917854971\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.7765453555044466\n",
            "BLEU score -> 0.7765453555044466\n",
            "BLEU score -> 0.6361254554431539\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5491426832978561\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.10287094924941535\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.26896050220204015\n",
            "BLEU score -> 0.3228914888628616\n",
            "BLEU score -> 0.40894314105732416\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.17521137606757647\n",
            "BLEU score -> 0.34718201116725705\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.49473859088183875\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5430321466633391\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.37524619246121804\n",
            "BLEU score -> 0.3082449726577108\n",
            "BLEU score -> 0.06902498108894259\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6696546139496112\n",
            "BLEU score -> 0.23505403213046533\n",
            "BLEU score -> 0.36064528799877893\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.5501589630653061\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.5023505432440377\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.3302232277439296\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.37288786399304175\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.487191740069029\n",
            "BLEU score -> 0.3423503955179092\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7165313105737893\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.48301556221513736\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.3141116890623593\n",
            "BLEU score -> 0.4256279541919936\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.23818690214719165\n",
            "BLEU score -> 0.36964680391866866\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6059285518620335\n",
            "BLEU score -> 0.46615352004842686\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.14516170898221817\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.3386854985606571\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.2592223702781733\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.3375804740497263\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.23661528215726477\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.17169351902980165\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.14162677936694862\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.6696546139496112\n",
            "BLEU score -> 0.6025286104785453\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.38890556115271097\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.41722614486115056\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.7608745412138651\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.0820849986238988\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.6696546139496112\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.5698081628956971\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6304570627961232\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.4061660547784665\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.3381307292971254\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.1314573673003644\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.46230595512422085\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.23039719050455434\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.5088274727401554\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.3490881360123862\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.645777692487244\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.38170386946328344\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.37264447638013715\n",
            "BLEU score -> 0.40866465020165643\n",
            "BLEU score -> 0.2636715372516468\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.41368954504257255\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.260899397956404\n",
            "BLEU score -> 0.43668354428478123\n",
            "BLEU score -> 0.37524619246121804\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.6505696445772021\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0.17035677145427366\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.4004970149398301\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4128713178810563\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.3085018208266527\n",
            "BLEU score -> 0.29071536848410967\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.6505696445772021\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5497775311418521\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6094445482670285\n",
            "BLEU score -> 0.3040559696901293\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.27629350710622463\n",
            "BLEU score -> 0.16954225822593183\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.6025172834222573\n",
            "BLEU score -> 0.26757884657176234\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.5270837136273565\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.41722614486115056\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7765453555044466\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.5122490320374565\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7259795291154771\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.345720784641941\n",
            "BLEU score -> 0.3008952140022968\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.2525819952812828\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6025286104785453\n",
            "BLEU score -> 0.5862121112664649\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.6985342056580097\n",
            "BLEU score -> 0.40866465020165643\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.3182683495906518\n",
            "BLEU score -> 0.46173663094410267\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5444358245099123\n",
            "BLEU score -> 0.1540917132492418\n",
            "BLEU score -> 0.3824396628290705\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.6828267746069693\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.5497775311418521\n",
            "BLEU score -> 0.4042892997911646\n",
            "BLEU score -> 0.15882481735499007\n",
            "BLEU score -> 0.46173663094410267\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0.35250657096759425\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.0524476438328049\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.027259328720003743\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4001601601922499\n",
            "BLEU score -> 0.24092085663272753\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4630777161991027\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.07368276169123442\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.11380295453101374\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.3291467773009902\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.41266825715677186\n",
            "BLEU score -> 0.4960923395774537\n",
            "BLEU score -> 0.2092314418483734\n",
            "BLEU score -> 0.7447819789879647\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.3280443278078623\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.46173663094410267\n",
            "BLEU score -> 0.32834169634343274\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6985342056580097\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6051012508914458\n",
            "BLEU score -> 0.21142141714303078\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.9036020036098448\n",
            "BLEU score -> 0.6795110078264116\n",
            "BLEU score -> 0.2718895182490884\n",
            "BLEU score -> 0.3023870654786627\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.7197629007461867\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7497440247808671\n",
            "BLEU score -> 0.40866465020165643\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3436446393954861\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.4608636396914616\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.35930411196308426\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.1314573673003644\n",
            "BLEU score -> 0.4639247374454443\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6696546139496112\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.4402274324685413\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.061959570172152735\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5206571060403834\n",
            "BLEU score -> 0.5166357204442371\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.19605997334125028\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.7118034480382983\n",
            "BLEU score -> 0.4578141331660858\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.11380295453101374\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4128074884933345\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.7118034480382983\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5247357977607321\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.3715011599826719\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.29765372490051634\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.3182683495906518\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0.4111336169005197\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.32406944672724197\n",
            "BLEU score -> 0.6059285518620335\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.35655270831683505\n",
            "BLEU score -> 0.10283257773094409\n",
            "BLEU score -> 0.21626014939773608\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.260899397956404\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7162326270441588\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.35733817274964674\n",
            "BLEU score -> 0.04469085405803316\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.427287006396234\n",
            "BLEU score -> 0.6494854111739939\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3302232277439296\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3485340766146352\n",
            "BLEU score -> 0.36064528799877893\n",
            "BLEU score -> 0.33125669191122536\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.408248290463863\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.3123777891373255\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.45622720708659226\n",
            "BLEU score -> 0.32965129549221617\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.46173663094410267\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.27748702735605824\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0.26430575030060605\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.26896050220204015\n",
            "BLEU score -> 0.35733817274964674\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.16490932887256968\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.4954150028307823\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.15777684932819508\n",
            "BLEU score -> 0.41722614486115056\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.30603689509300896\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.35640264633541835\n",
            "BLEU score -> 0.4111336169005197\n",
            "BLEU score -> 0.34718201116725705\n",
            "BLEU score -> 0.4859869096699081\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.18762935180380186\n",
            "BLEU score -> 0.3381307292971254\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5813189627146943\n",
            "BLEU score -> 0.41545589177443254\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6474469561695607\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.4288819424803534\n",
            "BLEU score -> 0.29642151188002913\n",
            "BLEU score -> 0.3485340766146352\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.6660502379419337\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.41105458056789007\n",
            "BLEU score -> 0.3367765041827165\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 0.48301556221513736\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.46199933699457096\n",
            "BLEU score -> 0.4859869096699081\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.4402274324685413\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.418030412918096\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.4056114983537769\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.49760938992507125\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.07368276169123442\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0.345720784641941\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.16842357369809585\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.3386854985606571\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.42311785416105785\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.33437015248821106\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.8801117367933934\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.23505403213046533\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4417918226831577\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.6474591278836639\n",
            "BLEU score -> 0.6474591278836639\n",
            "BLEU score -> 0.6474591278836639\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.43668354428478123\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.41368954504257255\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.14256747717620566\n",
            "BLEU score -> 0.16490932887256968\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.7447819789879647\n",
            "BLEU score -> 0.2525819952812828\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.43173061492439624\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.4548019047027907\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.46173663094410267\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.8891397050194614\n",
            "BLEU score -> 0.2236800154620159\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.43668354428478123\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6094445482670285\n",
            "BLEU score -> 0.5115078115793242\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.34823528327578546\n",
            "BLEU score -> 0.5430321466633391\n",
            "BLEU score -> 0.35831291876413535\n",
            "BLEU score -> 0.39459043571442387\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2998221389342337\n",
            "BLEU score -> 0.37239098949398236\n",
            "BLEU score -> 0.3381307292971254\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.3522218281048356\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.37239098949398236\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5200672613758193\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.293189764929112\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.43443712531357925\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.4591497693322865\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.21258844131063828\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6474591278836639\n",
            "BLEU score -> 0.37354385553848\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6474591278836639\n",
            "BLEU score -> 0.4960923395774537\n",
            "BLEU score -> 0.23661528215726477\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5247357977607321\n",
            "BLEU score -> 0.6009638585283708\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.4004970149398301\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.36177396082048563\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.3653166213293175\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.4919625503668659\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.4192033222798747\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.23505403213046533\n",
            "BLEU score -> 0.19605997334125028\n",
            "BLEU score -> 0.4630777161991027\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.15188686969779985\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.3525877739588073\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4919625503668659\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.09569649651041094\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.46230595512422085\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.4954150028307823\n",
            "BLEU score -> 0.16490932887256968\n",
            "BLEU score -> 0.3599156581257072\n",
            "BLEU score -> 0.513868592693246\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.42502814134169775\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5980456132683322\n",
            "BLEU score -> 0.36787944117144233\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.3639412530979476\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.38440911688639945\n",
            "BLEU score -> 0.47178233073791986\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.04439597201804127\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.37848698581337653\n",
            "BLEU score -> 0.3303164318013807\n",
            "BLEU score -> 0.5497775311418521\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4608636396914616\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.35930411196308426\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.46230595512422085\n",
            "BLEU score -> 0.26104909033290696\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.46944590249571533\n",
            "BLEU score -> 0.12068076399341783\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.3320866075042764\n",
            "BLEU score -> 0.7259795291154771\n",
            "BLEU score -> 0.24786763988804325\n",
            "BLEU score -> 0.7510499815709779\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.2601300475114445\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.31554317807824367\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5124797359336637\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.7259795291154771\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6417815203955494\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.392814650900513\n",
            "BLEU score -> 0.12287673380733741\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.42456725576936255\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.37846125781090306\n",
            "BLEU score -> 0.49473859088183875\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.5494128986804837\n",
            "BLEU score -> 0.4604061366599648\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.33334900461898226\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.64264183387712\n",
            "BLEU score -> 0.7364279629037999\n",
            "BLEU score -> 0.6198438407339825\n",
            "BLEU score -> 0.1751149016046819\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.23818690214719165\n",
            "BLEU score -> 0.11380295453101374\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.672620776528632\n",
            "BLEU score -> 0.37457288233497027\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5502659908318907\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.10843833698468709\n",
            "BLEU score -> 0.20142070913129906\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.43173061492439624\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.4628314075346599\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.576184260799304\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.3544801439961181\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.5837979488764491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.44116293593227063\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6334717766551771\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.6143868746168435\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.4791733671582712\n",
            "BLEU score -> 0.35655270831683505\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.12287673380733741\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6111127561260491\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.816496580927726\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.37150115998267197\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.40866465020165643\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.16490932887256968\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.22739562220830448\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.32406944672724197\n",
            "BLEU score -> 0.10283257773094409\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.38297956737438044\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.17066837315703817\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6905911470987942\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.3182683495906518\n",
            "BLEU score -> 0.338794894588181\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2642059951936232\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5467707026923339\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.18762935180380186\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.8091067115702212\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7186082239261684\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7447819789879647\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.8801117367933934\n",
            "BLEU score -> 0.35930411196308426\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.32320345265575745\n",
            "BLEU score -> 0.23933119729016494\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.2947659609337933\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.40866465020165643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.44470422371688834\n",
            "BLEU score -> 0.20687381245863395\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.14571389846110752\n",
            "BLEU score -> 0.3455732455922215\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.26714961079926036\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.34718201116725705\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.4128713178810563\n",
            "BLEU score -> 0.38827267775222324\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.4513512579750041\n",
            "BLEU score -> 0.22697340794651855\n",
            "BLEU score -> 0.1939032620198005\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.2574526462394603\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.037829991229878616\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.34718201116725705\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.30603689509300896\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.37991784282579627\n",
            "BLEU score -> 0.2613022659677713\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5349156151337846\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.3715011599826719\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.6051012508914458\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.6905911470987942\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.2643191130222548\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.23505403213046533\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.3490881360123862\n",
            "BLEU score -> 0.3123939936920256\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.427287006396234\n",
            "BLEU score -> 0.24808415001701817\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6905911470987942\n",
            "BLEU score -> 0.5378329261427206\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.011994743160417954\n",
            "BLEU score -> 0.34107725495137897\n",
            "BLEU score -> 0.4893531205487241\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.487191740069029\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.6606328636027614\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.3452817958081464\n",
            "BLEU score -> 0.33260249505555045\n",
            "BLEU score -> 0.3043119239482219\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.40505769908942735\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.8210967436686386\n",
            "BLEU score -> 0.8801117367933934\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.37848698581337653\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.25961246583333675\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.4608636396914616\n",
            "BLEU score -> 0.3654520756092885\n",
            "BLEU score -> 0.4350397028981374\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6597754819590111\n",
            "BLEU score -> 0.46615352004842686\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.1961113472255835\n",
            "BLEU score -> 0.7364279629037999\n",
            "BLEU score -> 0.4764846650730826\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6172911747975007\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6904573083274563\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.32253883071974426\n",
            "BLEU score -> 0.5878560343277044\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.4101872523732577\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.37150115998267197\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.41722614486115056\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.48549177170732344\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.659858515800358\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.3653166213293175\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.4001601601922499\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.3182683495906518\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.6334717766551771\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.4728708045015879\n",
            "BLEU score -> 0.6494854111739939\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.48301556221513736\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.33125669191122536\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2335030836430423\n",
            "BLEU score -> 0.4639247374454443\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.31702331385234306\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.7186082239261684\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.2601300475114445\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.07368276169123442\n",
            "BLEU score -> 0.38538569180303145\n",
            "BLEU score -> 0.45691722266180906\n",
            "BLEU score -> 0.260899397956404\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.659858515800358\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.5917606269910581\n",
            "BLEU score -> 0.5502659908318907\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.3433629176453739\n",
            "BLEU score -> 0.24092085663272753\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.1353352832366127\n",
            "BLEU score -> 0.32734582526857553\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.23735579159148829\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4929016180976005\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6807097238524916\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.4972214518340529\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7186082239261684\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.645777692487244\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.19896894256029013\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2925637512788283\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.6051012508914458\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4041031009353247\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0.15033932314270831\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.14256747717620566\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.48549177170732344\n",
            "BLEU score -> 0.0893629630824924\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5430321466633391\n",
            "BLEU score -> 0.645777692487244\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.2808708327044614\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.345720784641941\n",
            "BLEU score -> 0.816496580927726\n",
            "BLEU score -> 0.4919625503668659\n",
            "BLEU score -> 0.24731174153681904\n",
            "BLEU score -> 0.32406944672724197\n",
            "BLEU score -> 0.31287398156112384\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.659858515800358\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.21104245852031664\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.22616792214653433\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.45654245130026433\n",
            "BLEU score -> 0.34572078464194106\n",
            "BLEU score -> 0.45180100180492244\n",
            "BLEU score -> 0.20211771013895866\n",
            "BLEU score -> 0.3112878027628485\n",
            "BLEU score -> 0.4004970149398301\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.2442008265943985\n",
            "BLEU score -> 0.12772112425298793\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.043070009314404474\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.23818690214719165\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.013916876033168307\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.5828233954152654\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.04500448168902282\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0.33827609072236847\n",
            "BLEU score -> 0.7679634266158699\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.4859869096699081\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwaneB8KvEg4",
        "outputId": "7612056e-56f7-4dd8-db00-04acb73713f7"
      },
      "source": [
        "# Computing the BLEU score on a small batch of the training set\n",
        "\n",
        "# load dataset\n",
        "training_doc = load_document('/content/gdrive/MyDrive/text_files/trainingSet.txt')\n",
        "# split into pairs\n",
        "training_pairs = make_pairs(training_doc)\n",
        "#clean sentences\n",
        "cleaned_training_pairs = clean(training_pairs, 'all')\n",
        "#associate sentences to their respective language \n",
        "input_training, target_training = cleaned_training_pairs\n",
        "\n",
        "sum_bleu = 0\n",
        "\n",
        "for i in range(500) :\n",
        "    # set our translation and target\n",
        "    sentence = evaluate(input_training[i])[0]\n",
        "    target = [target_training[i]]\n",
        "    # split by words, remove the start and end tokens \n",
        "    sentence = [ word.split() for word in sentence ][0][1:-1]\n",
        "    target = [ word.split() for word in target ][0][1:-1]\n",
        "    # compute bleu score\n",
        "    curr_bleu = sentence_bleu([target],sentence)\n",
        "    print('BLEU score -> {}'.format(curr_bleu))\n",
        "    sum_bleu += curr_bleu\n",
        "\n",
        "training_average_bleu = sum_bleu/500"
      ],
      "id": "ZwaneB8KvEg4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score -> 0.3639412530979476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score -> 1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.18762935180380186\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.7788007830714049\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6803749333171202\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.24601580968354606\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.42456725576936255\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.4604061366599648\n",
            "BLEU score -> 0.4824015383731099\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.16490932887256968\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6756000774035172\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0.5755901146501323\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0.038264211051700706\n",
            "BLEU score -> 0.13501633901742352\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.12228923309170908\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.27768352844338134\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.6009638585283708\n",
            "BLEU score -> 0.3579090209118197\n",
            "BLEU score -> 0.36787944117144233\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.6051012508914458\n",
            "BLEU score -> 0.16954225822593183\n",
            "BLEU score -> 0.476273899703797\n",
            "BLEU score -> 0.23282685256666694\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.13501633901742352\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.6076795808137692\n",
            "BLEU score -> 0.5555238068023582\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5755901146501323\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.0673957081182292\n",
            "BLEU score -> 0.41368954504257255\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.38497426343184826\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.427287006396234\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.2501325796027163\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.5166357204442372\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.4548019047027907\n",
            "BLEU score -> 0.3324166001293853\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.5491426832978561\n",
            "BLEU score -> 0.6094445482670285\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.049787068367863944\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.5585783491121418\n",
            "BLEU score -> 0.6431870218238024\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.392814650900513\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.7247557929987696\n",
            "BLEU score -> 0.3288580454955831\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5329462628216854\n",
            "BLEU score -> 0.6009638585283708\n",
            "BLEU score -> 0.11883952355778202\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.4550524645870584\n",
            "BLEU score -> 0.3152861344254501\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.6025172834222573\n",
            "BLEU score -> 0.48109772909788073\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.260899397956404\n",
            "BLEU score -> 0.43443712531357925\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.15777684932819508\n",
            "BLEU score -> 0.816496580927726\n",
            "BLEU score -> 0.4539249573898065\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7090641203367742\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.6051012508914458\n",
            "BLEU score -> 0.5088274727401554\n",
            "BLEU score -> 0.4746358914289604\n",
            "BLEU score -> 0.3186445002375616\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.23818690214719165\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.3004843884984905\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.1353352832366127\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0.35733817274964674\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.43173061492439624\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.7186082239261684\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.38503228868787126\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.5899565399238539\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6042750794713536\n",
            "BLEU score -> 0.45622720708659226\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0.4608636396914616\n",
            "BLEU score -> 0.5408536609893481\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.30603689509300896\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.35250657096759425\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.7400828044922853\n",
            "BLEU score -> 0.4972214518340529\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.6930977286178778\n",
            "BLEU score -> 0.38753858253732953\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.17066837315703817\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.2925637512788283\n",
            "BLEU score -> 0.1603801045486562\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.3630407264452067\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.260899397956404\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.4539249573898065\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.34823528327578546\n",
            "BLEU score -> 0.816496580927726\n",
            "BLEU score -> 0.5497775311418521\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.6240195441936915\n",
            "BLEU score -> 0.4854917717073234\n",
            "BLEU score -> 0.17035677145427366\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.15777684932819508\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.48301556221513736\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.6025172834222573\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.1353352832366127\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.46173663094410267\n",
            "BLEU score -> 1.0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.5942647088781684\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6262844962765469\n",
            "BLEU score -> 0.5081327481546147\n",
            "BLEU score -> 0.6773709971213142\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.37846125781090306\n",
            "BLEU score -> 0.30096441267171586\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.46905226098954195\n",
            "BLEU score -> 0.7825422900366437\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.5330859115179258\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7226568811456053\n",
            "BLEU score -> 0.4924790605054523\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.24786763988804325\n",
            "BLEU score -> 0.30934850332660563\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.6964705665515708\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.6548907866815301\n",
            "BLEU score -> 0.15777684932819508\n",
            "BLEU score -> 0.5206571060403834\n",
            "BLEU score -> 0.5169731539571706\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.2650845609581655\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.3670124608961283\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.3860973950960897\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.39011264866539486\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.20029051217596075\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.261857854688235\n",
            "BLEU score -> 0.1961113472255835\n",
            "BLEU score -> 0.310263420349682\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.32030896835479866\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.5773502691896257\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.1076281893274762\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.11982354975183966\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.32342420890314744\n",
            "BLEU score -> 0.4566337854967312\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.429379141889796\n",
            "BLEU score -> 0.41368954504257255\n",
            "BLEU score -> 0.11380295453101374\n",
            "BLEU score -> 0.33401359264888447\n",
            "BLEU score -> 0.13501633901742352\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.4630777161991027\n",
            "BLEU score -> 0.5475182535069453\n",
            "BLEU score -> 0.5432852079582625\n",
            "BLEU score -> 0.5856596027429395\n",
            "BLEU score -> 0.6505696445772021\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7598356856515925\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0.6580370064762462\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.24951408905404326\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.6511126026643229\n",
            "BLEU score -> 0.5891510462454596\n",
            "BLEU score -> 0.5444460596606694\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.5850453652111616\n",
            "BLEU score -> 0.3939328149130935\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.7788007830714049\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.510029457493824\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5946035575013605\n",
            "BLEU score -> 0.5266403878479265\n",
            "BLEU score -> 0.6529942057256104\n",
            "BLEU score -> 0.6865890479690392\n",
            "BLEU score -> 0.668740304976422\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6080253214198359\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.8408964152537145\n",
            "BLEU score -> 0.5501589630653061\n",
            "BLEU score -> 0.6303647413359293\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.45348744138729463\n",
            "BLEU score -> 0.5695988432761473\n",
            "BLEU score -> 0.6481388934544839\n",
            "BLEU score -> 0.14725349301688395\n",
            "BLEU score -> 0.7259795291154771\n",
            "BLEU score -> 0.5432852079582625\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.3639412530979476\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.2025894847023147\n",
            "BLEU score -> 0.7311104457090247\n",
            "BLEU score -> 0.45782273986766686\n",
            "BLEU score -> 0.7071067811865476\n",
            "BLEU score -> 0.6084288535721682\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.4482700320176827\n",
            "BLEU score -> 0.5506953149031838\n",
            "BLEU score -> 0.5166357204442371\n",
            "BLEU score -> 0.31371436090802907\n",
            "BLEU score -> 0.537284965911771\n",
            "BLEU score -> 0.42888194248035344\n",
            "BLEU score -> 0.47897362544357464\n",
            "BLEU score -> 0.38827267775222324\n",
            "BLEU score -> 0.24092085663272753\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.43645382979233377\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.6223329772884784\n",
            "BLEU score -> 0.4671379777282001\n",
            "BLEU score -> 0.4128713178810563\n",
            "BLEU score -> 0.6337834876616586\n",
            "BLEU score -> 0.27952792741962756\n",
            "BLEU score -> 0.7952707287670506\n",
            "BLEU score -> 0.4760116549244004\n",
            "BLEU score -> 0.5623413251903491\n",
            "BLEU score -> 0.2947659609337933\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.14351442318493668\n",
            "BLEU score -> 0.5962494769762219\n",
            "BLEU score -> 0.6389431042462724\n",
            "BLEU score -> 0.5491004867761125\n",
            "BLEU score -> 0.5088274727401554\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0\n",
            "BLEU score -> 0.6147881529512643\n",
            "BLEU score -> 0.3237750138999811\n",
            "BLEU score -> 0.7952707287670506\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aw_q9VEoHjmu",
        "outputId": "4ef438db-5535-464b-f01e-c1b2b0152a94"
      },
      "source": [
        "print(\"BLEU score for the training set : \"+ str(training_average_bleu))\n",
        "print(\"BLEU score for the test set : \"+ str(test_average_bleu))"
      ],
      "id": "Aw_q9VEoHjmu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score for the training set : 0.5057138533013059\n",
            "BLEU score for the test set : 0.4577821393596426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gNJdVJvz7sF"
      },
      "source": [
        "Generating output from keyboard input"
      ],
      "id": "8gNJdVJvz7sF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNNHsa7z0IMw"
      },
      "source": [
        "def translate(sentence) :\n",
        "    # set the translation, remove the start and end tokens \n",
        "    translation = evaluate(sentence)[0][0][8:-5]\n",
        "    # capitalize the first letter\n",
        "    translation = translation.capitalize()\n",
        "    # capitalize i's\n",
        "    translation = translation.replace(\" i \", \" I \") \n",
        "    # remove spaces around punctuation\n",
        "    translation = translation.replace(\" . \", \".\")\n",
        "    translation = translation.replace(\" ,\", \",\")\n",
        "    translation = translation.replace(\" ? \", \"?\")\n",
        "    translation = translation.replace(\" ! \", \"!\")\n",
        "    translation = translation.replace(\" \\' \", \"\\'\")\n",
        "    translation = translation.replace(\" \\\" \", \"\\\"\")\n",
        "    return translation "
      ],
      "id": "UNNHsa7z0IMw",
      "execution_count": 2,
      "outputs": []
    }
  ]
}