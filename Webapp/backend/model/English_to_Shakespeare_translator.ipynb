{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "QL1vnxsLMH9F",
   "metadata": {
    "id": "QL1vnxsLMH9F"
   },
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qsjqyp7UMH9K",
   "metadata": {
    "id": "qsjqyp7UMH9K"
   },
   "outputs": [],
   "source": [
    "# --- H E R E   B E   I M P O R T S --- #\n",
    "#___________________________________________________________________________________\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "import collections\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "import time\n",
    "import requests\n",
    "import urllib.request\n",
    "import io\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from string import digits\n",
    "from collections import Counter\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy\n",
    "import math\n",
    "import random\n",
    "import fileinput\n",
    "from pickle import dump\n",
    "import pandas\n",
    "import statsmodels\n",
    "\n",
    "import numpy as np\n",
    "from numpy import array, asarray, zeros\n",
    "\n",
    "import unicodedata\n",
    "from unicodedata import normalize\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "#!pip install Keras\n",
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.data\n",
    "# Seq2Seq Items\n",
    "#import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell, MultiRNNCell\n",
    "#from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Embedding, Input, LSTM\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y3wS4CruMH9M",
   "metadata": {
    "id": "y3wS4CruMH9M"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], enable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5EraNrFbMH9O",
   "metadata": {
    "id": "5EraNrFbMH9O"
   },
   "source": [
    "Set up for tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j3MmA9gOMH9O",
   "metadata": {
    "id": "j3MmA9gOMH9O"
   },
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "num_words = None,\n",
    "filters='\"#$%&()*+-/:;=@[\\\\]^_`{|}~\\t\\n',\n",
    "lower=True,\n",
    "split=' ',\n",
    "char_level=False, #if True, every character is tokenized instead of word\n",
    "oov_token='eNiV')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r7HWcZvNMH9P",
   "metadata": {
    "id": "r7HWcZvNMH9P"
   },
   "source": [
    "Data scraping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bY-Kn04ZHm-T",
   "metadata": {
    "id": "bY-Kn04ZHm-T"
   },
   "outputs": [],
   "source": [
    "# uncomment these lines if you are using Google Colaboratory\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MfV_Vsq-MH9Q",
   "metadata": {
    "id": "MfV_Vsq-MH9Q"
   },
   "outputs": [],
   "source": [
    "# get rid of line breaks -- used by scraper\n",
    "def killbreaks(string):\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.replace('\\r', ' ')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zV-FNjiKMH9Q",
   "metadata": {
    "id": "zV-FNjiKMH9Q"
   },
   "outputs": [],
   "source": [
    "def sparknotesScraper(play, parity):\n",
    "    '''Writes a processed .txt file for the given shakespearean play on SparkNotes.\n",
    "        \n",
    "        Argument: play name as spelled on SparkNotes\n",
    "            ex: 'Romeo and Juliet' is romeojuliet\n",
    "    \n",
    "        Return: None\n",
    "        \n",
    "        Output: Two text files, play.txt and playCleaned.txt'''\n",
    "    # i is the current page, sparknotes counts most of them in even numbers\n",
    "    # which will be the training set while odd numbers will be used for the test set\n",
    "    if parity == \"even\" :\n",
    "        i=2\n",
    "    if parity == \"odd pt1\" :\n",
    "        i=3\n",
    "    if parity == \"odd pt2\" :\n",
    "        i=269\n",
    "        \n",
    "    print(\"Reading {0} from https://www.sparknotes.com/nofear/shakespeare/{0}/ ...\".format(play))\n",
    "    # makes a text file to which to copy <div> contents\n",
    "    with open('{0}.txt'.format(play),\"w+\",encoding=\"utf-8\")  as fp:\n",
    "    # comment the above line and uncomment the one below if you are using Google Colaboratory\n",
    "    #with open('/content/gdrive/MyDrive/{0}.txt'.format(play),\"w+\",encoding=\"utf-8\")  as fp:\n",
    "\n",
    "        # limit set at 500 pages, just to prevent infinite loop\n",
    "        #    in case break statement not triggered\n",
    "        while i <= 1000 :\n",
    "        \n",
    "            # 200 is the successful access status code. 300 are redirects\n",
    "            # and above is garbage, non-200 code means the page doesn't\n",
    "            # exist or is unreachable\n",
    "            head = requests.head(\"https://www.sparknotes.com/nofear/shakespeare/{0}/page_{1}/\".format(play,i))\n",
    "            if head.status_code >= 300:\n",
    "                print(\"End of visible play reached, {0} is last visible page at i={1}\".format(int(i/2),i))\n",
    "                break\n",
    "                \n",
    "            # start reading html content\n",
    "            # some <div>s contain linebreaks, killbreaks() gets rid of them\n",
    "            with urllib.request.urlopen(\"https://www.sparknotes.com/nofear/shakespeare/{0}/page_{1}/\".format(play,i)) as page:\n",
    "                soup = BeautifulSoup(page)\n",
    "                table = soup.find(\"table\")\n",
    "                rows = table.findAll(\"tr\")\n",
    "                for row in rows:\n",
    "                    for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--original\"}):\n",
    "                        for div in td.find_all(\"div\"):\n",
    "                            fp.write(killbreaks(div.text)+\" \")\n",
    "                        fp.write(\"\\n\")\n",
    "                    for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--modern\"}):\n",
    "                        for div in td.find_all(\"div\"):\n",
    "                            fp.write(killbreaks(div.text)+\" \")\n",
    "                        fp.write(\"\\n\\n\")\n",
    "            \n",
    "            if i%50 == 0:\n",
    "                print(\"Reached page {0} at i={1},\".format(int(i/2),i))\n",
    "            \n",
    "            i+=2\n",
    "            \n",
    "        print('\\'{0}\\' is read.'.format(play))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_1ZL0Du3MH9R",
   "metadata": {
    "id": "_1ZL0Du3MH9R"
   },
   "outputs": [],
   "source": [
    "# Plays\n",
    "sparknotesScraper('hamlet', \"even\")\n",
    "sparknotesScraper('macbeth', \"even\")\n",
    "sparknotesScraper('romeojuliet', \"even\")\n",
    "sparknotesScraper('lear', \"even\")\n",
    "sparknotesScraper('juliuscaesar', \"even\")\n",
    "sparknotesScraper('henryv', \"even\")\n",
    "sparknotesScraper('coriolanus', \"even\")\n",
    "sparknotesScraper('asyoulikeit', \"even\")\n",
    "sparknotesScraper('antony-and-cleopatra', \"even\")\n",
    "sparknotesScraper('measure-for-measure', \"even\")\n",
    "sparknotesScraper('errors', \"even\")\n",
    "sparknotesScraper('merchant', \"even\")\n",
    "sparknotesScraper('msnd', \"even\")\n",
    "sparknotesScraper('othello', \"even\")\n",
    "sparknotesScraper('richardii', \"even\")\n",
    "sparknotesScraper('richardiii', \"even\")\n",
    "sparknotesScraper('shrew', \"even\")\n",
    "sparknotesScraper('tempest', \"even\")\n",
    "sparknotesScraper('twelfthnight', \"even\")\n",
    "sparknotesScraper('twogentlemen', \"even\")\n",
    "sparknotesScraper('winterstale', \"even\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7TPiU8Q2MH9R",
   "metadata": {
    "id": "7TPiU8Q2MH9R"
   },
   "outputs": [],
   "source": [
    "# store all the even indexed plays in one file for trainind\n",
    "filenames = ['hamlet.txt', 'romeojuliet.txt', 'lear.txt', 'juliuscaesar.txt', 'henryv.txt', 'coriolanus.txt', 'errors.txt', 'asyoulikeit.txt','antony-and-cleopatra.txt', 'measure-for-measure.txt', 'merchant.txt', 'msnd.txt', 'othello.txt', 'richardii.txt', 'richardiii.txt', 'shrew.txt', 'tempest.txt', 'twelfthnight.txt', 'twogentlemen.txt', 'winterstale.txt']\n",
    "with open('trainingSet.txt', 'w', encoding=\"utf-8\") as outfile:\n",
    "# comment the above line and uncomment the one below if you are using Google Colaboratory\n",
    "#with open('/content/gdrive/MyDrive/trainingSet.txt', 'w', encoding=\"utf-8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf-8\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k3gzV5tRMH9S",
   "metadata": {
    "id": "k3gzV5tRMH9S"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Iu56r1MiMH9T",
   "metadata": {
    "id": "Iu56r1MiMH9T"
   },
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w\n",
    "\n",
    "def load_document(filename):\n",
    "    #open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    #store in variable  \n",
    "    text=file.read()\n",
    "    #close it\n",
    "    file.close\n",
    "    return text\n",
    "\n",
    "def make_pairs(document):\n",
    "    #group the corresponding lines\n",
    "    to_be_paired = document.split('\\n\\n')\n",
    "    #pair the corresponding translations\n",
    "    line_pairs = [line.split('\\n') for line in to_be_paired]\n",
    "    return line_pairs\n",
    "\n",
    "#---dataset cleaning------\n",
    "##Note that this is partially done by the tokenizer already\n",
    "##however, we implemented our own version Before using that\n",
    "##tokenizer, so we keep both for compatibility just in case\n",
    "def clean(pairs, num_sentences):\n",
    "    if num_sentences == 'all':\n",
    "        num_sentences = len(pairs)-1\n",
    "    cleaned= []\n",
    "    # regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    for pair in pairs[:num_sentences]:\n",
    "        clean_pair = [] \n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [re_print.sub('', word) for word in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word.translate(str.maketrans('', '', digits)) for word in line]\n",
    "            # add space before punctuation\n",
    "            line = [re.sub('([.,!?()])', r' \\1 ', word) for word in line]\n",
    "            # remove extra spaces\n",
    "            line = [re.sub(' +', ' ', word) for word in line]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        # keep only pairs of 2\n",
    "        if len(clean_pair) == 2:\n",
    "            # split into sentences instead of lines\n",
    "            if len(nltk.tokenize.sent_tokenize(clean_pair[0])) > 1 :\n",
    "                first = nltk.tokenize.sent_tokenize(clean_pair[0])\n",
    "                second = nltk.tokenize.sent_tokenize(clean_pair[1])\n",
    "                # add a start and an end token to the sentences\n",
    "                first = ['<start> ' + sentence + ' <end>' for sentence in first]\n",
    "                second = ['<start> ' + sentence + ' <end>' for sentence in second]\n",
    "                length = min(len(first), len(second))\n",
    "                i=0\n",
    "                while i < length :\n",
    "                # keep only short sentences and verify the lengths are about the same\n",
    "                    if len(first[i]) < max_length +15 and (2.5*len(first[i]) > len(second[i]) or 2.5*len(first[i]) > len(second[i])):\n",
    "                         cleaned.append([first[i],second[i]])\n",
    "                    i+=1\n",
    "            else :\n",
    "                # keep only short sentences\n",
    "                if len(clean_pair[0]) < max_length :\n",
    "                    # add a start and an end token to the sentences\n",
    "                    clean_pair = ['<start> ' + sentence + ' <end>' for sentence in clean_pair]\n",
    "                    cleaned.append(clean_pair)\n",
    "    #print(cleaned)\n",
    "    return zip(*cleaned)\n",
    "\n",
    "def load_dataset(path, num_sentences):\n",
    "    # load dataset\n",
    "    doc = load_document(path)\n",
    "    # split into pairs\n",
    "    pairs = make_pairs(doc)\n",
    "    #clean sentences\n",
    "    cleaned_pairs = clean(pairs, num_sentences)\n",
    "    # associate sentences to their respective language \n",
    "    shakespeare, modern = cleaned_pairs\n",
    "    # make a vocabulary and fit the tokenizer on it\n",
    "    vocabularySource = shakespeare + modern\n",
    "    tokenizer.fit_on_texts(vocabularySource)\n",
    "    # vectorize\n",
    "    modern_vector = pad(tokenize(modern))\n",
    "    shakespeare_vector = pad(tokenize(shakespeare))\n",
    "    \n",
    "    return modern_vector, shakespeare_vector, vocabularySource\n",
    "\n",
    "def pad(language_vector):\n",
    "    for sentence in language_vector:\n",
    "        # add 0s to sentence vector until it is the same length as max_length\n",
    "        for i in range(len(sentence), max_length):\n",
    "            sentence.append(0) \n",
    "    return language_vector\n",
    "\n",
    "def tokenize(array):\n",
    "    token = tokenizer.texts_to_sequences(array)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZVQD8LL4MH9T",
   "metadata": {
    "id": "ZVQD8LL4MH9T"
   },
   "outputs": [],
   "source": [
    "# run this line if you are using jupyter\n",
    "modern_vector, shakespeare_vector, vocabularySource = load_dataset('trainingSet.txt', 'all')\n",
    "# comment the above line and uncomment the one below if you are using Google Colaboratory\n",
    "#modern_vector, shakespeare_vector, vocabularySource = load_dataset('/content/gdrive/MyDrive/trainingSet.txt', 'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tuub-5IjMH9T",
   "metadata": {
    "id": "Tuub-5IjMH9T"
   },
   "source": [
    "-------------------------------------------\n",
    "The vast majority of what follows is unoriginal -- we were authorized to make use of pre-built libraries such as scikit-learn, but to our knowledge, there exists no ready-made transformer translator, and we were (rather unsurprisingly) unable to create our own.\n",
    "\n",
    "The principal source is thus: https://www.tensorflow.org/tutorials/text/transformer\n",
    "\n",
    "The modifications, where they occur, are mostly for dataset compatibility -- ours and the tutorials' were different and we did not have the same capabilities or built-in functions such as tokenizers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OCwM7knmMH9T",
   "metadata": {
    "id": "OCwM7knmMH9T"
   },
   "source": [
    "Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modular-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(np.arange(position)[:, np.newaxis], np.arange(d_model)[np.newaxis, :], d_model)\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v6zIsuGYMH9U",
   "metadata": {
    "id": "v6zIsuGYMH9U"
   },
   "source": [
    "Padding Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mv1Tg3RJMH9V",
   "metadata": {
    "id": "mv1Tg3RJMH9V"
   },
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, -1), tf.float32)\n",
    "    # add extra dimensions to add the padding to the attention logits.\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]  # (batch_size, 1, 1, seq_len)\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QBVrQ6haMH9V",
   "metadata": {
    "id": "QBVrQ6haMH9V"
   },
   "source": [
    "Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lPL-q89mMH9V",
   "metadata": {
    "id": "lPL-q89mMH9V"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    \"\"\"Calculate the attention weights.\n",
    "      q, k, v must have matching leading dimensions.\n",
    "      k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
    "      The mask has different shapes depending on its type(padding or look ahead) \n",
    "      but it must be broadcastable for addition.\n",
    "\n",
    "      Args:\n",
    "        q: query shape == (..., seq_len_q, depth)\n",
    "        k: key shape == (..., seq_len_k, depth)\n",
    "        v: value shape == (..., seq_len_v, depth_v)\n",
    "        mask: Float tensor with shape broadcastable \n",
    "              to (..., seq_len_q, seq_len_k). Defaults to None.\n",
    "\n",
    "      Returns:\n",
    "        output, attention_weights\n",
    "    \"\"\"\n",
    "\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    # scale matmul_qk\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    # add the mask to the scaled tensor.\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
    "    # add up to 1.\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)  # (..., seq_len_q, seq_len_k)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D9IQQLDpMH9V",
   "metadata": {
    "id": "D9IQQLDpMH9V"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        \"\"\"Split the last dimension into (num_heads, depth).\n",
    "        Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth)\n",
    "        \"\"\"\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
    "        k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
    "        v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        q = self.split_heads(q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
    "        k = self.split_heads(k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
    "        v = self.split_heads(v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
    "\n",
    "        # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
    "        # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        output = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r76ueoJUMH9W",
   "metadata": {
    "id": "r76ueoJUMH9W"
   },
   "source": [
    "Encoder and Decoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Dui8VlQrMH9W",
   "metadata": {
    "id": "Dui8VlQrMH9W"
   },
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
    "        tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t95_HE4FMH9W",
   "metadata": {
    "id": "t95_HE4FMH9W"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        attn_output, _ = self.mha(x, x, x, mask)  # (batch_size, input_seq_len, d_model)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # (batch_size, input_seq_len, d_model)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)  # (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0xN2YsWPMH9X",
   "metadata": {
    "id": "0xN2YsWPMH9X"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        # enc_output.shape == (batch_size, input_seq_len, d_model)\n",
    "\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)  # (batch_size, target_seq_len, d_model)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        ffn_output = self.ffn(out2)  # (batch_size, target_seq_len, d_model)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)  # (batch_size, target_seq_len, d_model)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GCYyHITSMH9X",
   "metadata": {
    "id": "GCYyHITSMH9X"
   },
   "source": [
    "Encoder and Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nd6g3AdzMH9X",
   "metadata": {
    "id": "Nd6g3AdzMH9X"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(input_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, training, mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "\n",
    "        # adding embedding and position encoding.\n",
    "        x = self.embedding(x)  # (batch_size, input_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "\n",
    "        return x  # (batch_size, input_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vjx4Y8LAMH9X",
   "metadata": {
    "id": "vjx4Y8LAMH9X"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
    "                           for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)  # (batch_size, target_seq_len, d_model)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "\n",
    "        # x.shape == (batch_size, target_seq_len, d_model)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mTzdX5A5MH9Y",
   "metadata": {
    "id": "mTzdX5A5MH9Y"
   },
   "source": [
    "Transformer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yb6_Ll1CMH9Y",
   "metadata": {
    "id": "Yb6_Ll1CMH9Y"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.tokenizer = Encoder(num_layers, d_model, num_heads, dff,  input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "\n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "\n",
    "        enc_output = self.tokenizer(inp, training, enc_padding_mask)  # (batch_size, inp_seq_len, d_model)\n",
    "\n",
    "        # dec_output.shape == (batch_size, tar_seq_len, d_model)\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)  # (batch_size, tar_seq_len, target_vocab_size)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NjomNer_MH9Z",
   "metadata": {
    "id": "NjomNer_MH9Z"
   },
   "outputs": [],
   "source": [
    "#Settings/parameters and model initialization\n",
    "#While mostly arbitrary, these hyperparameters\n",
    "#were inspired by the Attention is All You Need\n",
    "#paper\n",
    "num_layers = 6\n",
    "d_model = 128\n",
    "dff = 1024\n",
    "num_heads = 16\n",
    "dropout_rate = 0.2\n",
    "\n",
    "transformer = Transformer( num_layers=num_layers,\n",
    "                           d_model=d_model,\n",
    "                           num_heads=num_heads,\n",
    "                           dff=dff,\n",
    "                           input_vocab_size=len(tokenizer.word_index)+1,\n",
    "                           target_vocab_size=len(tokenizer.word_index)+1, \n",
    "                           pe_input=1000, \n",
    "                           pe_target=1000,\n",
    "                           rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mXGUxLHqMH9Z",
   "metadata": {
    "id": "mXGUxLHqMH9Z"
   },
   "source": [
    "Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1FAYRvnPMH9Z",
   "metadata": {
    "id": "1FAYRvnPMH9Z"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "    \n",
    "learning_rate = CustomSchedule(d_model)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xj577DMEMH9Z",
   "metadata": {
    "id": "xj577DMEMH9Z"
   },
   "source": [
    "Loss and accuracy functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FXpZE819MH9Z",
   "metadata": {
    "id": "FXpZE819MH9Z"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "\n",
    "def accuracy_function(real, pred):\n",
    "    accuracies = tf.equal(real, tf.argmax(pred, axis=2))\n",
    "\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    accuracies = tf.math.logical_and(mask, accuracies)\n",
    "\n",
    "    accuracies = tf.cast(accuracies, dtype=tf.float32)\n",
    "    mask = tf.cast(mask, dtype=tf.float32)\n",
    "    return tf.reduce_sum(accuracies)/tf.reduce_sum(mask)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rOZolh3pMH9a",
   "metadata": {
    "id": "rOZolh3pMH9a"
   },
   "outputs": [],
   "source": [
    "def create_masks(inp, tar):\n",
    "    # Encoder padding mask\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 2nd attention block in the decoder.\n",
    "    # This padding mask is used to mask the encoder outputs.\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    # Used in the 1st attention block in the decoder.\n",
    "    # It is used to pad and mask future tokens in the input received by \n",
    "    # the decoder.\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "\n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VrdOo-9gMH9b",
   "metadata": {
    "id": "VrdOo-9gMH9b"
   },
   "outputs": [],
   "source": [
    "tensormodEngENC = tf.convert_to_tensor(modern_vector, dtype=tf.int64)\n",
    "tensorshakeEngENC = tf.convert_to_tensor(shakespeare_vector, dtype=tf.int64)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((tensormodEngENC,tensorshakeEngENC))\n",
    "\n",
    "#This function is technically useless, but included for\n",
    "#compatibility -- the tutorial source we used does more\n",
    "#with it, but it is not needed in our implementation.\n",
    "#Unfortunately, it is called in a complicated (for us) way.\n",
    "def tokenize_pairs(sha, mod):\n",
    "    return sha, mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DTnZBE95MH9b",
   "metadata": {
    "id": "DTnZBE95MH9b"
   },
   "outputs": [],
   "source": [
    "#Partition dataset into batches for training\n",
    "BUFFER_SIZE = 20000\n",
    "BATCH_SIZE = 100\n",
    "def make_batches(ds):\n",
    "    return (\n",
    "        ds\n",
    "        .cache()\n",
    "        .shuffle(BUFFER_SIZE)\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(tokenize_pairs, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "dset = make_batches(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdlWh_oMMH9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdlWh_oMMH9b",
    "outputId": "88f2bf6e-0f30-4b33-d1b5-b928d01e2a7f"
   },
   "outputs": [],
   "source": [
    "#Weights are saved as checkpoints\n",
    "checkpoint_path = \"/content/gdrive/MyDrive/checkpoints_new/train\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "# if a checkpoint exists, restore the latest checkpoint.\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FmGmpBDzMH9b",
   "metadata": {
    "id": "FmGmpBDzMH9b"
   },
   "source": [
    "Training - do not run unless ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xggGuzh1MH9b",
   "metadata": {
    "id": "xggGuzh1MH9b"
   },
   "outputs": [],
   "source": [
    "#Setup for training\n",
    "EPOCHS = 600   # <<<================||X+\n",
    "\n",
    "train_step_signature = [\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "    tf.TensorSpec(shape=(None, None), dtype=tf.int64),\n",
    "]\n",
    "\n",
    "@tf.function(input_signature=train_step_signature)\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer(inp, tar_inp, \n",
    "                                         True, \n",
    "                                         enc_padding_mask, \n",
    "                                         combined_mask, \n",
    "                                         dec_padding_mask)\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(accuracy_function(tar_real, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QiKZMfGrMH9b",
   "metadata": {
    "id": "QiKZMfGrMH9b"
   },
   "outputs": [],
   "source": [
    "# --- T R A I N I N G --- #\n",
    "# GPUs BEWARE\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    # inp -> english, tar -> shakespearean\n",
    "    for (batch, (inp, tar)) in enumerate(dset):\n",
    "        train_step(inp, tar)\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print (f'Saving checkpoint for epoch {epoch+1} at {ckpt_save_path}')\n",
    "\n",
    "    print(f'Epoch {epoch + 1} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
    "    print(f'Time taken for 1 epoch: {time.time() - start:.2f} secs\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BD7LrJckMH9c",
   "metadata": {
    "id": "BD7LrJckMH9c"
   },
   "source": [
    "Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MkR0h2yxMH9c",
   "metadata": {
    "id": "MkR0h2yxMH9c"
   },
   "outputs": [],
   "source": [
    "#The transformer functions by building an output \n",
    "#word-by-word, starting from the <start> token\n",
    "#that signifies the beginning of a sentence.\n",
    "#There is a significant number of unused content\n",
    "#in this method. We were not able to implement\n",
    "#these before the D3 submission deadline.\n",
    "def evaluate(sentence):\n",
    "    sentence = tokenizer.texts_to_sequences([sentence])\n",
    "    difference = max_length - len(sentence[0])\n",
    "    while difference > 0:\n",
    "        sentence[0].append(0)\n",
    "        difference = difference - 1\n",
    "    encoder_input = tf.convert_to_tensor(sentence)\n",
    "    encoder_input = tf.cast(encoder_input, tf.int32)\n",
    "    start = 2\n",
    "    end = 3\n",
    "    output = [2]\n",
    "    output = tf.convert_to_tensor([output])\n",
    "    output = tf.cast(output, tf.int32)\n",
    "    for i in range(max_length):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "        predictions, attention_weights = transformer(encoder_input, \n",
    "                                                     output,\n",
    "                                                     False,\n",
    "                                                     enc_padding_mask,\n",
    "                                                     combined_mask,\n",
    "                                                     dec_padding_mask)\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.argmax(predictions, axis=-1)\n",
    "        predicted_id = tf.cast(predicted_id, tf.int32)\n",
    "        output = tf.cast(output, tf.int32)\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "        if predicted_id == end:\n",
    "            break\n",
    "    arrayn = output.numpy()\n",
    "    text = tokenizer.sequences_to_texts(arrayn)\n",
    "\n",
    "    return text, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "PYnkaCmFMH9f",
   "metadata": {
    "id": "PYnkaCmFMH9f"
   },
   "source": [
    "Sample outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HLPOlemLOTlX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HLPOlemLOTlX",
    "outputId": "cff4e975-3cc4-4d02-95a4-4eb34058bd3a"
   },
   "outputs": [],
   "source": [
    "sentence = \"<start> i swear to god , i am exceedingly tired . <end>\"\n",
    "print(evaluate(sentence)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ydwfs590O1Dr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ydwfs590O1Dr",
    "outputId": "c75127d4-cf0d-4ee0-d063-8065f95a354f"
   },
   "outputs": [],
   "source": [
    "sentence = \"<start> hello , how are you ? <end>\"\n",
    "print(evaluate(sentence)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5deJYNsQNpl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o5deJYNsQNpl",
    "outputId": "dc8381a8-7b5b-4dce-f712-198671a0b649"
   },
   "outputs": [],
   "source": [
    "sentence = \"<start> i do not speak very well . <end>\"\n",
    "print(evaluate(sentence)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_yrMjb2pMH9g",
   "metadata": {
    "id": "_yrMjb2pMH9g"
   },
   "source": [
    "BLEU score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E7xTLgI2MH9h",
   "metadata": {
    "id": "E7xTLgI2MH9h"
   },
   "outputs": [],
   "source": [
    "# creating a test set with the plays that have odd indices (henry4pt1 and henry4pt2)\n",
    "sparknotesScraper('henry4pt1', \"odd pt1\")\n",
    "sparknotesScraper('henry4pt2', \"odd pt2\")\n",
    "\n",
    "filenames = ['henry4pt1.txt', 'henry4pt2.txt']\n",
    "with open('testSet.txt', 'w', encoding=\"utf-8\") as outfile:\n",
    "# comment the above line and uncomment the one below if you are using Google Colaboratory\n",
    "#with open('/content/gdrive/MyDrive/testSet.txt', 'w', encoding=\"utf-8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf-8\") as infile:\n",
    "        # comment the above line and uncomment the one below if you are using Google Colaboratory\n",
    "        #with open('/content/gdrive/MyDrive/{}'.format(fname), encoding=\"utf-8\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9gDttMcoMH9h",
   "metadata": {
    "id": "9gDttMcoMH9h"
   },
   "outputs": [],
   "source": [
    "# Computing the BLEU score for the test set\n",
    "\n",
    "# load dataset\n",
    "test_doc = load_document('testSet.txt')\n",
    "# comment the above line and uncomment the one below if you are using Google Colaboratory\n",
    "# test_doc = load_document('/content/gdrive/MyDrive/testSet.txt')\n",
    "# split into pairs\n",
    "test_pairs = make_pairs(test_doc)\n",
    "#clean sentences\n",
    "cleaned_test_pairs = clean(test_pairs, 'all')\n",
    "#associate sentences to their respective language \n",
    "input_test, target_test = cleaned_test_pairs\n",
    "\n",
    "sum_bleu = 0\n",
    "\n",
    "for i in range(len(input_test)) :\n",
    "    # set our translation and target\n",
    "    sentence = evaluate(input_test[i])[0]\n",
    "    target = [target_test[i]]\n",
    "    # split by words, remove the start and end tokens \n",
    "    sentence = [ word.split() for word in sentence ][0][1:-1]\n",
    "    target = [ word.split() for word in target ][0][1:-1]\n",
    "    # compute bleu score\n",
    "    curr_bleu = sentence_bleu([target],sentence)\n",
    "    print('BLEU score -> {}'.format(curr_bleu))\n",
    "    sum_bleu += curr_bleu\n",
    "\n",
    "test_average_bleu = sum_bleu/len(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZwaneB8KvEg4",
   "metadata": {
    "id": "ZwaneB8KvEg4"
   },
   "outputs": [],
   "source": [
    "# Computing the BLEU score on a small batch of the training set\n",
    "\n",
    "# load dataset\n",
    "training_doc = load_document('trainingSet.txt')\n",
    "# comment the above line and uncomment the one below if you are using Google Colaboratory\n",
    "#training_doc = load_document('/content/gdrive/MyDrive/text_files/trainingSet.txt')\n",
    "# split into pairs\n",
    "training_pairs = make_pairs(training_doc)\n",
    "#clean sentences\n",
    "cleaned_training_pairs = clean(training_pairs, 'all')\n",
    "#associate sentences to their respective language \n",
    "input_training, target_training = cleaned_training_pairs\n",
    "\n",
    "sum_bleu = 0\n",
    "\n",
    "for i in range(500) :\n",
    "    # set our translation and target\n",
    "    sentence = evaluate(input_training[i])[0]\n",
    "    target = [target_training[i]]\n",
    "    # split by words, remove the start and end tokens \n",
    "    sentence = [ word.split() for word in sentence ][0][1:-1]\n",
    "    target = [ word.split() for word in target ][0][1:-1]\n",
    "    # compute bleu score\n",
    "    curr_bleu = sentence_bleu([target],sentence)\n",
    "    print('BLEU score -> {}'.format(curr_bleu))\n",
    "    sum_bleu += curr_bleu\n",
    "\n",
    "training_average_bleu = sum_bleu/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Aw_q9VEoHjmu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aw_q9VEoHjmu",
    "outputId": "4ef438db-5535-464b-f01e-c1b2b0152a94"
   },
   "outputs": [],
   "source": [
    "print(\"BLEU score for the training set : \"+ str(training_average_bleu))\n",
    "print(\"BLEU score for the test set : \"+ str(test_average_bleu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8gNJdVJvz7sF",
   "metadata": {
    "id": "8gNJdVJvz7sF"
   },
   "source": [
    "Generating output from keyboard input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UNNHsa7z0IMw",
   "metadata": {
    "id": "UNNHsa7z0IMw"
   },
   "outputs": [],
   "source": [
    "def translate(sentence) :\n",
    "    # set the translation, remove the start and end tokens \n",
    "    translation = evaluate(preprocess_sentence(sentence))[0][0][8:-5]\n",
    "    # capitalize the first letter\n",
    "    translation = translation.capitalize()\n",
    "    # capitalize i's\n",
    "    translation = translation.replace(\" i \", \" I \") \n",
    "    # remove spaces around punctuation\n",
    "    translation = translation.replace(\" . \", \".\")\n",
    "    translation = translation.replace(\" ,\", \",\")\n",
    "    translation = translation.replace(\" ? \", \"?\")\n",
    "    translation = translation.replace(\" ! \", \"!\")\n",
    "    translation = translation.replace(\" \\' \", \"\\'\")\n",
    "    translation = translation.replace(\" \\\" \", \"\\\"\")\n",
    "    return translation "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "English_to_Shakespeare_translator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
