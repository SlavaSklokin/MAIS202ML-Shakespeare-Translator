{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install Keras\n",
    "#!pip install tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from matplotlib import pylab\n",
    "from collections import Counter\n",
    "import csv\n",
    "\n",
    "# Seq2Seq Items\n",
    "#import tensorflow.contrib.seq2seq as seq2seq\n",
    "from tensorflow.python.ops.rnn_cell import LSTMCell\n",
    "from tensorflow.python.ops.rnn_cell import MultiRNNCell\n",
    "\n",
    "#from tensorflow.contrib.seq2seq.python.ops import attention_wrapper\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "import tensorflow\n",
    "\n",
    "import numpy\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "import requests\n",
    "import urllib.request\n",
    "#from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy\n",
    "import matplotlib\n",
    "import pandas\n",
    "import statsmodels\n",
    "import sklearn\n",
    "#import theano\n",
    "\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def killbreaks(string):\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.replace('\\r', ' ')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparknotesScraper(play):\n",
    "    '''Writes a processed .txt file for the given shakespearean play on SparkNotes.\n",
    "    \n",
    "        Argument: play name as spelled on SparkNotes\n",
    "            ex: 'Romeo and Juliet' is romeojuliet\n",
    "        \n",
    "        Return: None\n",
    "        \n",
    "        Output: Two text files, play.txt and playCleaned.txt'''\n",
    "    \n",
    "    # i is the current page, sparknotes counts them in even numbers\n",
    "    i=2\n",
    "    \n",
    "    print(\"Reading {0} from https://www.sparknotes.com/nofear/shakespeare/{0}/ ...\".format(play))\n",
    "    \n",
    "    # makes a text file to which to copy <div> contents\n",
    "    with open('{0}.txt'.format(play),\"w+\",encoding=\"utf-8\")  as fp:\n",
    "        \n",
    "        # limit set at 500 pages, just to prevent infinite loop\n",
    "        #    in case break statement not triggered\n",
    "        while i <= 1000 :\n",
    "            \n",
    "            # 200 is the successful access status code. 300 are redirects\n",
    "            # and above is garbage, non-200 code means the page doesn't\n",
    "            # exist or is unreachable\n",
    "            head = requests.head(\"https://www.sparknotes.com/nofear/shakespeare/{0}/page_{1}/\".format(play,i))\n",
    "            if head.status_code >= 300:\n",
    "                print(\"End of visible play reached, {0} is last visible page at i={1}\".format(int(i/2),i))\n",
    "                break\n",
    "                \n",
    "            # start reading html content\n",
    "            # some <div>s contain linebreaks, killbreaks() gets rid of them\n",
    "            with urllib.request.urlopen(\"https://www.sparknotes.com/nofear/shakespeare/{0}/page_{1}/\".format(play,i)) as page:\n",
    "                soup = BeautifulSoup(page)\n",
    "                table = soup.find(\"table\")\n",
    "                rows = table.findAll(\"tr\")\n",
    "                for row in rows:\n",
    "                    for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--original\"}):\n",
    "                        for div in td.find_all(\"div\"):\n",
    "                            fp.write(killbreaks(div.text)+\" \")\n",
    "                        fp.write(\"\\n\")\n",
    "                    for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--modern\"}):\n",
    "                        for div in td.find_all(\"div\"):\n",
    "                            fp.write(killbreaks(div.text)+\" \")\n",
    "                        fp.write(\"\\n\\n\")\n",
    "            \n",
    "            if i%50 == 0:\n",
    "                print(\"Reached page {0} at i={1},\".format(int(i/2),i))\n",
    "            \n",
    "            i+=2\n",
    "            \n",
    "        print('\\'{0}\\' is read.'.format(play))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading hamlet from https://www.sparknotes.com/nofear/shakespeare/hamlet/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "Reached page 150 at i=300,\n",
      "End of visible play reached, 169 is last visible page at i=338\n",
      "'hamlet' is read.\n",
      "Reading macbeth from https://www.sparknotes.com/nofear/shakespeare/macbeth/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "End of visible play reached, 110 is last visible page at i=220\n",
      "'macbeth' is read.\n",
      "Reading romeojuliet from https://www.sparknotes.com/nofear/shakespeare/romeojuliet/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "End of visible play reached, 144 is last visible page at i=288\n",
      "'romeojuliet' is read.\n"
     ]
    }
   ],
   "source": [
    "sparknotesScraper('hamlet')\n",
    "sparknotesScraper('macbeth')\n",
    "sparknotesScraper('romeojuliet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lear from https://www.sparknotes.com/nofear/shakespeare/lear/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "Reached page 150 at i=300,\n",
      "End of visible play reached, 156 is last visible page at i=312\n",
      "'lear' is read.\n",
      "Reading juliuscaesar from https://www.sparknotes.com/nofear/shakespeare/juliuscaesar/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "End of visible play reached, 120 is last visible page at i=240\n",
      "'juliuscaesar' is read.\n",
      "Reading henry4pt1 from https://www.sparknotes.com/nofear/shakespeare/henry4pt1/ ...\n",
      "End of visible play reached, 1 is last visible page at i=2\n",
      "'henry4pt1' is read.\n",
      "Reading henry4pt2 from https://www.sparknotes.com/nofear/shakespeare/henry4pt2/ ...\n",
      "End of visible play reached, 1 is last visible page at i=2\n",
      "'henry4pt2' is read.\n",
      "Reading henryv from https://www.sparknotes.com/nofear/shakespeare/henryv/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "End of visible play reached, 139 is last visible page at i=278\n",
      "'henryv' is read.\n",
      "Reading coriolanus from https://www.sparknotes.com/nofear/shakespeare/coriolanus/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "Reached page 150 at i=300,\n",
      "End of visible play reached, 168 is last visible page at i=336\n",
      "'coriolanus' is read.\n",
      "Reading asyoulikeit from https://www.sparknotes.com/nofear/shakespeare/asyoulikeit/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "End of visible play reached, 122 is last visible page at i=244\n",
      "'asyoulikeit' is read.\n",
      "Reading antony-and-cleopatra from https://www.sparknotes.com/nofear/shakespeare/antony-and-cleopatra/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "Reached page 150 at i=300,\n",
      "Reached page 175 at i=350,\n",
      "End of visible play reached, 177 is last visible page at i=354\n",
      "'antony-and-cleopatra' is read.\n",
      "Reading measure-for-measure from https://www.sparknotes.com/nofear/shakespeare/measure-for-measure/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "End of visible play reached, 128 is last visible page at i=256\n",
      "'measure-for-measure' is read.\n",
      "Reading errors from https://www.sparknotes.com/nofear/shakespeare/errors/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "End of visible play reached, 82 is last visible page at i=164\n",
      "'errors' is read.\n",
      "Reading merchant from https://www.sparknotes.com/nofear/shakespeare/merchant/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "End of visible play reached, 116 is last visible page at i=232\n",
      "'merchant' is read.\n",
      "Reading msnd from https://www.sparknotes.com/nofear/shakespeare/msnd/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "End of visible play reached, 91 is last visible page at i=182\n",
      "'msnd' is read.\n",
      "Reading othello from https://www.sparknotes.com/nofear/shakespeare/othello/ ...\n",
      "Reached page 25 at i=50,\n",
      "Reached page 50 at i=100,\n",
      "Reached page 75 at i=150,\n",
      "Reached page 100 at i=200,\n",
      "Reached page 125 at i=250,\n",
      "Reached page 150 at i=300,\n",
      "End of visible play reached, 154 is last visible page at i=308\n",
      "'othello' is read.\n"
     ]
    }
   ],
   "source": [
    "sparknotesScraper('lear')\n",
    "sparknotesScraper('juliuscaesar')\n",
    "sparknotesScraper('henry4pt1')\n",
    "sparknotesScraper('henry4pt2')\n",
    "sparknotesScraper('henryv')\n",
    "sparknotesScraper('coriolanus')\n",
    "sparknotesScraper('asyoulikeit')\n",
    "sparknotesScraper('antony-and-cleopatra')\n",
    "sparknotesScraper('measure-for-measure')\n",
    "sparknotesScraper('errors')\n",
    "sparknotesScraper('merchant')\n",
    "sparknotesScraper('msnd')\n",
    "sparknotesScraper('othello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to add : henry4pt1 and henry4pt2 (odd indices)\n",
    "filenames = ['hamlet.txt', 'romeojuliet.txt', 'lear.txt', 'juliuscaesar.txt', 'henryv.txt', 'coriolanus.txt', 'errors.txt', 'asyoulikeit.txt','antony-and-cleopatra.txt', 'measure-for-measure.txt', 'merchant.txt', 'msnd.txt', 'othello.txt']\n",
    "with open('trainingSet.txt', 'w', encoding=\"utf-8\") as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname, encoding=\"utf-8\") as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset pairing and cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts the unicode file to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def preprocess_sentence(w):\n",
    "    w = unicode_to_ascii(w.lower().strip())\n",
    "\n",
    "    # creating a space between a word and the punctuation following it\n",
    "    # eg: \"he is a boy.\" => \"he is a boy .\"\n",
    "    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
    "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
    "    w = re.sub(r'[\" \"]+', \" \", w)\n",
    "\n",
    "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
    "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
    "\n",
    "    w = w.strip()\n",
    "\n",
    "    # adding a start and an end token to the sentence\n",
    "    # so that the model know when to start and stop predicting.\n",
    "    w = '<start> ' + w + ' <end>'\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(filename):\n",
    "    #open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    #store in variable  \n",
    "    text=file.read()\n",
    "    #close it\n",
    "    file.close\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(document):\n",
    "    #group the corresponding lines\n",
    "    to_be_paired = document.split('\\n\\n')\n",
    "    #pair the corresponding translations\n",
    "    line_pairs = [line.split('\\n') for line in to_be_paired]\n",
    "    #pair the corresponding sentences\n",
    "    #sentece_pairs = [line.split('.') for line in  lines]\n",
    "    return line_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---dataset cleaning------\n",
    "def clean(pairs, num_sentences):\n",
    "    if num_sentences == 'all':\n",
    "        num_sentences = len(pairs)-1\n",
    "    cleaned= []\n",
    "    # regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in pairs[:num_sentences]:\n",
    "        clean_pair = [] \n",
    "        # remove every second sentence\n",
    "        pair = [sentence.split('.', 1)[0] for sentence in pair]\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', word) for word in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            #remove extra spaces\n",
    "            line = [re.sub(' +', ' ', word) for word in line]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        # add a start and an end token to the sentences\n",
    "        clean_pair = ['<start> ' + sentence + ' <end>' for sentence in clean_pair]\n",
    "        if len(clean_pair[0]) < 40:    \n",
    "            cleaned.append(clean_pair)\n",
    "    #return cleaned, an iterator of the pairs\n",
    "    return zip(*cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the sentences\n",
    "def tokenize(lang):\n",
    "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "    lang_tokenizer.fit_on_texts(lang)\n",
    "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,padding='post')\n",
    "    return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to vector\n",
    "def convert(lang, tensor):\n",
    "    for t in tensor:\n",
    "        if t!=0:\n",
    "            print (\"%d ----> %s\" % (t, lang.index_word[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, num_sentences):#=None):\n",
    "    # load dataset\n",
    "    doc = load_document(path)\n",
    "    # split into pairs\n",
    "    pairs = make_pairs(doc)\n",
    "    # clean sentences\n",
    "    cleaned_pairs = clean(pairs, num_sentences) #cleaned_pairs[0] : shakespearean English\n",
    "                                                #cleaned_pairs[1] : modern English \n",
    "    # save clean pairs to file  \n",
    "    #save_clean_data(cleaned_pairs, 'translation-clean.pkl')\n",
    "    \n",
    "    targ_lang, inp_lang = cleaned_pairs\n",
    "    \n",
    "    # creating cleaned input, output pairs\n",
    "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "    \n",
    "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-202-6318f428b1b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Try experimenting with the size of that dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mnum_examples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'all'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0minput_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarg_lang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'trainingSet.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Calculate max_length of the target tensors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-201-390a2ab167da>\u001b[0m in \u001b[0;36mload_dataset\u001b[1;34m(path, num_sentences)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mpairs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m# clean sentences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mtarg_lang\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp_lang\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_sentences\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#cleaned_pairs[0] : shakespearean English\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m                                                 \u001b[1;31m#cleaned_pairs[1] : modern English\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m# save clean pairs to file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# Try experimenting with the size of that dataset\n",
    "num_examples = 'all'\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset('trainingSet.txt', num_examples)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]\n",
    "\n",
    "# Creating training and vhttp://localhost:8888/notebooks/Desktop/WINTER%202021/MAIS%20202/translator/Untitled.ipynb#alidation sets using an 80-20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "# Show length\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset('trainingSet.txt', 'all')\n",
    "print (\"Modern English; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Shakespearean English; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tf.data dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index)+1\n",
    "vocab_tar_size = len(targ_lang.word_index)+1\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "#doc = load_document('hamlet.txt')\n",
    "# split into pairs\n",
    "#pairs = make_pairs(doc)\n",
    "# clean sentences\n",
    "#cleaned_pairs = clean(pairs)\n",
    "# save clean pairs to file\n",
    "#save_clean_data(cleaned_pairs, 'translation-clean.pkl')\n",
    "#cleaned_pairs[0] : shakespearean English\n",
    "#cleaned_pairs[1] : modern English\n",
    "#print(cleaned_pairs)\n",
    "\n",
    "# spot check\n",
    "#for i in range(10):\n",
    "#\tprint('[%s] => [%s]' % (cleaned_pairs[i][0], cleaned_pairs[i][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state = self.gru(x, initial_state = hidden)\n",
    "        return output, state\n",
    "\n",
    "    def initialize_hidden_state(self):\n",
    "        return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "# sample input\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "            self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_layer = BahdanauAttention(10)\n",
    "attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n",
    "\n",
    "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
    "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True,\n",
    "                                       recurrent_initializer='glorot_uniform')\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "        # used for attention\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, hidden, enc_output):\n",
    "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
    "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
    "        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
    "        x = self.embedding(x)\n",
    "        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        # passing the concatenated vector to the GRU\n",
    "        output, state = self.gru(x)\n",
    "        # output shape == (batch_size * 1, hidden_size)\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        # output shape == (batch_size, vocab)\n",
    "        x = self.fc(output)\n",
    "        return x, state, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),sample_hidden, sample_output)\n",
    "\n",
    "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization and Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,encoder=encoder,decoder=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
    "        dec_hidden = enc_hidden\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    enc_hidden = encoder.initialize_hidden_state()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ, enc_hidden)\n",
    "        total_loss += batch_loss\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence):\n",
    "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
    "\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],maxlen=max_length_inp, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    hidden = [tf.zeros((1, units))]\n",
    "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
    "\n",
    "    dec_hidden = enc_hidden\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(max_length_targ):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,dec_hidden, enc_out)\n",
    "\n",
    "    # storing the attention weights to plot later on\n",
    "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "    attention_plot[t] = attention_weights.numpy()\n",
    "\n",
    "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "    result += targ_lang.index_word[predicted_id] + ' '\n",
    "\n",
    "    if targ_lang.index_word[predicted_id] == '<end>':\n",
    "        return result, sentence, attention_plot\n",
    "\n",
    "    # the predicted ID is fed back into the model\n",
    "    dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for plotting the attention weights\n",
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    result, sentence, attention_plot = evaluate(sentence)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
    "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restoring the latest checkpoint in checkpoint_dir\n",
    "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "translate(u'It’ all the same')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
