{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "#from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import scipy\n",
    "import numpy\n",
    "import matplotlib\n",
    "import pandas\n",
    "import statsmodels\n",
    "import sklearn\n",
    "#import theano\n",
    "#import tensorflow\n",
    "#import keras\n",
    "import string\n",
    "import re\n",
    "from pickle import dump\n",
    "from unicodedata import normalize\n",
    "from numpy import array\n",
    "import fileinput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def killbreaks(string):\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.replace('\\r', ' ')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=2\n",
    "with open('hamlet.txt',\"w+\",encoding=\"utf-8\")  as fp:\n",
    "    while i <= 106 :\n",
    "        with urllib.request.urlopen(\"https://www.sparknotes.com/nofear/shakespeare/hamlet/page_{}/\".format(i)) as page:\n",
    "            soup = BeautifulSoup(page)\n",
    "            table = soup.find(\"table\", {\"class\":\"noFear noFear--hasNumbers\"})\n",
    "            rows = table.findAll(\"tr\")\n",
    "            for row in rows:\n",
    "                for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--original\"}):\n",
    "                    for div in td.find_all(\"div\"):\n",
    "                        fp.write(killbreaks(div.text))\n",
    "                    fp.write(\"\\n\")\n",
    "                for td in row.find_all(\"td\", {\"class\":\"noFear__cell noFear__cell--modern\"}):\n",
    "                    for div in td.find_all(\"div\"):\n",
    "                        fp.write(killbreaks(div.text))\n",
    "                    fp.write(\"\\n\")\n",
    "                    fp.write(\"\\n\")\n",
    "        # page 108 of Hamlet makes it crash\n",
    "        if i==106 :\n",
    "            i+=2    \n",
    "        i+=2\n",
    "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.readlines()\n",
    "with open(\"hamletLineless.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in lines:\n",
    "        isWrong = False\n",
    "        for word in line.split():\n",
    "            if word.isupper() and len(word) >= 2:\n",
    "                isWrong = True\n",
    "                break\n",
    "        if isWrong == False:\n",
    "            f.write(bigboi(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document(filename):\n",
    "  #open the file as read only\n",
    "  file = open(filename, mode='rt', encoding='utf-8')\n",
    "  #store in variable  \n",
    "  text=file.read()\n",
    "  #close it\n",
    "  file.close\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pairs(document):\n",
    "  #group the corresponding lines\n",
    "  to_be_paired = document.split('\\n\\n')\n",
    "  #pair the corresponding translations\n",
    "  line_pairs = [line.split('\\n') for line in to_be_paired]\n",
    "  #pair the corresponding sentences\n",
    "  #sentece_pairs = [line.split('.') for line in  lines]\n",
    "  return line_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigboi(string):\n",
    "    string = string.replace('A', ' A')\n",
    "    string = string.replace('B', ' B')\n",
    "    string = string.replace('C', ' C')\n",
    "    string = string.replace('D', ' D')\n",
    "    string = string.replace('E', ' E')\n",
    "    string = string.replace('F', ' F')\n",
    "    string = string.replace('G', ' G')\n",
    "    string = string.replace('H', ' H')\n",
    "    string = string.replace('I', ' I')\n",
    "    string = string.replace('J', ' J')\n",
    "    string = string.replace('K', ' K')\n",
    "    string = string.replace('L', ' L')\n",
    "    string = string.replace('M', ' M')\n",
    "    string = string.replace('N', ' N')\n",
    "    string = string.replace('O', ' O')\n",
    "    string = string.replace('P', ' P')\n",
    "    string = string.replace('Q', ' Q')\n",
    "    string = string.replace('R', ' R')\n",
    "    string = string.replace('S', ' S')\n",
    "    string = string.replace('T', ' T')\n",
    "    string = string.replace('U', ' U')\n",
    "    string = string.replace('V', ' V')\n",
    "    string = string.replace('W', ' W')\n",
    "    string = string.replace('X', ' X')\n",
    "    string = string.replace('Y', ' Y')\n",
    "    string = string.replace('Z', ' Z')\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---dataset cleaning------\n",
    "def clean(pairs):\n",
    "    cleaned= []\n",
    "    # regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for pair in pairs:\n",
    "        clean_pair = []\n",
    "        for line in pair:\n",
    "            # normalize unicode characters\n",
    "            line = normalize('NFD', line).encode('ascii', 'ignore')\n",
    "            line = line.decode('UTF-8')\n",
    "            # tokenize on white space\n",
    "            line = line.split()\n",
    "            # convert to lowercase\n",
    "            # line = [re_print.sub('', word) for word in line if not word.isupper()]\n",
    "            # convert to lowercase\n",
    "            line = [word.lower() for word in line]\n",
    "            # remove punctuation from each token\n",
    "            line = [word.translate(table) for word in line]\n",
    "            # remove non-printable chars form each token\n",
    "            line = [re_print.sub('', w) for w in line]\n",
    "            # remove tokens with numbers in them\n",
    "            line = [word for word in line if word.isalpha()]\n",
    "            #remove extra spaces\n",
    "            line = [re.sub(' +', ' ', w) for w in line]\n",
    "            # store as string\n",
    "            clean_pair.append(' '.join(line))\n",
    "        cleaned.append(clean_pair)\n",
    "    return cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a list of clean sentences to file\n",
    "def save_clean_data(sentences, filename):\n",
    "    dump(sentences, open(filename, 'wb'))\n",
    "    print('Saved: %s' % filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: translation-clean.pkl\n",
      "[] => [whos there]\n",
      "[nay answer me stand and unfold yourself] => [no who are you stop and identify yourself]\n",
      "[long live the king] => [long live the king]\n",
      "[barnardo] => [is that barnardo]\n",
      "[he] => [yes its me]\n",
      "[you come most carefully upon your hour] => [youve come right on time]\n",
      "[tis now struck twelve get thee to bed francisco] => [the clocks just striking twelve go home to bed francisco]\n",
      "[for this relief much thanks tis bitter cold and i am sick at heart] => [thanks for letting me go its bitterly cold out and im depressed]\n",
      "[have you had quiet guard] => [has it been a quiet night]\n",
      "[not a mouse stirring] => [i havent even heard a mouse squeak]\n",
      "[well good night if you do meet horatio and marcellus the rivals of my watch bid them make haste] => [well good night if you happen to see horatio and marcellus who are supposed to stand guard with me tonight tell them to hurry]\n",
      "[i think i hear them stand ho whos there] => [i think i hear them stop whos there]\n",
      "[] => [friends to this ground]\n",
      "[and liegemen to the dane] => [and servants of the danish king]\n",
      "[give you good night] => [good night to you both]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-225-b196d72bc091>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# spot check\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'[%s] => [%s]'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcleaned_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcleaned_pairs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "doc = load_document('hamletLineless.txt')\n",
    "# split into pairs\n",
    "pairs = make_pairs(doc)\n",
    "# clean sentences\n",
    "cleaned_pairs = clean(pairs)\n",
    "# save clean pairs to file\n",
    "#print(cleaned_pairs)\n",
    "save_clean_data(cleaned_pairs, 'translation-clean.pkl')\n",
    "# spot check\n",
    "for i in range(20):\n",
    "\tprint('[%s] => [%s]' % (cleaned_pairs[i][0], cleaned_pairs[i][1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
